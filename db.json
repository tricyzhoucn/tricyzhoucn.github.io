{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/readme","path":"readme","modified":0,"renderable":0},{"_id":"source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":0},{"_id":"source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"555e6ab5d13f5ad187d95ba324b34409ed1bcf8c","modified":1671413452668},{"_id":"source/CNAME","hash":"29f25f51b5326e18e33b9cf7357956b7aeffed91","modified":1671413452668},{"_id":"source/readme","hash":"2251c9aadb6a86bf640e5c959f26d52ee5a1da12","modified":1671413452699},{"_id":"source/images/favicon-16x16-next.png","hash":"3b35fa6bb139d5d8ceb8e9cb30aee42fc6038c6a","modified":1671413452699},{"_id":"source/images/favicon-32x32-next.png","hash":"fdb07f5cf525799ab6e00daee33740eeb1597470","modified":1671413452699},{"_id":"source/_posts/Data-Warehouse.md","hash":"a78fb82566afda77c60c05f7f6323a05039e2eb7","modified":1671413452668},{"_id":"source/_posts/big-data.md","hash":"cc674dc24c3b37a416dcd4dc1bc75b34c6a31b67","modified":1671413452668},{"_id":"source/_posts/code.md","hash":"af7bdf7c1abd203403017a6396f4216483d4498b","modified":1671413452669},{"_id":"source/_posts/dp-py.md","hash":"24aec58b28e14afba51bb12bf27b5aa5f6419a22","modified":1671413452669},{"_id":"source/_posts/hello-world.md","hash":"5fd0b8af3f12a4106c946276c1e0ff52b49f22f5","modified":1671413452669},{"_id":"source/_posts/binary-tree.md","hash":"363cbfd1abf75427e2e000a98a7a935adafa0fa2","modified":1671413452669},{"_id":"source/_posts/hdfs-block.md","hash":"290fcc7c959c1ec4116c0e408ec504c2c4c1e91a","modified":1671413452669},{"_id":"source/_posts/hive-data.md","hash":"1d7424f2442f996ad5563e1ba0cca355ace6a2bf","modified":1671413452683},{"_id":"source/_posts/markdown-rule.md","hash":"f171abef07e1b4ddea74117a425f4ef02f845ab4","modified":1671413452683},{"_id":"source/_posts/pytorch-log.md","hash":"0f4fb713dc193e292529f69b636261f83a7fde46","modified":1671413452684},{"_id":"source/_posts/linked-list.md","hash":"5847852413553eaad8469f5633e9439b7961e39e","modified":1671413452683},{"_id":"source/_posts/sort-py.md","hash":"f9d0d6b73fec9d74d454088b2ba1df144e2cf020","modified":1671413452684},{"_id":"source/_posts/hive-sql.md","hash":"d5ee60f9959ae44664050ed8bd3f0be102ff9462","modified":1671413452683},{"_id":"source/_posts/spark-opt.md","hash":"5a004eef37ae04bf40c23c0cc0a03161cded7584","modified":1671413452684},{"_id":"source/_posts/spark-sql.md","hash":"38e2a8a1281cf6da16b2ed96b723b7c3720975a5","modified":1671413452694},{"_id":"source/_posts/spark-yarn.md","hash":"fb2d87d4bcdd9cb63a2ce52a0631c05449b1623b","modified":1671413452697},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1671413452668},{"_id":"source/_posts/spark-role.md","hash":"600b62f8113649c752e3703d3e3bb21305008fd9","modified":1671413452693},{"_id":"source/_posts/hive-data/orc.png","hash":"c6dc4eef033c8cc54317ec4ecc9d5a4f41f4a3e7","modified":1671413452683},{"_id":"source/_posts/hive-data/parquet.png","hash":"963d1eb3d31603bbb819c53e906599c23a758ddd","modified":1671413452683},{"_id":"source/_posts/spark-sql/parser.png","hash":"2360421424f47e53f4d65cf2059e693167af5f7f","modified":1671413452696},{"_id":"source/_posts/spark-sql/summary.png","hash":"083258bab2185debaa1b499f21e86688f7cddfaa","modified":1671413452697},{"_id":"source/_posts/spark-yarn/sparkonyarn.png","hash":"d6a3bb6828573838d9d075b2591e2ee78884b41e","modified":1671413452697},{"_id":"source/_posts/spark-sql/predicate_pushdown.png","hash":"1109bd4db7077eefe6f98442d28b8a9885d2022b","modified":1671413452696},{"_id":"source/_posts/spark-sql/constant_fold.png","hash":"3d324753e649065d7c9a2b088b0660d98363dd73","modified":1671413452695},{"_id":"source/_posts/spark-sql/analyzer.png","hash":"9fc8f22f6cc83fe4f66bd880f17f8ffd5d77b334","modified":1671413452695},{"_id":"source/_posts/spark-opt/sortshufflemanager.png","hash":"d572d25a0ec36b90e7adaba7eb58fa31f2c61850","modified":1671413452692},{"_id":"source/_posts/hello-world/kafka.png","hash":"34825be847c5afe14a8d2846d2255551097d06d0","modified":1671413452683},{"_id":"source/_posts/spark-opt/spark-submit.png","hash":"9e201bdd162b8115115e10bff5c7fcf77aac6d1a","modified":1671413452693},{"_id":"source/_posts/spark-yarn/yarnclient.png","hash":"1aac39ad9a56a2e08a229bdde03b7114c9e386db","modified":1671413452698},{"_id":"source/_posts/spark-role/sparksubmit.png","hash":"be326f6f291d683a2c46e95269db8e8d3ff5c8e4","modified":1671413452694},{"_id":"source/_posts/spark-yarn/yarncluster.png","hash":"d3582fabd7dbf129546c16a66010c763380d7d25","modified":1671413452699},{"_id":"source/_posts/spark-opt/hashshufflemanageropt.png","hash":"08509fc488b11974401a70917e2de975dd0eabd9","modified":1671413452692},{"_id":"source/_posts/spark-opt/bypass.png","hash":"1649b4bb0197ec36b416158300a998fa3603fca7","modified":1671413452687},{"_id":"source/_posts/spark-opt/hashshufflemanager.png","hash":"e23a99e0e912350c504a2737b33773b020d51183","modified":1671413452689},{"_id":"source/_posts/hello-world/2022020901.jpeg","hash":"684d82ae93181dfdec5137bdaccab71a7bafdb23","modified":1671413452677},{"_id":"source/_posts/hello-world/2022020902.jpeg","hash":"e06f6e0e4ab09bb21551fe321cfd67e4c86452dd","modified":1671413452682},{"_id":"public/2022/12/19/hello-world/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/15/spark-role/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/09/dp-py/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/08/binary-tree/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/07/linked-list/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/05/hive-sql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/05/sort-py/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/15/spark-yarn/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/12/14/hdfs-block/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/11/28/Data-Warehouse/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/11/23/hive-data/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/03/16/code/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/02/08/big-data/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/01/23/markdown-rule/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/2022/01/23/pytorch-log/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/01/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/02/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/03/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/11/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/archives/2022/12/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/大数据/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/数据/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/HDFS/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/动态规划/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/树/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/链表/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/算法/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/pytorch/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/sql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/tags/Spark/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671413475208},{"_id":"public/readme","hash":"2251c9aadb6a86bf640e5c959f26d52ee5a1da12","modified":1671413475208},{"_id":"public/CNAME","hash":"29f25f51b5326e18e33b9cf7357956b7aeffed91","modified":1671413475208},{"_id":"public/images/favicon-32x32-next.png","hash":"fdb07f5cf525799ab6e00daee33740eeb1597470","modified":1671413475208},{"_id":"public/2022/11/23/hive-data/orc.png","hash":"c6dc4eef033c8cc54317ec4ecc9d5a4f41f4a3e7","modified":1671413475208},{"_id":"public/images/favicon-16x16-next.png","hash":"3b35fa6bb139d5d8ceb8e9cb30aee42fc6038c6a","modified":1671413475208},{"_id":"public/2022/11/23/hive-data/parquet.png","hash":"963d1eb3d31603bbb819c53e906599c23a758ddd","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/parser.png","hash":"2360421424f47e53f4d65cf2059e693167af5f7f","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/summary.png","hash":"083258bab2185debaa1b499f21e86688f7cddfaa","modified":1671413475208},{"_id":"public/2022/12/15/spark-yarn/sparkonyarn.png","hash":"d6a3bb6828573838d9d075b2591e2ee78884b41e","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/analyzer.png","hash":"9fc8f22f6cc83fe4f66bd880f17f8ffd5d77b334","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/constant_fold.png","hash":"3d324753e649065d7c9a2b088b0660d98363dd73","modified":1671413475208},{"_id":"public/2022/11/25/spark-sql/predicate_pushdown.png","hash":"1109bd4db7077eefe6f98442d28b8a9885d2022b","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/sortshufflemanager.png","hash":"d572d25a0ec36b90e7adaba7eb58fa31f2c61850","modified":1671413475208},{"_id":"public/2022/12/19/hello-world/kafka.png","hash":"34825be847c5afe14a8d2846d2255551097d06d0","modified":1671413475208},{"_id":"public/2022/12/15/spark-role/sparksubmit.png","hash":"be326f6f291d683a2c46e95269db8e8d3ff5c8e4","modified":1671413475208},{"_id":"public/2022/12/15/spark-yarn/yarncluster.png","hash":"d3582fabd7dbf129546c16a66010c763380d7d25","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/spark-submit.png","hash":"9e201bdd162b8115115e10bff5c7fcf77aac6d1a","modified":1671413475208},{"_id":"public/2022/12/15/spark-yarn/yarnclient.png","hash":"1aac39ad9a56a2e08a229bdde03b7114c9e386db","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/hashshufflemanageropt.png","hash":"08509fc488b11974401a70917e2de975dd0eabd9","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/hashshufflemanager.png","hash":"e23a99e0e912350c504a2737b33773b020d51183","modified":1671413475208},{"_id":"public/2022/12/09/spark-opt/bypass.png","hash":"1649b4bb0197ec36b416158300a998fa3603fca7","modified":1671413475208},{"_id":"public/2022/12/19/hello-world/2022020901.jpeg","hash":"684d82ae93181dfdec5137bdaccab71a7bafdb23","modified":1671413475208},{"_id":"public/2022/12/19/hello-world/2022020902.jpeg","hash":"e06f6e0e4ab09bb21551fe321cfd67e4c86452dd","modified":1671413475208},{"_id":"source/_posts/flink-start.md","hash":"c7c654468662b4c28bb94bf63db74d23df1f66cb","modified":1671587681379},{"_id":"source/_posts/clickhouse.md","hash":"a4725c83fa802556d996f1742e5a0cfa02e68fda","modified":1671590821620},{"_id":"source/_posts/hbase.md","hash":"4079693960d41c348e3561031a6aeeb2476c2269","modified":1671587665410},{"_id":"source/_posts/clickhouse/engine.png","hash":"53be16acb635c5917cdc56cded61ce459c025b01","modified":1671587848887},{"_id":"source/_posts/clickhouse/lsmtree.jpeg","hash":"e78476f79c409104ad48e7942aa0ec02dfa829e7","modified":1671590325507},{"_id":"source/_posts/flink-start/flinkstart.png","hash":"d43821bc69e1511bf4644d470f7793ba013d2c43","modified":1671417596125},{"_id":"public/2022/12/21/clickhouse/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/2022/12/20/hbase/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/2022/12/19/flink-start/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/archives/2022/12/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/tags/HBase/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/tags/Flink/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/tags/clickhouse/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671671506447},{"_id":"public/2022/12/19/flink-start/flinkstart.png","hash":"d43821bc69e1511bf4644d470f7793ba013d2c43","modified":1671671506447},{"_id":"public/2022/12/21/clickhouse/lsmtree.jpeg","hash":"e78476f79c409104ad48e7942aa0ec02dfa829e7","modified":1671671506447},{"_id":"public/2022/12/21/clickhouse/engine.png","hash":"53be16acb635c5917cdc56cded61ce459c025b01","modified":1671671506447}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"Data-Warehouse","date":"2022-11-28T06:24:23.000Z","_content":"\n> 数仓分层\n\n<!-- more -->\n\n### 数据仓库\n\n#### 简介\n\n包括etl、调度、建模在内的完整理论体系，以查询和分析为基础，应用于OLAP，支持复杂分析操作，侧重决策支持，提供直观易懂的查询结果\n\n#### 特点\n\n* 面向主题\n* 集成\n* 不可修改\n* 与时间相关\n\n### 数据分层\n\n基础分层思想：**数据运营层、数据仓库层、数据服务层**\n\n数据运营层：\n\n* 数据运营层ODS：Operation Data Store数据准备层，贴源层\n* 数据仓库层DW：从下到上DWD，DWB，DWS\n  * DWD：Data Warehouse Details细节数据层，业务层和数据仓库隔离层，主要是对ODS数据层走一些数据清洗和规范化操作；\n  * DWB：Data Warehouse Base数据基础层，客观数据，用作中间层，可以认为是大量指标的数据层；\n  * DWS：Data Warehouse Service数据服务层，主题域的服务数据层，宽表；\n* 数据服务/应用层ADS：数据产品和数据分析使用，存储在ES、MySQL等系统；\n\n### 数据库设计三范式\n\n* 第一范式：确保每列原子性，数据库表中所有字段都是不可分解原子值；\n* 第二范式：确保每列都和主键相关，一个表只存一种数据；\n* 第三范式：确保每列都是主键直接相关，而不是间接；\n\n### 参考\n\nhttps://www.cnblogs.com/amyzhu/p/13513425.html\n","source":"_posts/Data-Warehouse.md","raw":"---\ntitle: Data-Warehouse\ndate: 2022-11-28 14:24:23\ntags: 大数据\n---\n\n> 数仓分层\n\n<!-- more -->\n\n### 数据仓库\n\n#### 简介\n\n包括etl、调度、建模在内的完整理论体系，以查询和分析为基础，应用于OLAP，支持复杂分析操作，侧重决策支持，提供直观易懂的查询结果\n\n#### 特点\n\n* 面向主题\n* 集成\n* 不可修改\n* 与时间相关\n\n### 数据分层\n\n基础分层思想：**数据运营层、数据仓库层、数据服务层**\n\n数据运营层：\n\n* 数据运营层ODS：Operation Data Store数据准备层，贴源层\n* 数据仓库层DW：从下到上DWD，DWB，DWS\n  * DWD：Data Warehouse Details细节数据层，业务层和数据仓库隔离层，主要是对ODS数据层走一些数据清洗和规范化操作；\n  * DWB：Data Warehouse Base数据基础层，客观数据，用作中间层，可以认为是大量指标的数据层；\n  * DWS：Data Warehouse Service数据服务层，主题域的服务数据层，宽表；\n* 数据服务/应用层ADS：数据产品和数据分析使用，存储在ES、MySQL等系统；\n\n### 数据库设计三范式\n\n* 第一范式：确保每列原子性，数据库表中所有字段都是不可分解原子值；\n* 第二范式：确保每列都和主键相关，一个表只存一种数据；\n* 第三范式：确保每列都是主键直接相关，而不是间接；\n\n### 参考\n\nhttps://www.cnblogs.com/amyzhu/p/13513425.html\n","slug":"Data-Warehouse","published":1,"updated":"2022-12-19T01:30:52.668Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh2000016gn0o4e3og8","content":"<blockquote>\n<p>数仓分层</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"数据仓库\"><a href=\"#数据仓库\" class=\"headerlink\" title=\"数据仓库\"></a>数据仓库</h3><h4 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h4><p>包括etl、调度、建模在内的完整理论体系，以查询和分析为基础，应用于OLAP，支持复杂分析操作，侧重决策支持，提供直观易懂的查询结果</p>\n<h4 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h4><ul>\n<li>面向主题</li>\n<li>集成</li>\n<li>不可修改</li>\n<li>与时间相关</li>\n</ul>\n<h3 id=\"数据分层\"><a href=\"#数据分层\" class=\"headerlink\" title=\"数据分层\"></a>数据分层</h3><p>基础分层思想：<strong>数据运营层、数据仓库层、数据服务层</strong></p>\n<p>数据运营层：</p>\n<ul>\n<li>数据运营层ODS：Operation Data Store数据准备层，贴源层</li>\n<li>数据仓库层DW：从下到上DWD，DWB，DWS<ul>\n<li>DWD：Data Warehouse Details细节数据层，业务层和数据仓库隔离层，主要是对ODS数据层走一些数据清洗和规范化操作；</li>\n<li>DWB：Data Warehouse Base数据基础层，客观数据，用作中间层，可以认为是大量指标的数据层；</li>\n<li>DWS：Data Warehouse Service数据服务层，主题域的服务数据层，宽表；</li>\n</ul>\n</li>\n<li>数据服务/应用层ADS：数据产品和数据分析使用，存储在ES、MySQL等系统；</li>\n</ul>\n<h3 id=\"数据库设计三范式\"><a href=\"#数据库设计三范式\" class=\"headerlink\" title=\"数据库设计三范式\"></a>数据库设计三范式</h3><ul>\n<li>第一范式：确保每列原子性，数据库表中所有字段都是不可分解原子值；</li>\n<li>第二范式：确保每列都和主键相关，一个表只存一种数据；</li>\n<li>第三范式：确保每列都是主键直接相关，而不是间接；</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/amyzhu/p/13513425.html\">https://www.cnblogs.com/amyzhu/p/13513425.html</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>数仓分层</p>\n</blockquote>","more":"<h3 id=\"数据仓库\"><a href=\"#数据仓库\" class=\"headerlink\" title=\"数据仓库\"></a>数据仓库</h3><h4 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h4><p>包括etl、调度、建模在内的完整理论体系，以查询和分析为基础，应用于OLAP，支持复杂分析操作，侧重决策支持，提供直观易懂的查询结果</p>\n<h4 id=\"特点\"><a href=\"#特点\" class=\"headerlink\" title=\"特点\"></a>特点</h4><ul>\n<li>面向主题</li>\n<li>集成</li>\n<li>不可修改</li>\n<li>与时间相关</li>\n</ul>\n<h3 id=\"数据分层\"><a href=\"#数据分层\" class=\"headerlink\" title=\"数据分层\"></a>数据分层</h3><p>基础分层思想：<strong>数据运营层、数据仓库层、数据服务层</strong></p>\n<p>数据运营层：</p>\n<ul>\n<li>数据运营层ODS：Operation Data Store数据准备层，贴源层</li>\n<li>数据仓库层DW：从下到上DWD，DWB，DWS<ul>\n<li>DWD：Data Warehouse Details细节数据层，业务层和数据仓库隔离层，主要是对ODS数据层走一些数据清洗和规范化操作；</li>\n<li>DWB：Data Warehouse Base数据基础层，客观数据，用作中间层，可以认为是大量指标的数据层；</li>\n<li>DWS：Data Warehouse Service数据服务层，主题域的服务数据层，宽表；</li>\n</ul>\n</li>\n<li>数据服务/应用层ADS：数据产品和数据分析使用，存储在ES、MySQL等系统；</li>\n</ul>\n<h3 id=\"数据库设计三范式\"><a href=\"#数据库设计三范式\" class=\"headerlink\" title=\"数据库设计三范式\"></a>数据库设计三范式</h3><ul>\n<li>第一范式：确保每列原子性，数据库表中所有字段都是不可分解原子值；</li>\n<li>第二范式：确保每列都和主键相关，一个表只存一种数据；</li>\n<li>第三范式：确保每列都是主键直接相关，而不是间接；</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/amyzhu/p/13513425.html\">https://www.cnblogs.com/amyzhu/p/13513425.html</a></p>"},{"title":"算法相关","date":"2022-03-16T08:18:10.000Z","_content":"\n> 算法记录\n\n<!-- more -->\n\n## 4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？\n\n```python\ndef t1():\n    res = [[0]*(m+1) for _ in range(n+1)]\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if j < A[i-1]:\n                res[i][j] = res[i-1][j]\n            else:\n                res[i][j] = max(res[i-1][j], res[i-1][j-A[i-1]]+V[i-1])\n    return res[-1][-1]\ndef t2():\n    res = [0] * (m+1)\n    for i in range(1, n+1):\n        for j in range(m, 0, -1): # 必须逆序\n            if A[i-1]<=j:\n                res[j] = max(res[j], res[j-A[i-1]]+V[i-1])\n    return res[-1]\n```\n\n\n\n## 5.给定整数数组nums，给定元素个数n，求子数组？\n\n解法1：常规遍历\n\n```python\ndef subsets(nums):\n    output = [[]]\n    for num in nums:\n        for cur in output.copy():\n            ext = cur + [num]\n            output.append(ext)\n        # output += [cur + [num] for cur in output] # 简化\n    return output\nprint(subsets([1,2,3,4,5]))\n```\n\n解法2: 位运算\n\n```python\ndef subsets(nums):\n    output = []\n    for i in range(2**len(nums)):\n        sub = []\n        for j in range(len(nums)):\n            if (i >> j) % 2 == 1: # 当前位1取0不取\n                sub.append(nums[j])\n        output.append(sub)\n    return output\n```\n\n解法3: 回溯\n\n```python\ndef subsets(nums):\n    res = []\n    n = len(nums)\n    def helper(i, tmp):\n        res.append(tmp)\n        for j in range(i, n):\n            helper(j + 1, tmp+[nums[j]])\n    helper(0, [])\n    return res\nprint(subsets([1,2,3,4]))\n```\n\n> 变种1: 求子数组和可被整除\n\n解法1：拆分&查找\n\n```python\nprint(sum(map(lambda x:x%m==0, subsets(nums)[1:])))\n# 优化:拆分&查找\nmid = (len(nums)+1)/2\nnums1 = nums[:mid]\nnums1 = nums[mid:]\n```\n\n\n\n","source":"_posts/code.md","raw":"---\ntitle: 算法相关\ndate: 2022-03-16 16:18:10\ntags: 算法\n---\n\n> 算法记录\n\n<!-- more -->\n\n## 4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？\n\n```python\ndef t1():\n    res = [[0]*(m+1) for _ in range(n+1)]\n    for i in range(1, n+1):\n        for j in range(1, m+1):\n            if j < A[i-1]:\n                res[i][j] = res[i-1][j]\n            else:\n                res[i][j] = max(res[i-1][j], res[i-1][j-A[i-1]]+V[i-1])\n    return res[-1][-1]\ndef t2():\n    res = [0] * (m+1)\n    for i in range(1, n+1):\n        for j in range(m, 0, -1): # 必须逆序\n            if A[i-1]<=j:\n                res[j] = max(res[j], res[j-A[i-1]]+V[i-1])\n    return res[-1]\n```\n\n\n\n## 5.给定整数数组nums，给定元素个数n，求子数组？\n\n解法1：常规遍历\n\n```python\ndef subsets(nums):\n    output = [[]]\n    for num in nums:\n        for cur in output.copy():\n            ext = cur + [num]\n            output.append(ext)\n        # output += [cur + [num] for cur in output] # 简化\n    return output\nprint(subsets([1,2,3,4,5]))\n```\n\n解法2: 位运算\n\n```python\ndef subsets(nums):\n    output = []\n    for i in range(2**len(nums)):\n        sub = []\n        for j in range(len(nums)):\n            if (i >> j) % 2 == 1: # 当前位1取0不取\n                sub.append(nums[j])\n        output.append(sub)\n    return output\n```\n\n解法3: 回溯\n\n```python\ndef subsets(nums):\n    res = []\n    n = len(nums)\n    def helper(i, tmp):\n        res.append(tmp)\n        for j in range(i, n):\n            helper(j + 1, tmp+[nums[j]])\n    helper(0, [])\n    return res\nprint(subsets([1,2,3,4]))\n```\n\n> 变种1: 求子数组和可被整除\n\n解法1：拆分&查找\n\n```python\nprint(sum(map(lambda x:x%m==0, subsets(nums)[1:])))\n# 优化:拆分&查找\nmid = (len(nums)+1)/2\nnums1 = nums[:mid]\nnums1 = nums[mid:]\n```\n\n\n\n","slug":"code","published":1,"updated":"2022-12-19T01:30:52.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh4000116gn6t5zgq99","content":"<blockquote>\n<p>算法记录</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h2 id=\"4-动态规划-i个物品，价格ai，收益bi，预算m，单次购买最大收益？\"><a href=\"#4-动态规划-i个物品，价格ai，收益bi，预算m，单次购买最大收益？\" class=\"headerlink\" title=\"4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？\"></a>4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">t1</span>():</span></span><br><span class=\"line\">    res = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> j &lt; A[i-<span class=\"number\">1</span>]:</span><br><span class=\"line\">                res[i][j] = res[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                res[i][j] = <span class=\"built_in\">max</span>(res[i-<span class=\"number\">1</span>][j], res[i-<span class=\"number\">1</span>][j-A[i-<span class=\"number\">1</span>]]+V[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">t2</span>():</span></span><br><span class=\"line\">    res = [<span class=\"number\">0</span>] * (m+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m, <span class=\"number\">0</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 必须逆序</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> A[i-<span class=\"number\">1</span>]&lt;=j:</span><br><span class=\"line\">                res[j] = <span class=\"built_in\">max</span>(res[j], res[j-A[i-<span class=\"number\">1</span>]]+V[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res[-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"5-给定整数数组nums，给定元素个数n，求子数组？\"><a href=\"#5-给定整数数组nums，给定元素个数n，求子数组？\" class=\"headerlink\" title=\"5.给定整数数组nums，给定元素个数n，求子数组？\"></a>5.给定整数数组nums，给定元素个数n，求子数组？</h2><p>解法1：常规遍历</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    output = [[]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> nums:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> cur <span class=\"keyword\">in</span> output.copy():</span><br><span class=\"line\">            ext = cur + [num]</span><br><span class=\"line\">            output.append(ext)</span><br><span class=\"line\">        <span class=\"comment\"># output += [cur + [num] for cur in output] # 简化</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output</span><br><span class=\"line\"><span class=\"built_in\">print</span>(subsets([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n\n<p>解法2: 位运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    output = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>**<span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">        sub = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt;&gt; j) % <span class=\"number\">2</span> == <span class=\"number\">1</span>: <span class=\"comment\"># 当前位1取0不取</span></span><br><span class=\"line\">                sub.append(nums[j])</span><br><span class=\"line\">        output.append(sub)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n\n<p>解法3: 回溯</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    res = []</span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span>(<span class=\"params\">i, tmp</span>):</span></span><br><span class=\"line\">        res.append(tmp)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i, n):</span><br><span class=\"line\">            helper(j + <span class=\"number\">1</span>, tmp+[nums[j]])</span><br><span class=\"line\">    helper(<span class=\"number\">0</span>, [])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res</span><br><span class=\"line\"><span class=\"built_in\">print</span>(subsets([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]))</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>变种1: 求子数组和可被整除</p>\n</blockquote>\n<p>解法1：拆分&amp;查找</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">sum</span>(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x:x%m==<span class=\"number\">0</span>, subsets(nums)[<span class=\"number\">1</span>:])))</span><br><span class=\"line\"><span class=\"comment\"># 优化:拆分&amp;查找</span></span><br><span class=\"line\">mid = (<span class=\"built_in\">len</span>(nums)+<span class=\"number\">1</span>)/<span class=\"number\">2</span></span><br><span class=\"line\">nums1 = nums[:mid]</span><br><span class=\"line\">nums1 = nums[mid:]</span><br></pre></td></tr></table></figure>\n\n\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>算法记录</p>\n</blockquote>","more":"<h2 id=\"4-动态规划-i个物品，价格ai，收益bi，预算m，单次购买最大收益？\"><a href=\"#4-动态规划-i个物品，价格ai，收益bi，预算m，单次购买最大收益？\" class=\"headerlink\" title=\"4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？\"></a>4.动态规划: i个物品，价格ai，收益bi，预算m，单次购买最大收益？</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">t1</span>():</span></span><br><span class=\"line\">    res = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> j &lt; A[i-<span class=\"number\">1</span>]:</span><br><span class=\"line\">                res[i][j] = res[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                res[i][j] = <span class=\"built_in\">max</span>(res[i-<span class=\"number\">1</span>][j], res[i-<span class=\"number\">1</span>][j-A[i-<span class=\"number\">1</span>]]+V[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">t2</span>():</span></span><br><span class=\"line\">    res = [<span class=\"number\">0</span>] * (m+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m, <span class=\"number\">0</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 必须逆序</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> A[i-<span class=\"number\">1</span>]&lt;=j:</span><br><span class=\"line\">                res[j] = <span class=\"built_in\">max</span>(res[j], res[j-A[i-<span class=\"number\">1</span>]]+V[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res[-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n\n\n\n<h2 id=\"5-给定整数数组nums，给定元素个数n，求子数组？\"><a href=\"#5-给定整数数组nums，给定元素个数n，求子数组？\" class=\"headerlink\" title=\"5.给定整数数组nums，给定元素个数n，求子数组？\"></a>5.给定整数数组nums，给定元素个数n，求子数组？</h2><p>解法1：常规遍历</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    output = [[]]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> num <span class=\"keyword\">in</span> nums:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> cur <span class=\"keyword\">in</span> output.copy():</span><br><span class=\"line\">            ext = cur + [num]</span><br><span class=\"line\">            output.append(ext)</span><br><span class=\"line\">        <span class=\"comment\"># output += [cur + [num] for cur in output] # 简化</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> output</span><br><span class=\"line\"><span class=\"built_in\">print</span>(subsets([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n\n<p>解法2: 位运算</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    output = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>**<span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">        sub = []</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(nums)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i &gt;&gt; j) % <span class=\"number\">2</span> == <span class=\"number\">1</span>: <span class=\"comment\"># 当前位1取0不取</span></span><br><span class=\"line\">                sub.append(nums[j])</span><br><span class=\"line\">        output.append(sub)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> output</span><br></pre></td></tr></table></figure>\n\n<p>解法3: 回溯</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">subsets</span>(<span class=\"params\">nums</span>):</span></span><br><span class=\"line\">    res = []</span><br><span class=\"line\">    n = <span class=\"built_in\">len</span>(nums)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span>(<span class=\"params\">i, tmp</span>):</span></span><br><span class=\"line\">        res.append(tmp)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i, n):</span><br><span class=\"line\">            helper(j + <span class=\"number\">1</span>, tmp+[nums[j]])</span><br><span class=\"line\">    helper(<span class=\"number\">0</span>, [])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res</span><br><span class=\"line\"><span class=\"built_in\">print</span>(subsets([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>]))</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>变种1: 求子数组和可被整除</p>\n</blockquote>\n<p>解法1：拆分&amp;查找</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">sum</span>(<span class=\"built_in\">map</span>(<span class=\"keyword\">lambda</span> x:x%m==<span class=\"number\">0</span>, subsets(nums)[<span class=\"number\">1</span>:])))</span><br><span class=\"line\"><span class=\"comment\"># 优化:拆分&amp;查找</span></span><br><span class=\"line\">mid = (<span class=\"built_in\">len</span>(nums)+<span class=\"number\">1</span>)/<span class=\"number\">2</span></span><br><span class=\"line\">nums1 = nums[:mid]</span><br><span class=\"line\">nums1 = nums[mid:]</span><br></pre></td></tr></table></figure>"},{"title":"大数据组件","date":"2022-02-08T08:54:22.000Z","_content":"\n> 大数据相关整理\n\n<!-- more -->\n\n### 一、KAFKA\n\n#### 1、特性\n\n- 持久性：文件性存储，日志文件存储消息，达到阈值写磁盘，减少磁盘i/o，如果宕机会丢数据\n- 高吞吐：普通机器百万qps\n- 支持通过kafka服务器和消费机集群分区消息？\n- 支持hadoop并行数据加载\n\n#### 2、术语\n\n- Broker：消息中间处理节点，kafka节点=broker，一个或多个broker组成kafka集群\n- Topic：kafka根据topic归类消息\n- Producer：生产者\n- Consumer：消费者\n- ConsumerGroup：消费组\n- Partition：物理概念，一个topic分多个partition，partition内部有序\n\n#### 3、其他\n\n​\tpartition存储层面是append log文件，追加log文件尾部，offset标记消息在文件位置，offset是long数字，顺序写磁盘效率高于随机写内存，保证高吞吐\n\n#### 4、部署\n\n```shell\n[root@kafka ~]# wget https://archive.apache.org/dist/kafka/2.2.1/kafka_2.11-2.2.1.tgz\n[root@kafka ~]# tar zxf kafka_2.11-2.2.1.tgz \n[root@kafka ~]# mv kafka_2.11-2.2.1/ /usr/local/kafka\n[root@kafka ~]# cd /usr/local/kafka/bin/\n#启动zookeeper\n[root@kafka bin]# ./zookeeper-server-start.sh ../config/zookeeper.properties &\n#启动kafka\n[root@kafka bin]# ./kafka-server-start.sh ../config/server.properties &\n#查看端口是否在监听\n[root@kafka bin]# netstat -anput | grep 9092\ntcp6       0      0 :::9092                 :::*                    LISTEN      43326/java          \ntcp6       0      0 192.168.171.134:44388   192.168.171.134:9092    ESTABLISHED 43326/java          \ntcp6       0      0 192.168.171.134:9092    192.168.171.134:44388   ESTABLISHED 43326/java   \n#在本机创建kafka，副本数量为1，分区数量为1\n[root@kafka bin]# ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n#查看本机的topic\n[root@kafka bin]# ./kafka-topics.sh --list --bootstrap-server localhost:9092\ntest\n#发送消息到test\n[root@kafka bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test\n>aaa\n>bbb\n>ccc\n#开启新的终端，进行读取消息测试，“--from-beginning”表示从开头读取\n[root@kafka bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\naaa\nbbb\nccc\n```\n\n[kafka单机部署](https://cloud.tencent.com/developer/article/1624740)\n\n### 二、FLINK\n\n```shell\n# https://flink.apache.org/zh/downloads.html\n# scala2.12 flink-1.7.1\ntar zxvf flink-1.7.1-bin-scala_2.12.tgz\n# scala 2.11 flink-1.8.0\n# flink --version\ncd flink-1.7.1\n./bin/start-cluster.sh\n# http://9.134.196.182:8081\nwget https://streaming-with-flink.github.io/examples/download/examples-scala.jar\n./bin/flink run -c io.github.streamingwithflink.chapter1.AverageSensorReadings examples-scala.jar\n./log 日志\nweb ui cancel取消任务\n./bin/stop-cluster.sh \n\n\n# 依赖\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\nScala API: 为了使用 Scala API，将 flink-java 的 artifact id 替换为 flink-scala_2.11，同时将 flink-streaming-java_2.11 替换为 flink-streaming-scala_2.11。\n```\n\n[官网](https://flink.apache.org/)\n\n### 三、HADOOP\n\n```shell\n# cat /etc/profile\n# export JAVA_HOME=/usr/java/jdk1.8.0_231\n# export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3 # 需要重启命令行才生效\n[root@VM-196-182-centos /usr/local]# java -version\nopenjdk version \"1.8.0_71\"\nOpenJDK Runtime Environment (build 1.8.0_71-b15)\nOpenJDK 64-Bit Server VM (build 25.71-b15, mixed mode)\n[root@VM-196-182-centos /usr/local]# scala -version\nScala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL\n[root@VM-196-182-centos /usr/local]# hadoop version\n# hadoop-2.7.3.tar.gz\n# https://archive.apache.org/dist/hadoop/common/\nwget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\ntar zxvf hadoop-2.7.3.tar.gz \nmkdir -p /usr/local/hadoop/tmp/ # 指定hadoop运行时产生文件的存储路径 \nmkdir -p /usr/local/hadoop/hdfs/  \nmkdir -p /usr/local/hadoop/hdfs/data/ # datanode上数据块的物理存储位置\nmkdir -p /usr/local//hadoop/hdfs/name/ # namenode上存储hdfs名字空间元数据\nmv hadoop-2.7.3 /usr/local/hadoop\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hadoop-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64 # JAVA_HOME环境变量在这不生效 这里要显示声明一次\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml\n<configuration>\n    <!-- 指定HDFS老大（namenode）的通信地址 -->\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://9.134.196.182:9000</value>\n    </property>\n    <!-- 指定hadoop运行时产生文件的存储路径 -->\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/usr/local/hadoop/tmp</value>\n    </property>\n</configuration>\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml\n<configuration>\n    <property>\n        <name>dfs.name.dir</name>\n        <value>/usr/local/hadoop/hdfs/name</value>\n        <description>namenode上存储hdfs名字空间元数据 </description> \n    </property>\n    <property>\n        <name>dfs.data.dir</name>\n        <value>/usr/local/hadoop/hdfs/data</value>\n        <description>datanode上数据块的物理存储位置</description>\n    </property>\n    <!-- 设置hdfs副本数量 -->\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n# 配置yarn（非必需）\nmv mapred-site.xml.template mapred-site.xml\nvim mapred-site.xml\n<configuration>\n    <!-- 通知框架MR使用YARN -->\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/yarn-site.xml\n<configuration>\n    <!-- reducer取数据的方式是mapreduce_shuffle -->\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n# 格式化hdfs\ncd /usr/local/hadoop/hadoop-2.7.3\n./bin/hdfs namenode -format\n# 启动hdfs\n./sbin/start-dfs.sh // 需要输入本机密码 todo:配置免密\n# 停止hdfs\n./sbin/stop-dfs.sh // 需要输入本机密码 todo:配置免密\n# http://9.134.196.182:50070/ # hadoop管理界面\n# 启动yarn\n./sbin/start-yarn.sh // 需要输入本机密码 todo:配置免密\n# 停止yarn\n./sbin/stop-yarn.sh // 需要输入本机密码 todo:配置免密\n# http://9.134.196.182:8088/ # yarn管理界面\n```\n\n[Hadoop2.7.3在centos7上的单机版安装部署](https://blog.csdn.net/ytangdigl/article/details/109131492)\n\n### 四、HBASE\n\n```shell\n# hbase.apache.org/downloads.html\n# hbase-2.2.6-bin.tar.gz\nwget https://archive.apache.org/dist/hbase/2.2.6/hbase-2.2.6-bin.tar.gz\ntar zxvf hbase-2.2.6-bin.tar.gz\nmkdir -p /usr/local/hbase/hbase-2.2.6/hbase/\nmkdir -p /usr/local/hbase/hbase-2.2.6/zookeeper/\nvim /usr/local/hbase-2.2.6/conf/hbase-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64\nvim /usr/local/hbase-2.2.6/conf/hbase-site.xml\n<configuration>\n  <!-- hbase存放数据目录 /data/soft/hbase-2.2.6/hbase为自定义地址 -->\n  <property>\n    <name>hbase.rootdir</name>\n    <value>file:///data/soft/hbase-2.2.6/hbase</value>\n  </property>\n  <!-- ZooKeeper数据文件路径 -->\n  <property>\n    <name>hbase.zookeeper.property.dataDir</name>\n    <value>/usr/hbase/hbase-2.2.6/zookeeper</value>\n  </property>\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n    <description>\n      Controls whether HBase will check for stream capabilities (hflush/hsync).\n      Disable this if you intend to run on LocalFileSystem, denoted by a rootdir\n      with the 'file://' scheme, but be mindful of the NOTE below.\n      WARNING: Setting this to false blinds you to potential data loss and\n      inconsistent system state in the event of process and/or node failures. If\n      HBase is complaining of an inability to use hsync or hflush it's most\n      likely not a false positive.\n    </description>\n  </property>\n</configuration>\nexport HBASE_HOME=/usr/local/hbase-2.2.6\nexport PATH=$HBASE_HOME/bin:$PATH   \n# 启动hbase\n./bin/start-hbase.sh\n# 停止hbase\n./bin/stop-hbase.sh\n# http://9.134.196.182:16010/master-status\n# 连接\n./bin/hbase shell\nhbase(main):002:0> create_namespace 'test'\nTook 0.1420 seconds                                                             \nhbase(main):003:0> create 'test:wednesday', 'cf'\nCreated table test:wednesday\nTook 0.7373 seconds                                                             \n=> Hbase::Table - test:wednesday\nhbase(main):004:0> list 'test:wednesday'\nTABLE                                                                           \ntest:wednesday                                                                  \n1 row(s)\nTook 0.0191 seconds                                                             \n=> [\"test:wednesday\"]\nhbase(main):005:0> desc 'test:wednesday'\nTable test:wednesday is ENABLED                                                 \ntest:wednesday                                                                  \nCOLUMN FAMILIES DESCRIPTION                                                     \n{NAME => 'cf', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BE\nHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false'\n, DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICAT\nION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMO\nRY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'fal\nse', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}         \n1 row(s)\nQUOTAS                                                                          \n0 row(s)\nTook 0.1878 seconds                                                             \nhbase(main):007:0> put 'test:wednesday','r1','cf:k1','v1'\nTook 0.0056 seconds                                                             \nhbase(main):008:0> put 'test:wednesday','r2','cf:k2','v2'\nTook 0.0038 seconds                                                             \nhbase(main):009:0> put 'test:wednesday','r3','cf:k3','v3'\nTook 0.0038 seconds                                                             \nhbase(main):010:0> scan 'test:wednesday'\nROW                   COLUMN+CELL                                               \n r1                   column=cf:k1, timestamp=1644406717928, value=v1           \n r2                   column=cf:k2, timestamp=1644406748969, value=v2           \n r3                   column=cf:k3, timestamp=1644406756410, value=v3           \n3 row(s)\nTook 0.0238 seconds  \nhbase(main):011:0> get 'test:wednesday','r1'\nCOLUMN                CELL                                                      \n cf:k1                timestamp=1644406717928, value=v1                         \n1 row(s)\nTook 0.0108 seconds  \n### 补充 预分区填的是间隔\ncreate 'xxx',{NAME =>'cf',VERSIONS=>1,COMPRESSION=>'snappy',TTL => 500},{SPLITS => ['1','2','3','4','5','6']}\n```\n\n[HBase单机版安装部署](https://blog.csdn.net/ytangdigl/article/details/109139682)\n\n### 五、CLICKHOUSE\n\n### 六、HIVE\n\n### 七、ELASTICSEARCH\n\n### 八、LOGTSASH\n\n### Q&A\n\n> jar包不包含scala文件？\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>org.example</groupId>\n  <artifactId>wednesday</artifactId>\n  <version>1.0-SNAPSHOT</version>\n\n  <name>wednesday</name>\n  <!-- FIXME change it to the project's website -->\n  <url>http://www.example.com</url>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <maven.compiler.source>1.7</maven.compiler.source>\n    <maven.compiler.target>1.7</maven.compiler.target>\n    <scala.version>2.12.8</scala.version>\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.11</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.scala-lang</groupId>\n      <artifactId>scala-library</artifactId>\n      <version>${scala.version}</version>\n      <!--<scope>provided</scope>--><!--如果要java -jar运行,需要注释掉这里-->\n    </dependency>\n  </dependencies>\n  <build>\n    <finalName>DEMO</finalName>\n    <plugins>\n      <plugin>\n        <!--用这个插件来将依赖一块打进jar包-->\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-assembly-plugin</artifactId>\n        <version>3.0.0</version>\n        <executions>\n          <execution>\n            <id>make-assembly</id>\n            <phase>package</phase>\n            <goals>\n              <goal>single</goal>\n            </goals>\n          </execution>\n        </executions>\n        <configuration>\n          <archive>\n            <manifest>\n              <mainClass>com.example.ScalaApp</mainClass>\n            </manifest>\n          </archive>\n          <descriptorRefs>\n            <descriptorRef>jar-with-dependencies</descriptorRef>\n          </descriptorRefs>\n        </configuration>\n      </plugin>\n      <plugin>\n        <!--scala原始在sbt(类似java maven)上做开发,现可以用这个插件来在maven中进行开发-->\n        <groupId>org.scala-tools</groupId>\n        <artifactId>maven-scala-plugin</artifactId>\n        <version>2.15.2</version>\n        <executions>\n          <execution>\n            <id>scala-compile-first</id>\n            <goals>\n              <goal>compile</goal>\n            </goals>\n            <configuration>\n              <includes>\n                <include>**/*.scala</include>\n              </includes>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n#### 运行\njava -jar DEMO-jar-with-dependencies.jar # 已经指定main class了\nscala DEMO-jar-with-dependencies.jar\n\n```\n\n","source":"_posts/big-data.md","raw":"---\ntitle: 大数据组件\ndate: 2022-02-08 16:54:22\ntags: 数据\n---\n\n> 大数据相关整理\n\n<!-- more -->\n\n### 一、KAFKA\n\n#### 1、特性\n\n- 持久性：文件性存储，日志文件存储消息，达到阈值写磁盘，减少磁盘i/o，如果宕机会丢数据\n- 高吞吐：普通机器百万qps\n- 支持通过kafka服务器和消费机集群分区消息？\n- 支持hadoop并行数据加载\n\n#### 2、术语\n\n- Broker：消息中间处理节点，kafka节点=broker，一个或多个broker组成kafka集群\n- Topic：kafka根据topic归类消息\n- Producer：生产者\n- Consumer：消费者\n- ConsumerGroup：消费组\n- Partition：物理概念，一个topic分多个partition，partition内部有序\n\n#### 3、其他\n\n​\tpartition存储层面是append log文件，追加log文件尾部，offset标记消息在文件位置，offset是long数字，顺序写磁盘效率高于随机写内存，保证高吞吐\n\n#### 4、部署\n\n```shell\n[root@kafka ~]# wget https://archive.apache.org/dist/kafka/2.2.1/kafka_2.11-2.2.1.tgz\n[root@kafka ~]# tar zxf kafka_2.11-2.2.1.tgz \n[root@kafka ~]# mv kafka_2.11-2.2.1/ /usr/local/kafka\n[root@kafka ~]# cd /usr/local/kafka/bin/\n#启动zookeeper\n[root@kafka bin]# ./zookeeper-server-start.sh ../config/zookeeper.properties &\n#启动kafka\n[root@kafka bin]# ./kafka-server-start.sh ../config/server.properties &\n#查看端口是否在监听\n[root@kafka bin]# netstat -anput | grep 9092\ntcp6       0      0 :::9092                 :::*                    LISTEN      43326/java          \ntcp6       0      0 192.168.171.134:44388   192.168.171.134:9092    ESTABLISHED 43326/java          \ntcp6       0      0 192.168.171.134:9092    192.168.171.134:44388   ESTABLISHED 43326/java   \n#在本机创建kafka，副本数量为1，分区数量为1\n[root@kafka bin]# ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n#查看本机的topic\n[root@kafka bin]# ./kafka-topics.sh --list --bootstrap-server localhost:9092\ntest\n#发送消息到test\n[root@kafka bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test\n>aaa\n>bbb\n>ccc\n#开启新的终端，进行读取消息测试，“--from-beginning”表示从开头读取\n[root@kafka bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\naaa\nbbb\nccc\n```\n\n[kafka单机部署](https://cloud.tencent.com/developer/article/1624740)\n\n### 二、FLINK\n\n```shell\n# https://flink.apache.org/zh/downloads.html\n# scala2.12 flink-1.7.1\ntar zxvf flink-1.7.1-bin-scala_2.12.tgz\n# scala 2.11 flink-1.8.0\n# flink --version\ncd flink-1.7.1\n./bin/start-cluster.sh\n# http://9.134.196.182:8081\nwget https://streaming-with-flink.github.io/examples/download/examples-scala.jar\n./bin/flink run -c io.github.streamingwithflink.chapter1.AverageSensorReadings examples-scala.jar\n./log 日志\nweb ui cancel取消任务\n./bin/stop-cluster.sh \n\n\n# 依赖\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-java</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-streaming-java_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\n<dependency>\n  <groupId>org.apache.flink</groupId>\n  <artifactId>flink-clients_2.11</artifactId>\n  <version>1.14.3</version>\n</dependency>\nScala API: 为了使用 Scala API，将 flink-java 的 artifact id 替换为 flink-scala_2.11，同时将 flink-streaming-java_2.11 替换为 flink-streaming-scala_2.11。\n```\n\n[官网](https://flink.apache.org/)\n\n### 三、HADOOP\n\n```shell\n# cat /etc/profile\n# export JAVA_HOME=/usr/java/jdk1.8.0_231\n# export HADOOP_HOME=/usr/local/hadoop/hadoop-2.7.3 # 需要重启命令行才生效\n[root@VM-196-182-centos /usr/local]# java -version\nopenjdk version \"1.8.0_71\"\nOpenJDK Runtime Environment (build 1.8.0_71-b15)\nOpenJDK 64-Bit Server VM (build 25.71-b15, mixed mode)\n[root@VM-196-182-centos /usr/local]# scala -version\nScala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL\n[root@VM-196-182-centos /usr/local]# hadoop version\n# hadoop-2.7.3.tar.gz\n# https://archive.apache.org/dist/hadoop/common/\nwget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\ntar zxvf hadoop-2.7.3.tar.gz \nmkdir -p /usr/local/hadoop/tmp/ # 指定hadoop运行时产生文件的存储路径 \nmkdir -p /usr/local/hadoop/hdfs/  \nmkdir -p /usr/local/hadoop/hdfs/data/ # datanode上数据块的物理存储位置\nmkdir -p /usr/local//hadoop/hdfs/name/ # namenode上存储hdfs名字空间元数据\nmv hadoop-2.7.3 /usr/local/hadoop\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hadoop-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64 # JAVA_HOME环境变量在这不生效 这里要显示声明一次\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml\n<configuration>\n    <!-- 指定HDFS老大（namenode）的通信地址 -->\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://9.134.196.182:9000</value>\n    </property>\n    <!-- 指定hadoop运行时产生文件的存储路径 -->\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/usr/local/hadoop/tmp</value>\n    </property>\n</configuration>\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml\n<configuration>\n    <property>\n        <name>dfs.name.dir</name>\n        <value>/usr/local/hadoop/hdfs/name</value>\n        <description>namenode上存储hdfs名字空间元数据 </description> \n    </property>\n    <property>\n        <name>dfs.data.dir</name>\n        <value>/usr/local/hadoop/hdfs/data</value>\n        <description>datanode上数据块的物理存储位置</description>\n    </property>\n    <!-- 设置hdfs副本数量 -->\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n# 配置yarn（非必需）\nmv mapred-site.xml.template mapred-site.xml\nvim mapred-site.xml\n<configuration>\n    <!-- 通知框架MR使用YARN -->\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n</configuration>\nvim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/yarn-site.xml\n<configuration>\n    <!-- reducer取数据的方式是mapreduce_shuffle -->\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n</configuration>\n# 格式化hdfs\ncd /usr/local/hadoop/hadoop-2.7.3\n./bin/hdfs namenode -format\n# 启动hdfs\n./sbin/start-dfs.sh // 需要输入本机密码 todo:配置免密\n# 停止hdfs\n./sbin/stop-dfs.sh // 需要输入本机密码 todo:配置免密\n# http://9.134.196.182:50070/ # hadoop管理界面\n# 启动yarn\n./sbin/start-yarn.sh // 需要输入本机密码 todo:配置免密\n# 停止yarn\n./sbin/stop-yarn.sh // 需要输入本机密码 todo:配置免密\n# http://9.134.196.182:8088/ # yarn管理界面\n```\n\n[Hadoop2.7.3在centos7上的单机版安装部署](https://blog.csdn.net/ytangdigl/article/details/109131492)\n\n### 四、HBASE\n\n```shell\n# hbase.apache.org/downloads.html\n# hbase-2.2.6-bin.tar.gz\nwget https://archive.apache.org/dist/hbase/2.2.6/hbase-2.2.6-bin.tar.gz\ntar zxvf hbase-2.2.6-bin.tar.gz\nmkdir -p /usr/local/hbase/hbase-2.2.6/hbase/\nmkdir -p /usr/local/hbase/hbase-2.2.6/zookeeper/\nvim /usr/local/hbase-2.2.6/conf/hbase-env.sh\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64\nvim /usr/local/hbase-2.2.6/conf/hbase-site.xml\n<configuration>\n  <!-- hbase存放数据目录 /data/soft/hbase-2.2.6/hbase为自定义地址 -->\n  <property>\n    <name>hbase.rootdir</name>\n    <value>file:///data/soft/hbase-2.2.6/hbase</value>\n  </property>\n  <!-- ZooKeeper数据文件路径 -->\n  <property>\n    <name>hbase.zookeeper.property.dataDir</name>\n    <value>/usr/hbase/hbase-2.2.6/zookeeper</value>\n  </property>\n  <property>\n    <name>hbase.unsafe.stream.capability.enforce</name>\n    <value>false</value>\n    <description>\n      Controls whether HBase will check for stream capabilities (hflush/hsync).\n      Disable this if you intend to run on LocalFileSystem, denoted by a rootdir\n      with the 'file://' scheme, but be mindful of the NOTE below.\n      WARNING: Setting this to false blinds you to potential data loss and\n      inconsistent system state in the event of process and/or node failures. If\n      HBase is complaining of an inability to use hsync or hflush it's most\n      likely not a false positive.\n    </description>\n  </property>\n</configuration>\nexport HBASE_HOME=/usr/local/hbase-2.2.6\nexport PATH=$HBASE_HOME/bin:$PATH   \n# 启动hbase\n./bin/start-hbase.sh\n# 停止hbase\n./bin/stop-hbase.sh\n# http://9.134.196.182:16010/master-status\n# 连接\n./bin/hbase shell\nhbase(main):002:0> create_namespace 'test'\nTook 0.1420 seconds                                                             \nhbase(main):003:0> create 'test:wednesday', 'cf'\nCreated table test:wednesday\nTook 0.7373 seconds                                                             \n=> Hbase::Table - test:wednesday\nhbase(main):004:0> list 'test:wednesday'\nTABLE                                                                           \ntest:wednesday                                                                  \n1 row(s)\nTook 0.0191 seconds                                                             \n=> [\"test:wednesday\"]\nhbase(main):005:0> desc 'test:wednesday'\nTable test:wednesday is ENABLED                                                 \ntest:wednesday                                                                  \nCOLUMN FAMILIES DESCRIPTION                                                     \n{NAME => 'cf', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BE\nHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false'\n, DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICAT\nION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMO\nRY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'fal\nse', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}         \n1 row(s)\nQUOTAS                                                                          \n0 row(s)\nTook 0.1878 seconds                                                             \nhbase(main):007:0> put 'test:wednesday','r1','cf:k1','v1'\nTook 0.0056 seconds                                                             \nhbase(main):008:0> put 'test:wednesday','r2','cf:k2','v2'\nTook 0.0038 seconds                                                             \nhbase(main):009:0> put 'test:wednesday','r3','cf:k3','v3'\nTook 0.0038 seconds                                                             \nhbase(main):010:0> scan 'test:wednesday'\nROW                   COLUMN+CELL                                               \n r1                   column=cf:k1, timestamp=1644406717928, value=v1           \n r2                   column=cf:k2, timestamp=1644406748969, value=v2           \n r3                   column=cf:k3, timestamp=1644406756410, value=v3           \n3 row(s)\nTook 0.0238 seconds  \nhbase(main):011:0> get 'test:wednesday','r1'\nCOLUMN                CELL                                                      \n cf:k1                timestamp=1644406717928, value=v1                         \n1 row(s)\nTook 0.0108 seconds  \n### 补充 预分区填的是间隔\ncreate 'xxx',{NAME =>'cf',VERSIONS=>1,COMPRESSION=>'snappy',TTL => 500},{SPLITS => ['1','2','3','4','5','6']}\n```\n\n[HBase单机版安装部署](https://blog.csdn.net/ytangdigl/article/details/109139682)\n\n### 五、CLICKHOUSE\n\n### 六、HIVE\n\n### 七、ELASTICSEARCH\n\n### 八、LOGTSASH\n\n### Q&A\n\n> jar包不包含scala文件？\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>org.example</groupId>\n  <artifactId>wednesday</artifactId>\n  <version>1.0-SNAPSHOT</version>\n\n  <name>wednesday</name>\n  <!-- FIXME change it to the project's website -->\n  <url>http://www.example.com</url>\n\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <maven.compiler.source>1.7</maven.compiler.source>\n    <maven.compiler.target>1.7</maven.compiler.target>\n    <scala.version>2.12.8</scala.version>\n  </properties>\n\n  <dependencies>\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.11</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.scala-lang</groupId>\n      <artifactId>scala-library</artifactId>\n      <version>${scala.version}</version>\n      <!--<scope>provided</scope>--><!--如果要java -jar运行,需要注释掉这里-->\n    </dependency>\n  </dependencies>\n  <build>\n    <finalName>DEMO</finalName>\n    <plugins>\n      <plugin>\n        <!--用这个插件来将依赖一块打进jar包-->\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-assembly-plugin</artifactId>\n        <version>3.0.0</version>\n        <executions>\n          <execution>\n            <id>make-assembly</id>\n            <phase>package</phase>\n            <goals>\n              <goal>single</goal>\n            </goals>\n          </execution>\n        </executions>\n        <configuration>\n          <archive>\n            <manifest>\n              <mainClass>com.example.ScalaApp</mainClass>\n            </manifest>\n          </archive>\n          <descriptorRefs>\n            <descriptorRef>jar-with-dependencies</descriptorRef>\n          </descriptorRefs>\n        </configuration>\n      </plugin>\n      <plugin>\n        <!--scala原始在sbt(类似java maven)上做开发,现可以用这个插件来在maven中进行开发-->\n        <groupId>org.scala-tools</groupId>\n        <artifactId>maven-scala-plugin</artifactId>\n        <version>2.15.2</version>\n        <executions>\n          <execution>\n            <id>scala-compile-first</id>\n            <goals>\n              <goal>compile</goal>\n            </goals>\n            <configuration>\n              <includes>\n                <include>**/*.scala</include>\n              </includes>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n#### 运行\njava -jar DEMO-jar-with-dependencies.jar # 已经指定main class了\nscala DEMO-jar-with-dependencies.jar\n\n```\n\n","slug":"big-data","published":1,"updated":"2022-12-19T01:30:52.668Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh6000316gn67gccpol","content":"<blockquote>\n<p>大数据相关整理</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"一、KAFKA\"><a href=\"#一、KAFKA\" class=\"headerlink\" title=\"一、KAFKA\"></a>一、KAFKA</h3><h4 id=\"1、特性\"><a href=\"#1、特性\" class=\"headerlink\" title=\"1、特性\"></a>1、特性</h4><ul>\n<li>持久性：文件性存储，日志文件存储消息，达到阈值写磁盘，减少磁盘i/o，如果宕机会丢数据</li>\n<li>高吞吐：普通机器百万qps</li>\n<li>支持通过kafka服务器和消费机集群分区消息？</li>\n<li>支持hadoop并行数据加载</li>\n</ul>\n<h4 id=\"2、术语\"><a href=\"#2、术语\" class=\"headerlink\" title=\"2、术语\"></a>2、术语</h4><ul>\n<li>Broker：消息中间处理节点，kafka节点=broker，一个或多个broker组成kafka集群</li>\n<li>Topic：kafka根据topic归类消息</li>\n<li>Producer：生产者</li>\n<li>Consumer：消费者</li>\n<li>ConsumerGroup：消费组</li>\n<li>Partition：物理概念，一个topic分多个partition，partition内部有序</li>\n</ul>\n<h4 id=\"3、其他\"><a href=\"#3、其他\" class=\"headerlink\" title=\"3、其他\"></a>3、其他</h4><p>​    partition存储层面是append log文件，追加log文件尾部，offset标记消息在文件位置，offset是long数字，顺序写磁盘效率高于随机写内存，保证高吞吐</p>\n<h4 id=\"4、部署\"><a href=\"#4、部署\" class=\"headerlink\" title=\"4、部署\"></a>4、部署</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@kafka ~]# wget https://archive.apache.org/dist/kafka/2.2.1/kafka_2.11-2.2.1.tgz</span><br><span class=\"line\">[root@kafka ~]# tar zxf kafka_2.11-2.2.1.tgz </span><br><span class=\"line\">[root@kafka ~]# mv kafka_2.11-2.2.1/ /usr/local/kafka</span><br><span class=\"line\">[root@kafka ~]# cd /usr/local/kafka/bin/</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">启动zookeeper</span></span><br><span class=\"line\">[root@kafka bin]# ./zookeeper-server-start.sh ../config/zookeeper.properties &amp;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">启动kafka</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-server-start.sh ../config/server.properties &amp;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">查看端口是否在监听</span></span><br><span class=\"line\">[root@kafka bin]# netstat -anput | grep 9092</span><br><span class=\"line\">tcp6       0      0 :::9092                 :::*                    LISTEN      43326/java          </span><br><span class=\"line\">tcp6       0      0 192.168.171.134:44388   192.168.171.134:9092    ESTABLISHED 43326/java          </span><br><span class=\"line\">tcp6       0      0 192.168.171.134:9092    192.168.171.134:44388   ESTABLISHED 43326/java   </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">在本机创建kafka，副本数量为1，分区数量为1</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">查看本机的topic</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-topics.sh --list --bootstrap-server localhost:9092</span><br><span class=\"line\">test</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">发送消息到<span class=\"built_in\">test</span></span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">aaa</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">bbb</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">ccc</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">开启新的终端，进行读取消息测试，“--from-beginning”表示从开头读取</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class=\"line\">aaa</span><br><span class=\"line\">bbb</span><br><span class=\"line\">ccc</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://cloud.tencent.com/developer/article/1624740\">kafka单机部署</a></p>\n<h3 id=\"二、FLINK\"><a href=\"#二、FLINK\" class=\"headerlink\" title=\"二、FLINK\"></a>二、FLINK</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> https://flink.apache.org/zh/downloads.html</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> scala2.12 flink-1.7.1</span></span><br><span class=\"line\">tar zxvf flink-1.7.1-bin-scala_2.12.tgz</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> scala 2.11 flink-1.8.0</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> flink --version</span></span><br><span class=\"line\">cd flink-1.7.1</span><br><span class=\"line\">./bin/start-cluster.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:8081</span></span><br><span class=\"line\">wget https://streaming-with-flink.github.io/examples/download/examples-scala.jar</span><br><span class=\"line\">./bin/flink run -c io.github.streamingwithflink.chapter1.AverageSensorReadings examples-scala.jar</span><br><span class=\"line\">./log 日志</span><br><span class=\"line\">web ui cancel取消任务</span><br><span class=\"line\">./bin/stop-cluster.sh </span><br><span class=\"line\"><span class=\"meta\"></span></span><br><span class=\"line\"><span class=\"meta\"></span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 依赖</span></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">Scala API: 为了使用 Scala API，将 flink-java 的 artifact id 替换为 flink-scala_2.11，同时将 flink-streaming-java_2.11 替换为 flink-streaming-scala_2.11。</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://flink.apache.org/\">官网</a></p>\n<h3 id=\"三、HADOOP\"><a href=\"#三、HADOOP\" class=\"headerlink\" title=\"三、HADOOP\"></a>三、HADOOP</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> cat /etc/profile</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> <span class=\"built_in\">export</span> JAVA_HOME=/usr/java/jdk1.8.0_231</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> <span class=\"built_in\">export</span> HADOOP_HOME=/usr/<span class=\"built_in\">local</span>/hadoop/hadoop-2.7.3 <span class=\"comment\"># 需要重启命令行才生效</span></span></span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# java -version</span><br><span class=\"line\">openjdk version &quot;1.8.0_71&quot;</span><br><span class=\"line\">OpenJDK Runtime Environment (build 1.8.0_71-b15)</span><br><span class=\"line\">OpenJDK 64-Bit Server VM (build 25.71-b15, mixed mode)</span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# scala -version</span><br><span class=\"line\">Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL</span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# hadoop version</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hadoop-2.7.3.tar.gz</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> https://archive.apache.org/dist/hadoop/common/</span></span><br><span class=\"line\">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">tar zxvf hadoop-2.7.3.tar.gz </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/tmp/ # 指定hadoop运行时产生文件的存储路径 </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/hdfs/  </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/hdfs/data/ # datanode上数据块的物理存储位置</span><br><span class=\"line\">mkdir -p /usr/local//hadoop/hdfs/name/ # namenode上存储hdfs名字空间元数据</span><br><span class=\"line\">mv hadoop-2.7.3 /usr/local/hadoop</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hadoop-env.sh</span><br><span class=\"line\">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64 # JAVA_HOME环境变量在这不生效 这里要显示声明一次</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 指定HDFS老大（namenode）的通信地址 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;hdfs://9.134.196.182:9000&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/hdfs/name&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;namenode上存储hdfs名字空间元数据 &lt;/description&gt; </span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/hdfs/data&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;datanode上数据块的物理存储位置&lt;/description&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;!-- 设置hdfs副本数量 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 配置yarn（非必需）</span></span><br><span class=\"line\">mv mapred-site.xml.template mapred-site.xml</span><br><span class=\"line\">vim mapred-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 通知框架MR使用YARN --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/yarn-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 格式化hdfs</span></span><br><span class=\"line\">cd /usr/local/hadoop/hadoop-2.7.3</span><br><span class=\"line\">./bin/hdfs namenode -format</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动hdfs</span></span><br><span class=\"line\">./sbin/start-dfs.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止hdfs</span></span><br><span class=\"line\">./sbin/stop-dfs.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:50070/ <span class=\"comment\"># hadoop管理界面</span></span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动yarn</span></span><br><span class=\"line\">./sbin/start-yarn.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止yarn</span></span><br><span class=\"line\">./sbin/stop-yarn.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:8088/ <span class=\"comment\"># yarn管理界面</span></span></span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/ytangdigl/article/details/109131492\">Hadoop2.7.3在centos7上的单机版安装部署</a></p>\n<h3 id=\"四、HBASE\"><a href=\"#四、HBASE\" class=\"headerlink\" title=\"四、HBASE\"></a>四、HBASE</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hbase.apache.org/downloads.html</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hbase-2.2.6-bin.tar.gz</span></span><br><span class=\"line\">wget https://archive.apache.org/dist/hbase/2.2.6/hbase-2.2.6-bin.tar.gz</span><br><span class=\"line\">tar zxvf hbase-2.2.6-bin.tar.gz</span><br><span class=\"line\">mkdir -p /usr/local/hbase/hbase-2.2.6/hbase/</span><br><span class=\"line\">mkdir -p /usr/local/hbase/hbase-2.2.6/zookeeper/</span><br><span class=\"line\">vim /usr/local/hbase-2.2.6/conf/hbase-env.sh</span><br><span class=\"line\">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64</span><br><span class=\"line\">vim /usr/local/hbase-2.2.6/conf/hbase-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">  &lt;!-- hbase存放数据目录 /data/soft/hbase-2.2.6/hbase为自定义地址 --&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;file:///data/soft/hbase-2.2.6/hbase&lt;/value&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;!-- ZooKeeper数据文件路径 --&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/usr/hbase/hbase-2.2.6/zookeeper&lt;/value&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;false&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Controls whether HBase will check for stream capabilities (hflush/hsync).</span><br><span class=\"line\">      Disable this if you intend to run on LocalFileSystem, denoted by a rootdir</span><br><span class=\"line\">      with the &#x27;file://&#x27; scheme, but be mindful of the NOTE below.</span><br><span class=\"line\">      WARNING: Setting this to false blinds you to potential data loss and</span><br><span class=\"line\">      inconsistent system state in the event of process and/or node failures. If</span><br><span class=\"line\">      HBase is complaining of an inability to use hsync or hflush it&#x27;s most</span><br><span class=\"line\">      likely not a false positive.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">export HBASE_HOME=/usr/local/hbase-2.2.6</span><br><span class=\"line\">export PATH=$HBASE_HOME/bin:$PATH   </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动hbase</span></span><br><span class=\"line\">./bin/start-hbase.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止hbase</span></span><br><span class=\"line\">./bin/stop-hbase.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:16010/master-status</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 连接</span></span><br><span class=\"line\">./bin/hbase shell</span><br><span class=\"line\">hbase(main):002:0&gt; create_namespace &#x27;test&#x27;</span><br><span class=\"line\">Took 0.1420 seconds                                                             </span><br><span class=\"line\">hbase(main):003:0&gt; create &#x27;test:wednesday&#x27;, &#x27;cf&#x27;</span><br><span class=\"line\">Created table test:wednesday</span><br><span class=\"line\">Took 0.7373 seconds                                                             </span><br><span class=\"line\">=&gt; Hbase::Table - test:wednesday</span><br><span class=\"line\">hbase(main):004:0&gt; list &#x27;test:wednesday&#x27;</span><br><span class=\"line\">TABLE                                                                           </span><br><span class=\"line\">test:wednesday                                                                  </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">Took 0.0191 seconds                                                             </span><br><span class=\"line\">=&gt; [&quot;test:wednesday&quot;]</span><br><span class=\"line\">hbase(main):005:0&gt; desc &#x27;test:wednesday&#x27;</span><br><span class=\"line\">Table test:wednesday is ENABLED                                                 </span><br><span class=\"line\">test:wednesday                                                                  </span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION                                                     </span><br><span class=\"line\">&#123;NAME =&gt; &#x27;cf&#x27;, VERSIONS =&gt; &#x27;1&#x27;, EVICT_BLOCKS_ON_CLOSE =&gt; &#x27;false&#x27;, NEW_VERSION_BE</span><br><span class=\"line\">HAVIOR =&gt; &#x27;false&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FALSE&#x27;, CACHE_DATA_ON_WRITE =&gt; &#x27;false&#x27;</span><br><span class=\"line\">, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, REPLICAT</span><br><span class=\"line\">ION_SCOPE =&gt; &#x27;0&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, CACHE_INDEX_ON_WRITE =&gt; &#x27;false&#x27;, IN_MEMO</span><br><span class=\"line\">RY =&gt; &#x27;false&#x27;, CACHE_BLOOMS_ON_WRITE =&gt; &#x27;false&#x27;, PREFETCH_BLOCKS_ON_OPEN =&gt; &#x27;fal</span><br><span class=\"line\">se&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, BLOCKCACHE =&gt; &#x27;true&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;&#125;         </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">QUOTAS                                                                          </span><br><span class=\"line\">0 row(s)</span><br><span class=\"line\">Took 0.1878 seconds                                                             </span><br><span class=\"line\">hbase(main):007:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r1&#x27;,&#x27;cf:k1&#x27;,&#x27;v1&#x27;</span><br><span class=\"line\">Took 0.0056 seconds                                                             </span><br><span class=\"line\">hbase(main):008:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r2&#x27;,&#x27;cf:k2&#x27;,&#x27;v2&#x27;</span><br><span class=\"line\">Took 0.0038 seconds                                                             </span><br><span class=\"line\">hbase(main):009:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r3&#x27;,&#x27;cf:k3&#x27;,&#x27;v3&#x27;</span><br><span class=\"line\">Took 0.0038 seconds                                                             </span><br><span class=\"line\">hbase(main):010:0&gt; scan &#x27;test:wednesday&#x27;</span><br><span class=\"line\">ROW                   COLUMN+CELL                                               </span><br><span class=\"line\"> r1                   column=cf:k1, timestamp=1644406717928, value=v1           </span><br><span class=\"line\"> r2                   column=cf:k2, timestamp=1644406748969, value=v2           </span><br><span class=\"line\"> r3                   column=cf:k3, timestamp=1644406756410, value=v3           </span><br><span class=\"line\">3 row(s)</span><br><span class=\"line\">Took 0.0238 seconds  </span><br><span class=\"line\">hbase(main):011:0&gt; get &#x27;test:wednesday&#x27;,&#x27;r1&#x27;</span><br><span class=\"line\">COLUMN                CELL                                                      </span><br><span class=\"line\"> cf:k1                timestamp=1644406717928, value=v1                         </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">Took 0.0108 seconds  </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"><span class=\"comment\">## 补充 预分区填的是间隔</span></span></span><br><span class=\"line\">create &#x27;xxx&#x27;,&#123;NAME =&gt;&#x27;cf&#x27;,VERSIONS=&gt;1,COMPRESSION=&gt;&#x27;snappy&#x27;,TTL =&gt; 500&#125;,&#123;SPLITS =&gt; [&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;,&#x27;6&#x27;]&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/ytangdigl/article/details/109139682\">HBase单机版安装部署</a></p>\n<h3 id=\"五、CLICKHOUSE\"><a href=\"#五、CLICKHOUSE\" class=\"headerlink\" title=\"五、CLICKHOUSE\"></a>五、CLICKHOUSE</h3><h3 id=\"六、HIVE\"><a href=\"#六、HIVE\" class=\"headerlink\" title=\"六、HIVE\"></a>六、HIVE</h3><h3 id=\"七、ELASTICSEARCH\"><a href=\"#七、ELASTICSEARCH\" class=\"headerlink\" title=\"七、ELASTICSEARCH\"></a>七、ELASTICSEARCH</h3><h3 id=\"八、LOGTSASH\"><a href=\"#八、LOGTSASH\" class=\"headerlink\" title=\"八、LOGTSASH\"></a>八、LOGTSASH</h3><h3 id=\"Q-amp-A\"><a href=\"#Q-amp-A\" class=\"headerlink\" title=\"Q&amp;A\"></a>Q&amp;A</h3><blockquote>\n<p>jar包不包含scala文件？</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class=\"line\">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;groupId&gt;org.example&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;wednesday&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;name&gt;wednesday&lt;/name&gt;</span><br><span class=\"line\">  &lt;!-- FIXME change it to the project&#x27;s website --&gt;</span><br><span class=\"line\">  &lt;url&gt;http://www.example.com&lt;/url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;properties&gt;</span><br><span class=\"line\">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class=\"line\">    &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;</span><br><span class=\"line\">    &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;</span><br><span class=\"line\">    &lt;scala.version&gt;2.12.8&lt;/scala.version&gt;</span><br><span class=\"line\">  &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;dependencies&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;4.11&lt;/version&gt;</span><br><span class=\"line\">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class=\"line\">      &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;!--如果要java -jar运行,需要注释掉这里--&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\">  &lt;/dependencies&gt;</span><br><span class=\"line\">  &lt;build&gt;</span><br><span class=\"line\">    &lt;finalName&gt;DEMO&lt;/finalName&gt;</span><br><span class=\"line\">    &lt;plugins&gt;</span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">        &lt;!--用这个插件来将依赖一块打进jar包--&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;3.0.0&lt;/version&gt;</span><br><span class=\"line\">        &lt;executions&gt;</span><br><span class=\"line\">          &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class=\"line\">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">              &lt;goal&gt;single&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">          &lt;/execution&gt;</span><br><span class=\"line\">        &lt;/executions&gt;</span><br><span class=\"line\">        &lt;configuration&gt;</span><br><span class=\"line\">          &lt;archive&gt;</span><br><span class=\"line\">            &lt;manifest&gt;</span><br><span class=\"line\">              &lt;mainClass&gt;com.example.ScalaApp&lt;/mainClass&gt;</span><br><span class=\"line\">            &lt;/manifest&gt;</span><br><span class=\"line\">          &lt;/archive&gt;</span><br><span class=\"line\">          &lt;descriptorRefs&gt;</span><br><span class=\"line\">            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class=\"line\">          &lt;/descriptorRefs&gt;</span><br><span class=\"line\">        &lt;/configuration&gt;</span><br><span class=\"line\">      &lt;/plugin&gt;</span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">        &lt;!--scala原始在sbt(类似java maven)上做开发,现可以用这个插件来在maven中进行开发--&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;2.15.2&lt;/version&gt;</span><br><span class=\"line\">        &lt;executions&gt;</span><br><span class=\"line\">          &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;scala-compile-first&lt;/id&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">              &lt;goal&gt;compile&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">            &lt;configuration&gt;</span><br><span class=\"line\">              &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;**/*.scala&lt;/include&gt;</span><br><span class=\"line\">              &lt;/includes&gt;</span><br><span class=\"line\">            &lt;/configuration&gt;</span><br><span class=\"line\">          &lt;/execution&gt;</span><br><span class=\"line\">        &lt;/executions&gt;</span><br><span class=\"line\">      &lt;/plugin&gt;</span><br><span class=\"line\">    &lt;/plugins&gt;</span><br><span class=\"line\">  &lt;/build&gt;</span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\">#### 运行</span><br><span class=\"line\">java -jar DEMO-jar-with-dependencies.jar # 已经指定main class了</span><br><span class=\"line\">scala DEMO-jar-with-dependencies.jar</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>大数据相关整理</p>\n</blockquote>","more":"<h3 id=\"一、KAFKA\"><a href=\"#一、KAFKA\" class=\"headerlink\" title=\"一、KAFKA\"></a>一、KAFKA</h3><h4 id=\"1、特性\"><a href=\"#1、特性\" class=\"headerlink\" title=\"1、特性\"></a>1、特性</h4><ul>\n<li>持久性：文件性存储，日志文件存储消息，达到阈值写磁盘，减少磁盘i/o，如果宕机会丢数据</li>\n<li>高吞吐：普通机器百万qps</li>\n<li>支持通过kafka服务器和消费机集群分区消息？</li>\n<li>支持hadoop并行数据加载</li>\n</ul>\n<h4 id=\"2、术语\"><a href=\"#2、术语\" class=\"headerlink\" title=\"2、术语\"></a>2、术语</h4><ul>\n<li>Broker：消息中间处理节点，kafka节点=broker，一个或多个broker组成kafka集群</li>\n<li>Topic：kafka根据topic归类消息</li>\n<li>Producer：生产者</li>\n<li>Consumer：消费者</li>\n<li>ConsumerGroup：消费组</li>\n<li>Partition：物理概念，一个topic分多个partition，partition内部有序</li>\n</ul>\n<h4 id=\"3、其他\"><a href=\"#3、其他\" class=\"headerlink\" title=\"3、其他\"></a>3、其他</h4><p>​    partition存储层面是append log文件，追加log文件尾部，offset标记消息在文件位置，offset是long数字，顺序写磁盘效率高于随机写内存，保证高吞吐</p>\n<h4 id=\"4、部署\"><a href=\"#4、部署\" class=\"headerlink\" title=\"4、部署\"></a>4、部署</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[root@kafka ~]# wget https://archive.apache.org/dist/kafka/2.2.1/kafka_2.11-2.2.1.tgz</span><br><span class=\"line\">[root@kafka ~]# tar zxf kafka_2.11-2.2.1.tgz </span><br><span class=\"line\">[root@kafka ~]# mv kafka_2.11-2.2.1/ /usr/local/kafka</span><br><span class=\"line\">[root@kafka ~]# cd /usr/local/kafka/bin/</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">启动zookeeper</span></span><br><span class=\"line\">[root@kafka bin]# ./zookeeper-server-start.sh ../config/zookeeper.properties &amp;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">启动kafka</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-server-start.sh ../config/server.properties &amp;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">查看端口是否在监听</span></span><br><span class=\"line\">[root@kafka bin]# netstat -anput | grep 9092</span><br><span class=\"line\">tcp6       0      0 :::9092                 :::*                    LISTEN      43326/java          </span><br><span class=\"line\">tcp6       0      0 192.168.171.134:44388   192.168.171.134:9092    ESTABLISHED 43326/java          </span><br><span class=\"line\">tcp6       0      0 192.168.171.134:9092    192.168.171.134:44388   ESTABLISHED 43326/java   </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">在本机创建kafka，副本数量为1，分区数量为1</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">查看本机的topic</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-topics.sh --list --bootstrap-server localhost:9092</span><br><span class=\"line\">test</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">发送消息到<span class=\"built_in\">test</span></span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">aaa</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">bbb</span></span><br><span class=\"line\"><span class=\"meta\">&gt;</span><span class=\"bash\">ccc</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\">开启新的终端，进行读取消息测试，“--from-beginning”表示从开头读取</span></span><br><span class=\"line\">[root@kafka bin]# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class=\"line\">aaa</span><br><span class=\"line\">bbb</span><br><span class=\"line\">ccc</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://cloud.tencent.com/developer/article/1624740\">kafka单机部署</a></p>\n<h3 id=\"二、FLINK\"><a href=\"#二、FLINK\" class=\"headerlink\" title=\"二、FLINK\"></a>二、FLINK</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> https://flink.apache.org/zh/downloads.html</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> scala2.12 flink-1.7.1</span></span><br><span class=\"line\">tar zxvf flink-1.7.1-bin-scala_2.12.tgz</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> scala 2.11 flink-1.8.0</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> flink --version</span></span><br><span class=\"line\">cd flink-1.7.1</span><br><span class=\"line\">./bin/start-cluster.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:8081</span></span><br><span class=\"line\">wget https://streaming-with-flink.github.io/examples/download/examples-scala.jar</span><br><span class=\"line\">./bin/flink run -c io.github.streamingwithflink.chapter1.AverageSensorReadings examples-scala.jar</span><br><span class=\"line\">./log 日志</span><br><span class=\"line\">web ui cancel取消任务</span><br><span class=\"line\">./bin/stop-cluster.sh </span><br><span class=\"line\"><span class=\"meta\"></span></span><br><span class=\"line\"><span class=\"meta\"></span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 依赖</span></span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">&lt;dependency&gt;</span><br><span class=\"line\">  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.14.3&lt;/version&gt;</span><br><span class=\"line\">&lt;/dependency&gt;</span><br><span class=\"line\">Scala API: 为了使用 Scala API，将 flink-java 的 artifact id 替换为 flink-scala_2.11，同时将 flink-streaming-java_2.11 替换为 flink-streaming-scala_2.11。</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://flink.apache.org/\">官网</a></p>\n<h3 id=\"三、HADOOP\"><a href=\"#三、HADOOP\" class=\"headerlink\" title=\"三、HADOOP\"></a>三、HADOOP</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> cat /etc/profile</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> <span class=\"built_in\">export</span> JAVA_HOME=/usr/java/jdk1.8.0_231</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> <span class=\"built_in\">export</span> HADOOP_HOME=/usr/<span class=\"built_in\">local</span>/hadoop/hadoop-2.7.3 <span class=\"comment\"># 需要重启命令行才生效</span></span></span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# java -version</span><br><span class=\"line\">openjdk version &quot;1.8.0_71&quot;</span><br><span class=\"line\">OpenJDK Runtime Environment (build 1.8.0_71-b15)</span><br><span class=\"line\">OpenJDK 64-Bit Server VM (build 25.71-b15, mixed mode)</span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# scala -version</span><br><span class=\"line\">Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL</span><br><span class=\"line\">[root@VM-196-182-centos /usr/local]# hadoop version</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hadoop-2.7.3.tar.gz</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> https://archive.apache.org/dist/hadoop/common/</span></span><br><span class=\"line\">wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</span><br><span class=\"line\">tar zxvf hadoop-2.7.3.tar.gz </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/tmp/ # 指定hadoop运行时产生文件的存储路径 </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/hdfs/  </span><br><span class=\"line\">mkdir -p /usr/local/hadoop/hdfs/data/ # datanode上数据块的物理存储位置</span><br><span class=\"line\">mkdir -p /usr/local//hadoop/hdfs/name/ # namenode上存储hdfs名字空间元数据</span><br><span class=\"line\">mv hadoop-2.7.3 /usr/local/hadoop</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hadoop-env.sh</span><br><span class=\"line\">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64 # JAVA_HOME环境变量在这不生效 这里要显示声明一次</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/core-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 指定HDFS老大（namenode）的通信地址 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;hdfs://9.134.196.182:9000&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;!-- 指定hadoop运行时产生文件的存储路径 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/hdfs-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/hdfs/name&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;namenode上存储hdfs名字空间元数据 &lt;/description&gt; </span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;/usr/local/hadoop/hdfs/data&lt;/value&gt;</span><br><span class=\"line\">        &lt;description&gt;datanode上数据块的物理存储位置&lt;/description&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">    &lt;!-- 设置hdfs副本数量 --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 配置yarn（非必需）</span></span><br><span class=\"line\">mv mapred-site.xml.template mapred-site.xml</span><br><span class=\"line\">vim mapred-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- 通知框架MR使用YARN --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">vim /usr/local/hadoop/hadoop-2.7.3/etc/hadoop/yarn-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">    &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class=\"line\">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class=\"line\">    &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 格式化hdfs</span></span><br><span class=\"line\">cd /usr/local/hadoop/hadoop-2.7.3</span><br><span class=\"line\">./bin/hdfs namenode -format</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动hdfs</span></span><br><span class=\"line\">./sbin/start-dfs.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止hdfs</span></span><br><span class=\"line\">./sbin/stop-dfs.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:50070/ <span class=\"comment\"># hadoop管理界面</span></span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动yarn</span></span><br><span class=\"line\">./sbin/start-yarn.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止yarn</span></span><br><span class=\"line\">./sbin/stop-yarn.sh // 需要输入本机密码 todo:配置免密</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:8088/ <span class=\"comment\"># yarn管理界面</span></span></span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/ytangdigl/article/details/109131492\">Hadoop2.7.3在centos7上的单机版安装部署</a></p>\n<h3 id=\"四、HBASE\"><a href=\"#四、HBASE\" class=\"headerlink\" title=\"四、HBASE\"></a>四、HBASE</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hbase.apache.org/downloads.html</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> hbase-2.2.6-bin.tar.gz</span></span><br><span class=\"line\">wget https://archive.apache.org/dist/hbase/2.2.6/hbase-2.2.6-bin.tar.gz</span><br><span class=\"line\">tar zxvf hbase-2.2.6-bin.tar.gz</span><br><span class=\"line\">mkdir -p /usr/local/hbase/hbase-2.2.6/hbase/</span><br><span class=\"line\">mkdir -p /usr/local/hbase/hbase-2.2.6/zookeeper/</span><br><span class=\"line\">vim /usr/local/hbase-2.2.6/conf/hbase-env.sh</span><br><span class=\"line\">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.71-2.b15.el7_2.x86_64</span><br><span class=\"line\">vim /usr/local/hbase-2.2.6/conf/hbase-site.xml</span><br><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\">  &lt;!-- hbase存放数据目录 /data/soft/hbase-2.2.6/hbase为自定义地址 --&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;file:///data/soft/hbase-2.2.6/hbase&lt;/value&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;!-- ZooKeeper数据文件路径 --&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;/usr/hbase/hbase-2.2.6/zookeeper&lt;/value&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;</span><br><span class=\"line\">    &lt;value&gt;false&lt;/value&gt;</span><br><span class=\"line\">    &lt;description&gt;</span><br><span class=\"line\">      Controls whether HBase will check for stream capabilities (hflush/hsync).</span><br><span class=\"line\">      Disable this if you intend to run on LocalFileSystem, denoted by a rootdir</span><br><span class=\"line\">      with the &#x27;file://&#x27; scheme, but be mindful of the NOTE below.</span><br><span class=\"line\">      WARNING: Setting this to false blinds you to potential data loss and</span><br><span class=\"line\">      inconsistent system state in the event of process and/or node failures. If</span><br><span class=\"line\">      HBase is complaining of an inability to use hsync or hflush it&#x27;s most</span><br><span class=\"line\">      likely not a false positive.</span><br><span class=\"line\">    &lt;/description&gt;</span><br><span class=\"line\">  &lt;/property&gt;</span><br><span class=\"line\">&lt;/configuration&gt;</span><br><span class=\"line\">export HBASE_HOME=/usr/local/hbase-2.2.6</span><br><span class=\"line\">export PATH=$HBASE_HOME/bin:$PATH   </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 启动hbase</span></span><br><span class=\"line\">./bin/start-hbase.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 停止hbase</span></span><br><span class=\"line\">./bin/stop-hbase.sh</span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> http://9.134.196.182:16010/master-status</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> 连接</span></span><br><span class=\"line\">./bin/hbase shell</span><br><span class=\"line\">hbase(main):002:0&gt; create_namespace &#x27;test&#x27;</span><br><span class=\"line\">Took 0.1420 seconds                                                             </span><br><span class=\"line\">hbase(main):003:0&gt; create &#x27;test:wednesday&#x27;, &#x27;cf&#x27;</span><br><span class=\"line\">Created table test:wednesday</span><br><span class=\"line\">Took 0.7373 seconds                                                             </span><br><span class=\"line\">=&gt; Hbase::Table - test:wednesday</span><br><span class=\"line\">hbase(main):004:0&gt; list &#x27;test:wednesday&#x27;</span><br><span class=\"line\">TABLE                                                                           </span><br><span class=\"line\">test:wednesday                                                                  </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">Took 0.0191 seconds                                                             </span><br><span class=\"line\">=&gt; [&quot;test:wednesday&quot;]</span><br><span class=\"line\">hbase(main):005:0&gt; desc &#x27;test:wednesday&#x27;</span><br><span class=\"line\">Table test:wednesday is ENABLED                                                 </span><br><span class=\"line\">test:wednesday                                                                  </span><br><span class=\"line\">COLUMN FAMILIES DESCRIPTION                                                     </span><br><span class=\"line\">&#123;NAME =&gt; &#x27;cf&#x27;, VERSIONS =&gt; &#x27;1&#x27;, EVICT_BLOCKS_ON_CLOSE =&gt; &#x27;false&#x27;, NEW_VERSION_BE</span><br><span class=\"line\">HAVIOR =&gt; &#x27;false&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FALSE&#x27;, CACHE_DATA_ON_WRITE =&gt; &#x27;false&#x27;</span><br><span class=\"line\">, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, REPLICAT</span><br><span class=\"line\">ION_SCOPE =&gt; &#x27;0&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, CACHE_INDEX_ON_WRITE =&gt; &#x27;false&#x27;, IN_MEMO</span><br><span class=\"line\">RY =&gt; &#x27;false&#x27;, CACHE_BLOOMS_ON_WRITE =&gt; &#x27;false&#x27;, PREFETCH_BLOCKS_ON_OPEN =&gt; &#x27;fal</span><br><span class=\"line\">se&#x27;, COMPRESSION =&gt; &#x27;NONE&#x27;, BLOCKCACHE =&gt; &#x27;true&#x27;, BLOCKSIZE =&gt; &#x27;65536&#x27;&#125;         </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">QUOTAS                                                                          </span><br><span class=\"line\">0 row(s)</span><br><span class=\"line\">Took 0.1878 seconds                                                             </span><br><span class=\"line\">hbase(main):007:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r1&#x27;,&#x27;cf:k1&#x27;,&#x27;v1&#x27;</span><br><span class=\"line\">Took 0.0056 seconds                                                             </span><br><span class=\"line\">hbase(main):008:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r2&#x27;,&#x27;cf:k2&#x27;,&#x27;v2&#x27;</span><br><span class=\"line\">Took 0.0038 seconds                                                             </span><br><span class=\"line\">hbase(main):009:0&gt; put &#x27;test:wednesday&#x27;,&#x27;r3&#x27;,&#x27;cf:k3&#x27;,&#x27;v3&#x27;</span><br><span class=\"line\">Took 0.0038 seconds                                                             </span><br><span class=\"line\">hbase(main):010:0&gt; scan &#x27;test:wednesday&#x27;</span><br><span class=\"line\">ROW                   COLUMN+CELL                                               </span><br><span class=\"line\"> r1                   column=cf:k1, timestamp=1644406717928, value=v1           </span><br><span class=\"line\"> r2                   column=cf:k2, timestamp=1644406748969, value=v2           </span><br><span class=\"line\"> r3                   column=cf:k3, timestamp=1644406756410, value=v3           </span><br><span class=\"line\">3 row(s)</span><br><span class=\"line\">Took 0.0238 seconds  </span><br><span class=\"line\">hbase(main):011:0&gt; get &#x27;test:wednesday&#x27;,&#x27;r1&#x27;</span><br><span class=\"line\">COLUMN                CELL                                                      </span><br><span class=\"line\"> cf:k1                timestamp=1644406717928, value=v1                         </span><br><span class=\"line\">1 row(s)</span><br><span class=\"line\">Took 0.0108 seconds  </span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"><span class=\"comment\">## 补充 预分区填的是间隔</span></span></span><br><span class=\"line\">create &#x27;xxx&#x27;,&#123;NAME =&gt;&#x27;cf&#x27;,VERSIONS=&gt;1,COMPRESSION=&gt;&#x27;snappy&#x27;,TTL =&gt; 500&#125;,&#123;SPLITS =&gt; [&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;,&#x27;6&#x27;]&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://blog.csdn.net/ytangdigl/article/details/109139682\">HBase单机版安装部署</a></p>\n<h3 id=\"五、CLICKHOUSE\"><a href=\"#五、CLICKHOUSE\" class=\"headerlink\" title=\"五、CLICKHOUSE\"></a>五、CLICKHOUSE</h3><h3 id=\"六、HIVE\"><a href=\"#六、HIVE\" class=\"headerlink\" title=\"六、HIVE\"></a>六、HIVE</h3><h3 id=\"七、ELASTICSEARCH\"><a href=\"#七、ELASTICSEARCH\" class=\"headerlink\" title=\"七、ELASTICSEARCH\"></a>七、ELASTICSEARCH</h3><h3 id=\"八、LOGTSASH\"><a href=\"#八、LOGTSASH\" class=\"headerlink\" title=\"八、LOGTSASH\"></a>八、LOGTSASH</h3><h3 id=\"Q-amp-A\"><a href=\"#Q-amp-A\" class=\"headerlink\" title=\"Q&amp;A\"></a>Q&amp;A</h3><blockquote>\n<p>jar包不包含scala文件？</p>\n</blockquote>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class=\"line\">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class=\"line\">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;groupId&gt;org.example&lt;/groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;wednesday&lt;/artifactId&gt;</span><br><span class=\"line\">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;name&gt;wednesday&lt;/name&gt;</span><br><span class=\"line\">  &lt;!-- FIXME change it to the project&#x27;s website --&gt;</span><br><span class=\"line\">  &lt;url&gt;http://www.example.com&lt;/url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;properties&gt;</span><br><span class=\"line\">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class=\"line\">    &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;</span><br><span class=\"line\">    &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;</span><br><span class=\"line\">    &lt;scala.version&gt;2.12.8&lt;/scala.version&gt;</span><br><span class=\"line\">  &lt;/properties&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;dependencies&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;4.11&lt;/version&gt;</span><br><span class=\"line\">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt;</span><br><span class=\"line\">      &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&lt;!--如果要java -jar运行,需要注释掉这里--&gt;</span><br><span class=\"line\">    &lt;/dependency&gt;</span><br><span class=\"line\">  &lt;/dependencies&gt;</span><br><span class=\"line\">  &lt;build&gt;</span><br><span class=\"line\">    &lt;finalName&gt;DEMO&lt;/finalName&gt;</span><br><span class=\"line\">    &lt;plugins&gt;</span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">        &lt;!--用这个插件来将依赖一块打进jar包--&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;3.0.0&lt;/version&gt;</span><br><span class=\"line\">        &lt;executions&gt;</span><br><span class=\"line\">          &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class=\"line\">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">              &lt;goal&gt;single&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">          &lt;/execution&gt;</span><br><span class=\"line\">        &lt;/executions&gt;</span><br><span class=\"line\">        &lt;configuration&gt;</span><br><span class=\"line\">          &lt;archive&gt;</span><br><span class=\"line\">            &lt;manifest&gt;</span><br><span class=\"line\">              &lt;mainClass&gt;com.example.ScalaApp&lt;/mainClass&gt;</span><br><span class=\"line\">            &lt;/manifest&gt;</span><br><span class=\"line\">          &lt;/archive&gt;</span><br><span class=\"line\">          &lt;descriptorRefs&gt;</span><br><span class=\"line\">            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class=\"line\">          &lt;/descriptorRefs&gt;</span><br><span class=\"line\">        &lt;/configuration&gt;</span><br><span class=\"line\">      &lt;/plugin&gt;</span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">        &lt;!--scala原始在sbt(类似java maven)上做开发,现可以用这个插件来在maven中进行开发--&gt;</span><br><span class=\"line\">        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;</span><br><span class=\"line\">        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;</span><br><span class=\"line\">        &lt;version&gt;2.15.2&lt;/version&gt;</span><br><span class=\"line\">        &lt;executions&gt;</span><br><span class=\"line\">          &lt;execution&gt;</span><br><span class=\"line\">            &lt;id&gt;scala-compile-first&lt;/id&gt;</span><br><span class=\"line\">            &lt;goals&gt;</span><br><span class=\"line\">              &lt;goal&gt;compile&lt;/goal&gt;</span><br><span class=\"line\">            &lt;/goals&gt;</span><br><span class=\"line\">            &lt;configuration&gt;</span><br><span class=\"line\">              &lt;includes&gt;</span><br><span class=\"line\">                &lt;include&gt;**/*.scala&lt;/include&gt;</span><br><span class=\"line\">              &lt;/includes&gt;</span><br><span class=\"line\">            &lt;/configuration&gt;</span><br><span class=\"line\">          &lt;/execution&gt;</span><br><span class=\"line\">        &lt;/executions&gt;</span><br><span class=\"line\">      &lt;/plugin&gt;</span><br><span class=\"line\">    &lt;/plugins&gt;</span><br><span class=\"line\">  &lt;/build&gt;</span><br><span class=\"line\">&lt;/project&gt;</span><br><span class=\"line\">#### 运行</span><br><span class=\"line\">java -jar DEMO-jar-with-dependencies.jar # 已经指定main class了</span><br><span class=\"line\">scala DEMO-jar-with-dependencies.jar</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>"},{"title":"hdfs-block","date":"2022-12-14T01:48:41.000Z","_content":"\n> HDFS block介绍\n\n<!-- more -->\n\n### 介绍\n\nHDFS文件在物理上是分块存储(block)，块大小通过参数dfs.blocksize控制，hadoop2.x默认128M，老版本64M；\n\nHDFS的设计特点：\n\n1）可以进行超大文件存储\n\n2）对商用硬件要求不高，可以在廉价机器上运行\n\n3）流式数据访问：适合一次写入，多次读出的场景，适合用来做数据分析，并不适合做网盘应用等文件系统\n\n4）HDFS只支持单个写入者，而且文件的写入只能以“添加”方式在文件末尾写数据\n\n5）因为NameNode的原因，不适合大量小文件的存储\n\n6）数据访问的延迟相对较高，不适合进行低延迟处理\n\n默认128M因为**最佳传输损耗理论**：在一次传输中，寻址时间占总传输时间的1%时，本次传输损耗最小，为最佳性价比传输，目前硬件的发展条件，普通磁盘写速率大概为100M/s，寻址时间10ms，10ms/1%=1s，1s*100M/s=100M，块传输每64k校验一次，因此块大小必须为2的n方，最接近100M的就是128M；\n\n实际开发中要把block设置远大于128MB，比如存储文件是1TB，一般把block大小设置为512MB，但是也不能设置特别大，因为mapreduce任务中map一次只处理一个块中的数据（默认切片大小等于block大小），如果设置太大，任务数就会少，任务运行速度慢；\n\n如果文件小于块大小，不会占用整个块；\n\n如果是固态硬盘，写速度300M/s，块调整到256M；如果是固态硬盘，写速度500M/s，块调整到512M；\n\n不能太大：1）在一些分块读取的场景，不够灵活，会带来额外的网络消耗；2）在上传文件时，一旦发生故障，会造成资源的浪费；\n\n不能太小：1）块太小，同样大小的文件会占用过多的NameNode的元数据空间；2）块太小，在进行读写操作时，会消耗额外的寻址时间；\n\n块缓存：通常DataNode从磁盘读取块，但是对于频繁访问的数据块，DataNode会将其缓存到DataNode节点内存中，以堆外缓存形式存在。默认情况下，一个块只会缓存到一个DataNode内存中，这样计算框架就可以在缓存块上进行计算任务，大大提高读操作性能，进而提高任务效率；\n\n### 参考\n\nhttps://www.cnblogs.com/sunbr/p/13262242.html\n\nhttps://blog.csdn.net/qq_26442553/article/details/79117897\n","source":"_posts/hdfs-block.md","raw":"---\ntitle: hdfs-block\ndate: 2022-12-14 09:48:41\ntags: HDFS\n---\n\n> HDFS block介绍\n\n<!-- more -->\n\n### 介绍\n\nHDFS文件在物理上是分块存储(block)，块大小通过参数dfs.blocksize控制，hadoop2.x默认128M，老版本64M；\n\nHDFS的设计特点：\n\n1）可以进行超大文件存储\n\n2）对商用硬件要求不高，可以在廉价机器上运行\n\n3）流式数据访问：适合一次写入，多次读出的场景，适合用来做数据分析，并不适合做网盘应用等文件系统\n\n4）HDFS只支持单个写入者，而且文件的写入只能以“添加”方式在文件末尾写数据\n\n5）因为NameNode的原因，不适合大量小文件的存储\n\n6）数据访问的延迟相对较高，不适合进行低延迟处理\n\n默认128M因为**最佳传输损耗理论**：在一次传输中，寻址时间占总传输时间的1%时，本次传输损耗最小，为最佳性价比传输，目前硬件的发展条件，普通磁盘写速率大概为100M/s，寻址时间10ms，10ms/1%=1s，1s*100M/s=100M，块传输每64k校验一次，因此块大小必须为2的n方，最接近100M的就是128M；\n\n实际开发中要把block设置远大于128MB，比如存储文件是1TB，一般把block大小设置为512MB，但是也不能设置特别大，因为mapreduce任务中map一次只处理一个块中的数据（默认切片大小等于block大小），如果设置太大，任务数就会少，任务运行速度慢；\n\n如果文件小于块大小，不会占用整个块；\n\n如果是固态硬盘，写速度300M/s，块调整到256M；如果是固态硬盘，写速度500M/s，块调整到512M；\n\n不能太大：1）在一些分块读取的场景，不够灵活，会带来额外的网络消耗；2）在上传文件时，一旦发生故障，会造成资源的浪费；\n\n不能太小：1）块太小，同样大小的文件会占用过多的NameNode的元数据空间；2）块太小，在进行读写操作时，会消耗额外的寻址时间；\n\n块缓存：通常DataNode从磁盘读取块，但是对于频繁访问的数据块，DataNode会将其缓存到DataNode节点内存中，以堆外缓存形式存在。默认情况下，一个块只会缓存到一个DataNode内存中，这样计算框架就可以在缓存块上进行计算任务，大大提高读操作性能，进而提高任务效率；\n\n### 参考\n\nhttps://www.cnblogs.com/sunbr/p/13262242.html\n\nhttps://blog.csdn.net/qq_26442553/article/details/79117897\n","slug":"hdfs-block","published":1,"updated":"2022-12-19T01:30:52.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh6000416gn6vrkg2t8","content":"<blockquote>\n<p>HDFS block介绍</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>HDFS文件在物理上是分块存储(block)，块大小通过参数dfs.blocksize控制，hadoop2.x默认128M，老版本64M；</p>\n<p>HDFS的设计特点：</p>\n<p>1）可以进行超大文件存储</p>\n<p>2）对商用硬件要求不高，可以在廉价机器上运行</p>\n<p>3）流式数据访问：适合一次写入，多次读出的场景，适合用来做数据分析，并不适合做网盘应用等文件系统</p>\n<p>4）HDFS只支持单个写入者，而且文件的写入只能以“添加”方式在文件末尾写数据</p>\n<p>5）因为NameNode的原因，不适合大量小文件的存储</p>\n<p>6）数据访问的延迟相对较高，不适合进行低延迟处理</p>\n<p>默认128M因为<strong>最佳传输损耗理论</strong>：在一次传输中，寻址时间占总传输时间的1%时，本次传输损耗最小，为最佳性价比传输，目前硬件的发展条件，普通磁盘写速率大概为100M/s，寻址时间10ms，10ms/1%=1s，1s*100M/s=100M，块传输每64k校验一次，因此块大小必须为2的n方，最接近100M的就是128M；</p>\n<p>实际开发中要把block设置远大于128MB，比如存储文件是1TB，一般把block大小设置为512MB，但是也不能设置特别大，因为mapreduce任务中map一次只处理一个块中的数据（默认切片大小等于block大小），如果设置太大，任务数就会少，任务运行速度慢；</p>\n<p>如果文件小于块大小，不会占用整个块；</p>\n<p>如果是固态硬盘，写速度300M/s，块调整到256M；如果是固态硬盘，写速度500M/s，块调整到512M；</p>\n<p>不能太大：1）在一些分块读取的场景，不够灵活，会带来额外的网络消耗；2）在上传文件时，一旦发生故障，会造成资源的浪费；</p>\n<p>不能太小：1）块太小，同样大小的文件会占用过多的NameNode的元数据空间；2）块太小，在进行读写操作时，会消耗额外的寻址时间；</p>\n<p>块缓存：通常DataNode从磁盘读取块，但是对于频繁访问的数据块，DataNode会将其缓存到DataNode节点内存中，以堆外缓存形式存在。默认情况下，一个块只会缓存到一个DataNode内存中，这样计算框架就可以在缓存块上进行计算任务，大大提高读操作性能，进而提高任务效率；</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/sunbr/p/13262242.html\">https://www.cnblogs.com/sunbr/p/13262242.html</a></p>\n<p><a href=\"https://blog.csdn.net/qq_26442553/article/details/79117897\">https://blog.csdn.net/qq_26442553/article/details/79117897</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>HDFS block介绍</p>\n</blockquote>","more":"<h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p>HDFS文件在物理上是分块存储(block)，块大小通过参数dfs.blocksize控制，hadoop2.x默认128M，老版本64M；</p>\n<p>HDFS的设计特点：</p>\n<p>1）可以进行超大文件存储</p>\n<p>2）对商用硬件要求不高，可以在廉价机器上运行</p>\n<p>3）流式数据访问：适合一次写入，多次读出的场景，适合用来做数据分析，并不适合做网盘应用等文件系统</p>\n<p>4）HDFS只支持单个写入者，而且文件的写入只能以“添加”方式在文件末尾写数据</p>\n<p>5）因为NameNode的原因，不适合大量小文件的存储</p>\n<p>6）数据访问的延迟相对较高，不适合进行低延迟处理</p>\n<p>默认128M因为<strong>最佳传输损耗理论</strong>：在一次传输中，寻址时间占总传输时间的1%时，本次传输损耗最小，为最佳性价比传输，目前硬件的发展条件，普通磁盘写速率大概为100M/s，寻址时间10ms，10ms/1%=1s，1s*100M/s=100M，块传输每64k校验一次，因此块大小必须为2的n方，最接近100M的就是128M；</p>\n<p>实际开发中要把block设置远大于128MB，比如存储文件是1TB，一般把block大小设置为512MB，但是也不能设置特别大，因为mapreduce任务中map一次只处理一个块中的数据（默认切片大小等于block大小），如果设置太大，任务数就会少，任务运行速度慢；</p>\n<p>如果文件小于块大小，不会占用整个块；</p>\n<p>如果是固态硬盘，写速度300M/s，块调整到256M；如果是固态硬盘，写速度500M/s，块调整到512M；</p>\n<p>不能太大：1）在一些分块读取的场景，不够灵活，会带来额外的网络消耗；2）在上传文件时，一旦发生故障，会造成资源的浪费；</p>\n<p>不能太小：1）块太小，同样大小的文件会占用过多的NameNode的元数据空间；2）块太小，在进行读写操作时，会消耗额外的寻址时间；</p>\n<p>块缓存：通常DataNode从磁盘读取块，但是对于频繁访问的数据块，DataNode会将其缓存到DataNode节点内存中，以堆外缓存形式存在。默认情况下，一个块只会缓存到一个DataNode内存中，这样计算框架就可以在缓存块上进行计算任务，大大提高读操作性能，进而提高任务效率；</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/sunbr/p/13262242.html\">https://www.cnblogs.com/sunbr/p/13262242.html</a></p>\n<p><a href=\"https://blog.csdn.net/qq_26442553/article/details/79117897\">https://blog.csdn.net/qq_26442553/article/details/79117897</a></p>"},{"title":"dp-py","date":"2022-12-09T07:13:30.000Z","_content":"\n> 动态规划问题汇总\n\n<!-- more -->\n\n### 背包\n\n```python\n# 01背包\n# dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大重量】；\n# w[i]表示第i件的物品的重量，对应到索引上是i-1\n# 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + w[i])\ndef backpack1(m, w):\n    dp = [[0]*(m+1) for _ in range(len(w)+1)]\n    for i in range(1, len(w)+1):\n        for j in range(1, m+1):\n            if w[i-1]>j:\n                dp[i][j] = dp[i-1][j]\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i-1]]+w[i-1])\n    return dp[-1][-1]\n# 01背包\n# dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大价值】；\n# w[i]表示第i件的物品的重量，v[i]表示第i件的物品的重量, 对应到索引上是i-1\n# 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + v[i])\ndef backpack2(m, w, v):\n    dp = [[0]*(m+1) for _ in range(len(w)+1)]\n    for i in range(1, len(w)+1):\n        for j in range(1, m+1):\n            if w[i-1]>j:\n                dp[i][j] = dp[i-1][j]\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i-1]]+v[i-1])\n    return dp[-1][-1]\n# 优化空间复杂度\ndef backpack3(m, w, v):\n    dp = [0]*(m+1) # 只占用m+1长度内存\n    for i in range(1, len(w)+1): # 遍历物品\n        for j in range(m, 0, -1): # 逆序遍历 上轮前面dp[j-w[i-1]]还要用\n            if w[i-1]<=j: # 如果可以放下\n                dp[j] = max(dp[j], dp[j-w[i-1]] + v[i-1])\n    return dp[-1]\nbackpack1(10, [3,4,8,5])\nbackpack2(10, [3,4,8,5], [10,4,5,2])\nbackpack3(10, [3,4,8,5], [10,4,5,2])\n```\n\n### 爬楼梯\n\n```python\n# 爬楼梯 n阶楼梯，可以选择上1阶或2阶共有多少种方法\n# 1.递归\ndef climb1(n):\n    if n<=2:\n        return n\n    return climb1(n-1) + climb1(n-2)\n# climb1(10)\n\n# 2.动态规划\n# 状态转移方程：f[i] = f[i-1] + f[i-2]\ndef climb2(n):\n    dp = [0]*n\n    dp[0] = 1\n    dp[1] = 2\n    for i in range(2, n):\n        dp[i] = dp[i-1]+dp[i-2]\n    return dp[-1]\n# climb2(10)\n```\n\n### 零钱兑换\n\n```python\n#### dp[i] = min(dp[i], dp[i-coins[j]]+1), dp[i]表示组成i金额最小硬币数\ndef coinChange(coins, amount):\n    dp = [float(\"inf\")]*(amount+1) \n    dp[0] = 0\n    for i in range(1, amount+1): # 遍历总金额\n        for j in range(len(coins)): # 遍历硬币\n            if i>coins[j]:\n                dp[i] = min(dp[i],dp[i-coins[j]]+1)\n    if dp[-1]>amount:\n        return -1\n    return dp[-1]\n```\n\n","source":"_posts/dp-py.md","raw":"---\ntitle: dp-py\ndate: 2022-12-09 15:13:30\ntags: 动态规划\n---\n\n> 动态规划问题汇总\n\n<!-- more -->\n\n### 背包\n\n```python\n# 01背包\n# dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大重量】；\n# w[i]表示第i件的物品的重量，对应到索引上是i-1\n# 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + w[i])\ndef backpack1(m, w):\n    dp = [[0]*(m+1) for _ in range(len(w)+1)]\n    for i in range(1, len(w)+1):\n        for j in range(1, m+1):\n            if w[i-1]>j:\n                dp[i][j] = dp[i-1][j]\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i-1]]+w[i-1])\n    return dp[-1][-1]\n# 01背包\n# dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大价值】；\n# w[i]表示第i件的物品的重量，v[i]表示第i件的物品的重量, 对应到索引上是i-1\n# 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + v[i])\ndef backpack2(m, w, v):\n    dp = [[0]*(m+1) for _ in range(len(w)+1)]\n    for i in range(1, len(w)+1):\n        for j in range(1, m+1):\n            if w[i-1]>j:\n                dp[i][j] = dp[i-1][j]\n            else:\n                dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i-1]]+v[i-1])\n    return dp[-1][-1]\n# 优化空间复杂度\ndef backpack3(m, w, v):\n    dp = [0]*(m+1) # 只占用m+1长度内存\n    for i in range(1, len(w)+1): # 遍历物品\n        for j in range(m, 0, -1): # 逆序遍历 上轮前面dp[j-w[i-1]]还要用\n            if w[i-1]<=j: # 如果可以放下\n                dp[j] = max(dp[j], dp[j-w[i-1]] + v[i-1])\n    return dp[-1]\nbackpack1(10, [3,4,8,5])\nbackpack2(10, [3,4,8,5], [10,4,5,2])\nbackpack3(10, [3,4,8,5], [10,4,5,2])\n```\n\n### 爬楼梯\n\n```python\n# 爬楼梯 n阶楼梯，可以选择上1阶或2阶共有多少种方法\n# 1.递归\ndef climb1(n):\n    if n<=2:\n        return n\n    return climb1(n-1) + climb1(n-2)\n# climb1(10)\n\n# 2.动态规划\n# 状态转移方程：f[i] = f[i-1] + f[i-2]\ndef climb2(n):\n    dp = [0]*n\n    dp[0] = 1\n    dp[1] = 2\n    for i in range(2, n):\n        dp[i] = dp[i-1]+dp[i-2]\n    return dp[-1]\n# climb2(10)\n```\n\n### 零钱兑换\n\n```python\n#### dp[i] = min(dp[i], dp[i-coins[j]]+1), dp[i]表示组成i金额最小硬币数\ndef coinChange(coins, amount):\n    dp = [float(\"inf\")]*(amount+1) \n    dp[0] = 0\n    for i in range(1, amount+1): # 遍历总金额\n        for j in range(len(coins)): # 遍历硬币\n            if i>coins[j]:\n                dp[i] = min(dp[i],dp[i-coins[j]]+1)\n    if dp[-1]>amount:\n        return -1\n    return dp[-1]\n```\n\n","slug":"dp-py","published":1,"updated":"2022-12-19T01:30:52.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh7000516gndftv911s","content":"<blockquote>\n<p>动态规划问题汇总</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"背包\"><a href=\"#背包\" class=\"headerlink\" title=\"背包\"></a>背包</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 01背包</span></span><br><span class=\"line\"><span class=\"comment\"># dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大重量】；</span></span><br><span class=\"line\"><span class=\"comment\"># w[i]表示第i件的物品的重量，对应到索引上是i-1</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + w[i])</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack1</span>(<span class=\"params\">m, w</span>):</span></span><br><span class=\"line\">    dp = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&gt;j:</span><br><span class=\"line\">                dp[i][j] = dp[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dp[i][j] = <span class=\"built_in\">max</span>(dp[i-<span class=\"number\">1</span>][j], dp[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+w[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 01背包</span></span><br><span class=\"line\"><span class=\"comment\"># dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大价值】；</span></span><br><span class=\"line\"><span class=\"comment\"># w[i]表示第i件的物品的重量，v[i]表示第i件的物品的重量, 对应到索引上是i-1</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + v[i])</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack2</span>(<span class=\"params\">m, w, v</span>):</span></span><br><span class=\"line\">    dp = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&gt;j:</span><br><span class=\"line\">                dp[i][j] = dp[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dp[i][j] = <span class=\"built_in\">max</span>(dp[i-<span class=\"number\">1</span>][j], dp[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 优化空间复杂度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack3</span>(<span class=\"params\">m, w, v</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"comment\"># 只占用m+1长度内存</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>): <span class=\"comment\"># 遍历物品</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m, <span class=\"number\">0</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 逆序遍历 上轮前面dp[j-w[i-1]]还要用</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&lt;=j: <span class=\"comment\"># 如果可以放下</span></span><br><span class=\"line\">                dp[j] = <span class=\"built_in\">max</span>(dp[j], dp[j-w[i-<span class=\"number\">1</span>]] + v[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br><span class=\"line\">backpack1(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">backpack2(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>], [<span class=\"number\">10</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">backpack3(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>], [<span class=\"number\">10</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"爬楼梯\"><a href=\"#爬楼梯\" class=\"headerlink\" title=\"爬楼梯\"></a>爬楼梯</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 爬楼梯 n阶楼梯，可以选择上1阶或2阶共有多少种方法</span></span><br><span class=\"line\"><span class=\"comment\"># 1.递归</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">climb1</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n&lt;=<span class=\"number\">2</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> climb1(n-<span class=\"number\">1</span>) + climb1(n-<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\"># climb1(10)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.动态规划</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程：f[i] = f[i-1] + f[i-2]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">climb2</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"number\">0</span>]*n</span><br><span class=\"line\">    dp[<span class=\"number\">0</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">    dp[<span class=\"number\">1</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>, n):</span><br><span class=\"line\">        dp[i] = dp[i-<span class=\"number\">1</span>]+dp[i-<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># climb2(10)</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"零钱兑换\"><a href=\"#零钱兑换\" class=\"headerlink\" title=\"零钱兑换\"></a>零钱兑换</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#### dp[i] = min(dp[i], dp[i-coins[j]]+1), dp[i]表示组成i金额最小硬币数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coinChange</span>(<span class=\"params\">coins, amount</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)]*(amount+<span class=\"number\">1</span>) </span><br><span class=\"line\">    dp[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, amount+<span class=\"number\">1</span>): <span class=\"comment\"># 遍历总金额</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(coins)): <span class=\"comment\"># 遍历硬币</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> i&gt;coins[j]:</span><br><span class=\"line\">                dp[i] = <span class=\"built_in\">min</span>(dp[i],dp[i-coins[j]]+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dp[-<span class=\"number\">1</span>]&gt;amount:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>动态规划问题汇总</p>\n</blockquote>","more":"<h3 id=\"背包\"><a href=\"#背包\" class=\"headerlink\" title=\"背包\"></a>背包</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 01背包</span></span><br><span class=\"line\"><span class=\"comment\"># dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大重量】；</span></span><br><span class=\"line\"><span class=\"comment\"># w[i]表示第i件的物品的重量，对应到索引上是i-1</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + w[i])</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack1</span>(<span class=\"params\">m, w</span>):</span></span><br><span class=\"line\">    dp = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&gt;j:</span><br><span class=\"line\">                dp[i][j] = dp[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dp[i][j] = <span class=\"built_in\">max</span>(dp[i-<span class=\"number\">1</span>][j], dp[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+w[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 01背包</span></span><br><span class=\"line\"><span class=\"comment\"># dp[i][j]表示在0~i下标物品中选取，在总承重0~j的情况下获得的【最大价值】；</span></span><br><span class=\"line\"><span class=\"comment\"># w[i]表示第i件的物品的重量，v[i]表示第i件的物品的重量, 对应到索引上是i-1</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程:f[i][j] = max(f[i-1][j], f[i-1][j-w[i]] + v[i])</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack2</span>(<span class=\"params\">m, w, v</span>):</span></span><br><span class=\"line\">    dp = [[<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&gt;j:</span><br><span class=\"line\">                dp[i][j] = dp[i-<span class=\"number\">1</span>][j]</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                dp[i][j] = <span class=\"built_in\">max</span>(dp[i-<span class=\"number\">1</span>][j], dp[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>][-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># 优化空间复杂度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">backpack3</span>(<span class=\"params\">m, w, v</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"number\">0</span>]*(m+<span class=\"number\">1</span>) <span class=\"comment\"># 只占用m+1长度内存</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(w)+<span class=\"number\">1</span>): <span class=\"comment\"># 遍历物品</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m, <span class=\"number\">0</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 逆序遍历 上轮前面dp[j-w[i-1]]还要用</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> w[i-<span class=\"number\">1</span>]&lt;=j: <span class=\"comment\"># 如果可以放下</span></span><br><span class=\"line\">                dp[j] = <span class=\"built_in\">max</span>(dp[j], dp[j-w[i-<span class=\"number\">1</span>]] + v[i-<span class=\"number\">1</span>])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br><span class=\"line\">backpack1(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>])</span><br><span class=\"line\">backpack2(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>], [<span class=\"number\">10</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">backpack3(<span class=\"number\">10</span>, [<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">8</span>,<span class=\"number\">5</span>], [<span class=\"number\">10</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">2</span>])</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"爬楼梯\"><a href=\"#爬楼梯\" class=\"headerlink\" title=\"爬楼梯\"></a>爬楼梯</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 爬楼梯 n阶楼梯，可以选择上1阶或2阶共有多少种方法</span></span><br><span class=\"line\"><span class=\"comment\"># 1.递归</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">climb1</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n&lt;=<span class=\"number\">2</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> climb1(n-<span class=\"number\">1</span>) + climb1(n-<span class=\"number\">2</span>)</span><br><span class=\"line\"><span class=\"comment\"># climb1(10)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2.动态规划</span></span><br><span class=\"line\"><span class=\"comment\"># 状态转移方程：f[i] = f[i-1] + f[i-2]</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">climb2</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"number\">0</span>]*n</span><br><span class=\"line\">    dp[<span class=\"number\">0</span>] = <span class=\"number\">1</span></span><br><span class=\"line\">    dp[<span class=\"number\">1</span>] = <span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">2</span>, n):</span><br><span class=\"line\">        dp[i] = dp[i-<span class=\"number\">1</span>]+dp[i-<span class=\"number\">2</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br><span class=\"line\"><span class=\"comment\"># climb2(10)</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"零钱兑换\"><a href=\"#零钱兑换\" class=\"headerlink\" title=\"零钱兑换\"></a>零钱兑换</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#### dp[i] = min(dp[i], dp[i-coins[j]]+1), dp[i]表示组成i金额最小硬币数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">coinChange</span>(<span class=\"params\">coins, amount</span>):</span></span><br><span class=\"line\">    dp = [<span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>)]*(amount+<span class=\"number\">1</span>) </span><br><span class=\"line\">    dp[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>, amount+<span class=\"number\">1</span>): <span class=\"comment\"># 遍历总金额</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(coins)): <span class=\"comment\"># 遍历硬币</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> i&gt;coins[j]:</span><br><span class=\"line\">                dp[i] = <span class=\"built_in\">min</span>(dp[i],dp[i-coins[j]]+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> dp[-<span class=\"number\">1</span>]&gt;amount:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dp[-<span class=\"number\">1</span>]</span><br></pre></td></tr></table></figure>"},{"title":"binary-tree","date":"2022-12-08T06:20:24.000Z","_content":"\n> 树相关汇总\n\n<!-- more -->\n\n### 普通遍历\n\n```python\n# 前序遍历(根左右)\ndef preOrderTravesal(root):\n    if not root:\n        return\n    print(root.v)\n    preOrderTravesal(root.l)\n    preOrderTravesal(root.r)\n# 中序遍历(左根右)\ndef inOrderTravesal(root):\n    if not root:\n        return\n    inOrderTravesal(root.l)\n    print(root.v)\n    inOrderTravesal(root.r)\n# 后续遍历(左右根)\ndef posOrderTravesal(root):\n    if not root:\n        return\n    posOrderTravesal(root.l)\n    posOrderTravesal(root.r)\n    print(root.v)\n\n# 非递归前序遍历\ndef preOrder(root):\n    if not root:\n        return None\n    result = [] # 结果\n    stack = []  # 缓存\n    while root!=None or len(stack)!=0: \n        while root!=None: # 遍历左子树\n            result.append(root.v) # 根结点先入结果集\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1] # 弹出\n        stack = stack[:len(stack)-1] # 清理缓存\n        root = node.r # root为弹出节点的右节点\n    return result\n\n# 非递归中序遍历\ndef inOrder(root):\n    if not root:\n        return None\n    result = []\n    stack = []\n    while root!=None or len(stack)!=0:\n        while root!=None:\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1]\n        stack = stack[:len(stack)-1]\n        result.append(node.v) # 左节点先入结果集(和前序区别)\n        root = node.r\n    return result\n\n# 非递归后序遍历\ndef posOrder(root):\n    if not root:\n        return None\n    result = []\n    stack = []\n    lastVisit = None\n    while root!=None or len(stack)!=0:\n        while root!=None:\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1]\n        if node.r==None or node.r==lastVisit: # 确保右节点先于跟节点进入结果集\n            stack = stack[:len(stack)-1] \n            result.append(node.v) \n            lastVisit = node\n        else:\n            root = node.r # 确保右节点先于跟节点进入结果集\n    return result\n```\n\n### 优先遍历\n\n```python\n# 深度优先遍历(前序遍历)\nresult = []\ndef dfs(root):\n    if root==None:\n        return\n    result.append(root.v)\n    dfs(root.l)\n    dfs(root.r)\n# 广度优先遍历(层次遍历)\ndef bfs(root):\n    if root==None:\n        return \n    queue = [root]\n    result = []\n    while queue:\n        node = queue.pop(0)\n        result.append(node.v)\n        if node.l: queue.append(node.l)\n        if node.r: queue.append(node.r)\n    return result\n```\n\n### 变种\n\n```python\n# 最大深度\ndef maxDepth(root):\n    if not root:\n        return 0\n    return max(maxDepth(root.l), maxDepth(root.r))+1\n \n# 是否平衡树(平衡返回最大深度)\n# 平衡树:左右子树深度差不超过1\ndef balanceTree(root):\n    if not root:\n        return 0\n    left = maxDepth(root.l)\n    right = maxDepth(root.r)\n    if abs(left-right)>1:\n        return -1\n    return max(left,right)+1\n \n# 最大路径值和\nresult = float(\"-inf\")\ndef maxPathSum(root):\n    if not root:\n        return 0\n    left = maxPathSum(root.l)\n    right = maxPathSum(root.r)\n    global result\n    result = max(left + right + root.v, result) # 如果当前节点为根节点最大路径和\n    return max(0, max(left, right) + root.v) # 当前节点作为子节点能提供的最大路径和\nmaxPathSum(n1)\nprint(result)\n\n# 公共祖先\ndef commonAncester(root, p, q):\n    if not root or root==p or root==q:\n        return root\n    left = commonAncester(root.l, p, q) # 是否包含p或q\n    right = commonAncester(root.r, p, q)\n    if not left: return right # 没在左说明在右\n    if not right: return left # 没在右说明在左\n    return root # 左右都有说明跟节点是公共祖先\ncommonAncester(n1,n2,n3).v\n\n# 是否是二叉搜索树\ndef bst(root):\n    def helper(root, minv, maxv):\n        if not root:\n            return True\n        if not minv<root.v<maxv:\n            return False\n        return helper(root.l, minv, root.v) and helper(root.r, root.v, maxv)\n    return helper(root, float(\"-inf\"), float(\"inf\"))\nbst(n1)\n\n# 插入bst\ndef insertBST(root, v):\n    if not root:\n        return TreeNode(v)\n    if root.v < v:\n        root.r = insertBST(root.r, v)\n    if root.v > v:\n        root.l = insertBST(root.l, v)\n    return root\n```\n\n### 建树\n\n```python\nclass TreeNode:\n    def __init__(self, v, l, r):\n        self.v = v\n        self.l = l\n        self.r = r\nn1,n2,n3,n4,n5,n6,n7 = TreeNode(1,None,None),TreeNode(2,None,None),TreeNode(3,None,None),TreeNode(4,None,None),TreeNode(5,None,None),TreeNode(6,None,None),TreeNode(7,None,None)\nn1.l=n2\nn1.r=n3\nn2.l=n4\nn2.r=n5\nn3.l=n6\nn3.r=n7\n#    1\n#  2   3\n# 4 5 6 7\n# 3层 长度7 非子节点最大索引7/2\nn1.r.r.v\n```\n\n","source":"_posts/binary-tree.md","raw":"---\ntitle: binary-tree\ndate: 2022-12-08 14:20:24\ntags: 树\n---\n\n> 树相关汇总\n\n<!-- more -->\n\n### 普通遍历\n\n```python\n# 前序遍历(根左右)\ndef preOrderTravesal(root):\n    if not root:\n        return\n    print(root.v)\n    preOrderTravesal(root.l)\n    preOrderTravesal(root.r)\n# 中序遍历(左根右)\ndef inOrderTravesal(root):\n    if not root:\n        return\n    inOrderTravesal(root.l)\n    print(root.v)\n    inOrderTravesal(root.r)\n# 后续遍历(左右根)\ndef posOrderTravesal(root):\n    if not root:\n        return\n    posOrderTravesal(root.l)\n    posOrderTravesal(root.r)\n    print(root.v)\n\n# 非递归前序遍历\ndef preOrder(root):\n    if not root:\n        return None\n    result = [] # 结果\n    stack = []  # 缓存\n    while root!=None or len(stack)!=0: \n        while root!=None: # 遍历左子树\n            result.append(root.v) # 根结点先入结果集\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1] # 弹出\n        stack = stack[:len(stack)-1] # 清理缓存\n        root = node.r # root为弹出节点的右节点\n    return result\n\n# 非递归中序遍历\ndef inOrder(root):\n    if not root:\n        return None\n    result = []\n    stack = []\n    while root!=None or len(stack)!=0:\n        while root!=None:\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1]\n        stack = stack[:len(stack)-1]\n        result.append(node.v) # 左节点先入结果集(和前序区别)\n        root = node.r\n    return result\n\n# 非递归后序遍历\ndef posOrder(root):\n    if not root:\n        return None\n    result = []\n    stack = []\n    lastVisit = None\n    while root!=None or len(stack)!=0:\n        while root!=None:\n            stack.append(root)\n            root = root.l\n        node = stack[len(stack)-1]\n        if node.r==None or node.r==lastVisit: # 确保右节点先于跟节点进入结果集\n            stack = stack[:len(stack)-1] \n            result.append(node.v) \n            lastVisit = node\n        else:\n            root = node.r # 确保右节点先于跟节点进入结果集\n    return result\n```\n\n### 优先遍历\n\n```python\n# 深度优先遍历(前序遍历)\nresult = []\ndef dfs(root):\n    if root==None:\n        return\n    result.append(root.v)\n    dfs(root.l)\n    dfs(root.r)\n# 广度优先遍历(层次遍历)\ndef bfs(root):\n    if root==None:\n        return \n    queue = [root]\n    result = []\n    while queue:\n        node = queue.pop(0)\n        result.append(node.v)\n        if node.l: queue.append(node.l)\n        if node.r: queue.append(node.r)\n    return result\n```\n\n### 变种\n\n```python\n# 最大深度\ndef maxDepth(root):\n    if not root:\n        return 0\n    return max(maxDepth(root.l), maxDepth(root.r))+1\n \n# 是否平衡树(平衡返回最大深度)\n# 平衡树:左右子树深度差不超过1\ndef balanceTree(root):\n    if not root:\n        return 0\n    left = maxDepth(root.l)\n    right = maxDepth(root.r)\n    if abs(left-right)>1:\n        return -1\n    return max(left,right)+1\n \n# 最大路径值和\nresult = float(\"-inf\")\ndef maxPathSum(root):\n    if not root:\n        return 0\n    left = maxPathSum(root.l)\n    right = maxPathSum(root.r)\n    global result\n    result = max(left + right + root.v, result) # 如果当前节点为根节点最大路径和\n    return max(0, max(left, right) + root.v) # 当前节点作为子节点能提供的最大路径和\nmaxPathSum(n1)\nprint(result)\n\n# 公共祖先\ndef commonAncester(root, p, q):\n    if not root or root==p or root==q:\n        return root\n    left = commonAncester(root.l, p, q) # 是否包含p或q\n    right = commonAncester(root.r, p, q)\n    if not left: return right # 没在左说明在右\n    if not right: return left # 没在右说明在左\n    return root # 左右都有说明跟节点是公共祖先\ncommonAncester(n1,n2,n3).v\n\n# 是否是二叉搜索树\ndef bst(root):\n    def helper(root, minv, maxv):\n        if not root:\n            return True\n        if not minv<root.v<maxv:\n            return False\n        return helper(root.l, minv, root.v) and helper(root.r, root.v, maxv)\n    return helper(root, float(\"-inf\"), float(\"inf\"))\nbst(n1)\n\n# 插入bst\ndef insertBST(root, v):\n    if not root:\n        return TreeNode(v)\n    if root.v < v:\n        root.r = insertBST(root.r, v)\n    if root.v > v:\n        root.l = insertBST(root.l, v)\n    return root\n```\n\n### 建树\n\n```python\nclass TreeNode:\n    def __init__(self, v, l, r):\n        self.v = v\n        self.l = l\n        self.r = r\nn1,n2,n3,n4,n5,n6,n7 = TreeNode(1,None,None),TreeNode(2,None,None),TreeNode(3,None,None),TreeNode(4,None,None),TreeNode(5,None,None),TreeNode(6,None,None),TreeNode(7,None,None)\nn1.l=n2\nn1.r=n3\nn2.l=n4\nn2.r=n5\nn3.l=n6\nn3.r=n7\n#    1\n#  2   3\n# 4 5 6 7\n# 3层 长度7 非子节点最大索引7/2\nn1.r.r.v\n```\n\n","slug":"binary-tree","published":1,"updated":"2022-12-19T01:30:52.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh7000816gn905734e1","content":"<blockquote>\n<p>树相关汇总</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"普通遍历\"><a href=\"#普通遍历\" class=\"headerlink\" title=\"普通遍历\"></a>普通遍历</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 前序遍历(根左右)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">preOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\">    preOrderTravesal(root.l)</span><br><span class=\"line\">    preOrderTravesal(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 中序遍历(左根右)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    inOrderTravesal(root.l)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\">    inOrderTravesal(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 后续遍历(左右根)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">posOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    posOrderTravesal(root.l)</span><br><span class=\"line\">    posOrderTravesal(root.r)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归前序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">preOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = [] <span class=\"comment\"># 结果</span></span><br><span class=\"line\">    stack = []  <span class=\"comment\"># 缓存</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>: </span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>: <span class=\"comment\"># 遍历左子树</span></span><br><span class=\"line\">            result.append(root.v) <span class=\"comment\"># 根结点先入结果集</span></span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] <span class=\"comment\"># 弹出</span></span><br><span class=\"line\">        stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] <span class=\"comment\"># 清理缓存</span></span><br><span class=\"line\">        root = node.r <span class=\"comment\"># root为弹出节点的右节点</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归中序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    stack = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        result.append(node.v) <span class=\"comment\"># 左节点先入结果集(和前序区别)</span></span><br><span class=\"line\">        root = node.r</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归后序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">posOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    stack = []</span><br><span class=\"line\">    lastVisit = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.r==<span class=\"literal\">None</span> <span class=\"keyword\">or</span> node.r==lastVisit: <span class=\"comment\"># 确保右节点先于跟节点进入结果集</span></span><br><span class=\"line\">            stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] </span><br><span class=\"line\">            result.append(node.v) </span><br><span class=\"line\">            lastVisit = node</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            root = node.r <span class=\"comment\"># 确保右节点先于跟节点进入结果集</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"优先遍历\"><a href=\"#优先遍历\" class=\"headerlink\" title=\"优先遍历\"></a>优先遍历</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 深度优先遍历(前序遍历)</span></span><br><span class=\"line\">result = []</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfs</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> root==<span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    result.append(root.v)</span><br><span class=\"line\">    dfs(root.l)</span><br><span class=\"line\">    dfs(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 广度优先遍历(层次遍历)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bfs</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> root==<span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> </span><br><span class=\"line\">    queue = [root]</span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> queue:</span><br><span class=\"line\">        node = queue.pop(<span class=\"number\">0</span>)</span><br><span class=\"line\">        result.append(node.v)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.l: queue.append(node.l)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.r: queue.append(node.r)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"变种\"><a href=\"#变种\" class=\"headerlink\" title=\"变种\"></a>变种</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 最大深度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxDepth</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(maxDepth(root.l), maxDepth(root.r))+<span class=\"number\">1</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 是否平衡树(平衡返回最大深度)</span></span><br><span class=\"line\"><span class=\"comment\"># 平衡树:左右子树深度差不超过1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">balanceTree</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    left = maxDepth(root.l)</span><br><span class=\"line\">    right = maxDepth(root.r)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">abs</span>(left-right)&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(left,right)+<span class=\"number\">1</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 最大路径值和</span></span><br><span class=\"line\">result = <span class=\"built_in\">float</span>(<span class=\"string\">&quot;-inf&quot;</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxPathSum</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    left = maxPathSum(root.l)</span><br><span class=\"line\">    right = maxPathSum(root.r)</span><br><span class=\"line\">    <span class=\"keyword\">global</span> result</span><br><span class=\"line\">    result = <span class=\"built_in\">max</span>(left + right + root.v, result) <span class=\"comment\"># 如果当前节点为根节点最大路径和</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(<span class=\"number\">0</span>, <span class=\"built_in\">max</span>(left, right) + root.v) <span class=\"comment\"># 当前节点作为子节点能提供的最大路径和</span></span><br><span class=\"line\">maxPathSum(n1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 公共祖先</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">commonAncester</span>(<span class=\"params\">root, p, q</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root <span class=\"keyword\">or</span> root==p <span class=\"keyword\">or</span> root==q:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> root</span><br><span class=\"line\">    left = commonAncester(root.l, p, q) <span class=\"comment\"># 是否包含p或q</span></span><br><span class=\"line\">    right = commonAncester(root.r, p, q)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> left: <span class=\"keyword\">return</span> right <span class=\"comment\"># 没在左说明在右</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> right: <span class=\"keyword\">return</span> left <span class=\"comment\"># 没在右说明在左</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> root <span class=\"comment\"># 左右都有说明跟节点是公共祖先</span></span><br><span class=\"line\">commonAncester(n1,n2,n3).v</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否是二叉搜索树</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bst</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span>(<span class=\"params\">root, minv, maxv</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> minv&lt;root.v&lt;maxv:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> helper(root.l, minv, root.v) <span class=\"keyword\">and</span> helper(root.r, root.v, maxv)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> helper(root, <span class=\"built_in\">float</span>(<span class=\"string\">&quot;-inf&quot;</span>), <span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>))</span><br><span class=\"line\">bst(n1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 插入bst</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertBST</span>(<span class=\"params\">root, v</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> TreeNode(v)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> root.v &lt; v:</span><br><span class=\"line\">        root.r = insertBST(root.r, v)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> root.v &gt; v:</span><br><span class=\"line\">        root.l = insertBST(root.l, v)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> root</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"建树\"><a href=\"#建树\" class=\"headerlink\" title=\"建树\"></a>建树</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, v, l, r</span>):</span></span><br><span class=\"line\">        self.v = v</span><br><span class=\"line\">        self.l = l</span><br><span class=\"line\">        self.r = r</span><br><span class=\"line\">n1,n2,n3,n4,n5,n6,n7 = TreeNode(<span class=\"number\">1</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">2</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">3</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">4</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">5</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">6</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">7</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>)</span><br><span class=\"line\">n1.l=n2</span><br><span class=\"line\">n1.r=n3</span><br><span class=\"line\">n2.l=n4</span><br><span class=\"line\">n2.r=n5</span><br><span class=\"line\">n3.l=n6</span><br><span class=\"line\">n3.r=n7</span><br><span class=\"line\"><span class=\"comment\">#    1</span></span><br><span class=\"line\"><span class=\"comment\">#  2   3</span></span><br><span class=\"line\"><span class=\"comment\"># 4 5 6 7</span></span><br><span class=\"line\"><span class=\"comment\"># 3层 长度7 非子节点最大索引7/2</span></span><br><span class=\"line\">n1.r.r.v</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>树相关汇总</p>\n</blockquote>","more":"<h3 id=\"普通遍历\"><a href=\"#普通遍历\" class=\"headerlink\" title=\"普通遍历\"></a>普通遍历</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 前序遍历(根左右)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">preOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\">    preOrderTravesal(root.l)</span><br><span class=\"line\">    preOrderTravesal(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 中序遍历(左根右)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    inOrderTravesal(root.l)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\">    inOrderTravesal(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 后续遍历(左右根)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">posOrderTravesal</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    posOrderTravesal(root.l)</span><br><span class=\"line\">    posOrderTravesal(root.r)</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(root.v)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归前序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">preOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = [] <span class=\"comment\"># 结果</span></span><br><span class=\"line\">    stack = []  <span class=\"comment\"># 缓存</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>: </span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>: <span class=\"comment\"># 遍历左子树</span></span><br><span class=\"line\">            result.append(root.v) <span class=\"comment\"># 根结点先入结果集</span></span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] <span class=\"comment\"># 弹出</span></span><br><span class=\"line\">        stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] <span class=\"comment\"># 清理缓存</span></span><br><span class=\"line\">        root = node.r <span class=\"comment\"># root为弹出节点的右节点</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归中序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">inOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    stack = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        result.append(node.v) <span class=\"comment\"># 左节点先入结果集(和前序区别)</span></span><br><span class=\"line\">        root = node.r</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 非递归后序遍历</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">posOrder</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    stack = []</span><br><span class=\"line\">    lastVisit = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span> <span class=\"keyword\">or</span> <span class=\"built_in\">len</span>(stack)!=<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> root!=<span class=\"literal\">None</span>:</span><br><span class=\"line\">            stack.append(root)</span><br><span class=\"line\">            root = root.l</span><br><span class=\"line\">        node = stack[<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.r==<span class=\"literal\">None</span> <span class=\"keyword\">or</span> node.r==lastVisit: <span class=\"comment\"># 确保右节点先于跟节点进入结果集</span></span><br><span class=\"line\">            stack = stack[:<span class=\"built_in\">len</span>(stack)-<span class=\"number\">1</span>] </span><br><span class=\"line\">            result.append(node.v) </span><br><span class=\"line\">            lastVisit = node</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            root = node.r <span class=\"comment\"># 确保右节点先于跟节点进入结果集</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"优先遍历\"><a href=\"#优先遍历\" class=\"headerlink\" title=\"优先遍历\"></a>优先遍历</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 深度优先遍历(前序遍历)</span></span><br><span class=\"line\">result = []</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dfs</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> root==<span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span></span><br><span class=\"line\">    result.append(root.v)</span><br><span class=\"line\">    dfs(root.l)</span><br><span class=\"line\">    dfs(root.r)</span><br><span class=\"line\"><span class=\"comment\"># 广度优先遍历(层次遍历)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bfs</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> root==<span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> </span><br><span class=\"line\">    queue = [root]</span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> queue:</span><br><span class=\"line\">        node = queue.pop(<span class=\"number\">0</span>)</span><br><span class=\"line\">        result.append(node.v)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.l: queue.append(node.l)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> node.r: queue.append(node.r)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"变种\"><a href=\"#变种\" class=\"headerlink\" title=\"变种\"></a>变种</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 最大深度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxDepth</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(maxDepth(root.l), maxDepth(root.r))+<span class=\"number\">1</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 是否平衡树(平衡返回最大深度)</span></span><br><span class=\"line\"><span class=\"comment\"># 平衡树:左右子树深度差不超过1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">balanceTree</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    left = maxDepth(root.l)</span><br><span class=\"line\">    right = maxDepth(root.r)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">abs</span>(left-right)&gt;<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(left,right)+<span class=\"number\">1</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># 最大路径值和</span></span><br><span class=\"line\">result = <span class=\"built_in\">float</span>(<span class=\"string\">&quot;-inf&quot;</span>)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">maxPathSum</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    left = maxPathSum(root.l)</span><br><span class=\"line\">    right = maxPathSum(root.r)</span><br><span class=\"line\">    <span class=\"keyword\">global</span> result</span><br><span class=\"line\">    result = <span class=\"built_in\">max</span>(left + right + root.v, result) <span class=\"comment\"># 如果当前节点为根节点最大路径和</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">max</span>(<span class=\"number\">0</span>, <span class=\"built_in\">max</span>(left, right) + root.v) <span class=\"comment\"># 当前节点作为子节点能提供的最大路径和</span></span><br><span class=\"line\">maxPathSum(n1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 公共祖先</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">commonAncester</span>(<span class=\"params\">root, p, q</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root <span class=\"keyword\">or</span> root==p <span class=\"keyword\">or</span> root==q:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> root</span><br><span class=\"line\">    left = commonAncester(root.l, p, q) <span class=\"comment\"># 是否包含p或q</span></span><br><span class=\"line\">    right = commonAncester(root.r, p, q)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> left: <span class=\"keyword\">return</span> right <span class=\"comment\"># 没在左说明在右</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> right: <span class=\"keyword\">return</span> left <span class=\"comment\"># 没在右说明在左</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> root <span class=\"comment\"># 左右都有说明跟节点是公共祖先</span></span><br><span class=\"line\">commonAncester(n1,n2,n3).v</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 是否是二叉搜索树</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bst</span>(<span class=\"params\">root</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">helper</span>(<span class=\"params\">root, minv, maxv</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">True</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> minv&lt;root.v&lt;maxv:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> helper(root.l, minv, root.v) <span class=\"keyword\">and</span> helper(root.r, root.v, maxv)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> helper(root, <span class=\"built_in\">float</span>(<span class=\"string\">&quot;-inf&quot;</span>), <span class=\"built_in\">float</span>(<span class=\"string\">&quot;inf&quot;</span>))</span><br><span class=\"line\">bst(n1)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 插入bst</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertBST</span>(<span class=\"params\">root, v</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> root:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> TreeNode(v)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> root.v &lt; v:</span><br><span class=\"line\">        root.r = insertBST(root.r, v)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> root.v &gt; v:</span><br><span class=\"line\">        root.l = insertBST(root.l, v)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> root</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"建树\"><a href=\"#建树\" class=\"headerlink\" title=\"建树\"></a>建树</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TreeNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, v, l, r</span>):</span></span><br><span class=\"line\">        self.v = v</span><br><span class=\"line\">        self.l = l</span><br><span class=\"line\">        self.r = r</span><br><span class=\"line\">n1,n2,n3,n4,n5,n6,n7 = TreeNode(<span class=\"number\">1</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">2</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">3</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">4</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">5</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">6</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>),TreeNode(<span class=\"number\">7</span>,<span class=\"literal\">None</span>,<span class=\"literal\">None</span>)</span><br><span class=\"line\">n1.l=n2</span><br><span class=\"line\">n1.r=n3</span><br><span class=\"line\">n2.l=n4</span><br><span class=\"line\">n2.r=n5</span><br><span class=\"line\">n3.l=n6</span><br><span class=\"line\">n3.r=n7</span><br><span class=\"line\"><span class=\"comment\">#    1</span></span><br><span class=\"line\"><span class=\"comment\">#  2   3</span></span><br><span class=\"line\"><span class=\"comment\"># 4 5 6 7</span></span><br><span class=\"line\"><span class=\"comment\"># 3层 长度7 非子节点最大索引7/2</span></span><br><span class=\"line\">n1.r.r.v</span><br></pre></td></tr></table></figure>"},{"title":"linked-list","date":"2022-12-07T08:58:27.000Z","_content":"\n> 链表相关汇总\n\n<!-- more -->\n\n### 总结\n\n```python\n#【去重】有序重复链表元素\ndef distinctLinkedList(head):\n    dummy = head # 虚构指针指向头节点\n    while head:\n        while head.n and head.v == head.n.v:\n            head.n = head.n.n\n        head = head.n\n    return dummy \n# 【删除】有序重复链表元素\ndef deleteLinkedList(head):\n    dummy = ListNode(0)\n    dummy.n = head # 虚构指针连接头结点\n    head = dummy  # 头节点指向虚构节点\n    while head.n and head.n.n: \n        if head.n.v == head.n.n.v: # 如果重复\n            dv = head.n.v # 记录重复的值\n            while head.n and head.n.v == dv: # 所有重复值\n                head.n = head.n.n\n        else:\n            head = head.n\n    return dummy.n    \n# 翻转链表\ndef reverseLinkedList(head):\n    if not head:\n        return head\n    dummy = head\n    while head.n:\n        next = head.n # 把头直连的节点摘出来\n        head.n = head.n.n # 摘出来之后前后连上\n        next.n = dummy # 摘出来的节点放到最前\n        dummy = next # 摘出来的节点变为最前\n    return dummy \n# 翻转链表第m到第n的节点\ndef reverseLinkedListMN(head,m,n):\n    dummy = ListNode(0)\n    dummy.n = head\n    prev = dummy # 记录前面节点\n    for _ in range(m-1): # 头结点走到m节点\n        prev = prev.n\n        head = head.n\n    for _ in range(n-m): # head之后有n-m个节点要塞到前面\n        next = head.n # 把头直连的节点摘出来\n        head.n = head.n.n # 摘出来之后前后连上\n        next.n = prev.n # 摘出来的节点放到需要翻转的节点最前(也就是prev之后)\n        prev.n = next # prev连接最新翻转过来的节点\n    return dummy.n   \n# 翻转链表【回溯法】\ndef reverseLinkedListRecall(head):\n    if not head: # 空链\n        return head\n    if not head.n: # 最后节点\n        return head \n    next = reverseLinkedListRecall(head.n) # 取得最后节点只作最后返回用\n    head.n.n = head # 翻转连接 左右都直连head head连接None\n    head.n = None\n    return next\n  \nclass ListNode:\n    def __init__(self, v, n=None):\n        self.v = v\n        self.n = n\nx=ListNode(1)\ny=ListNode(2)\nx.n=y\nprint(x.v)\nprint(x.n.v)\n```\n\n","source":"_posts/linked-list.md","raw":"---\ntitle: linked-list\ndate: 2022-12-07 16:58:27\ntags: 链表\n---\n\n> 链表相关汇总\n\n<!-- more -->\n\n### 总结\n\n```python\n#【去重】有序重复链表元素\ndef distinctLinkedList(head):\n    dummy = head # 虚构指针指向头节点\n    while head:\n        while head.n and head.v == head.n.v:\n            head.n = head.n.n\n        head = head.n\n    return dummy \n# 【删除】有序重复链表元素\ndef deleteLinkedList(head):\n    dummy = ListNode(0)\n    dummy.n = head # 虚构指针连接头结点\n    head = dummy  # 头节点指向虚构节点\n    while head.n and head.n.n: \n        if head.n.v == head.n.n.v: # 如果重复\n            dv = head.n.v # 记录重复的值\n            while head.n and head.n.v == dv: # 所有重复值\n                head.n = head.n.n\n        else:\n            head = head.n\n    return dummy.n    \n# 翻转链表\ndef reverseLinkedList(head):\n    if not head:\n        return head\n    dummy = head\n    while head.n:\n        next = head.n # 把头直连的节点摘出来\n        head.n = head.n.n # 摘出来之后前后连上\n        next.n = dummy # 摘出来的节点放到最前\n        dummy = next # 摘出来的节点变为最前\n    return dummy \n# 翻转链表第m到第n的节点\ndef reverseLinkedListMN(head,m,n):\n    dummy = ListNode(0)\n    dummy.n = head\n    prev = dummy # 记录前面节点\n    for _ in range(m-1): # 头结点走到m节点\n        prev = prev.n\n        head = head.n\n    for _ in range(n-m): # head之后有n-m个节点要塞到前面\n        next = head.n # 把头直连的节点摘出来\n        head.n = head.n.n # 摘出来之后前后连上\n        next.n = prev.n # 摘出来的节点放到需要翻转的节点最前(也就是prev之后)\n        prev.n = next # prev连接最新翻转过来的节点\n    return dummy.n   \n# 翻转链表【回溯法】\ndef reverseLinkedListRecall(head):\n    if not head: # 空链\n        return head\n    if not head.n: # 最后节点\n        return head \n    next = reverseLinkedListRecall(head.n) # 取得最后节点只作最后返回用\n    head.n.n = head # 翻转连接 左右都直连head head连接None\n    head.n = None\n    return next\n  \nclass ListNode:\n    def __init__(self, v, n=None):\n        self.v = v\n        self.n = n\nx=ListNode(1)\ny=ListNode(2)\nx.n=y\nprint(x.v)\nprint(x.n.v)\n```\n\n","slug":"linked-list","published":1,"updated":"2022-12-19T01:30:52.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh8000916gnb6zs7ydn","content":"<blockquote>\n<p>链表相关汇总</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#【去重】有序重复链表元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distinctLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    dummy = head <span class=\"comment\"># 虚构指针指向头节点</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> head:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.v == head.n.v:</span><br><span class=\"line\">            head.n = head.n.n</span><br><span class=\"line\">        head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy </span><br><span class=\"line\"><span class=\"comment\"># 【删除】有序重复链表元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deleteLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    dummy = ListNode(<span class=\"number\">0</span>)</span><br><span class=\"line\">    dummy.n = head <span class=\"comment\"># 虚构指针连接头结点</span></span><br><span class=\"line\">    head = dummy  <span class=\"comment\"># 头节点指向虚构节点</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.n.n: </span><br><span class=\"line\">        <span class=\"keyword\">if</span> head.n.v == head.n.n.v: <span class=\"comment\"># 如果重复</span></span><br><span class=\"line\">            dv = head.n.v <span class=\"comment\"># 记录重复的值</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.n.v == dv: <span class=\"comment\"># 所有重复值</span></span><br><span class=\"line\">                head.n = head.n.n</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy.n    </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\">    dummy = head</span><br><span class=\"line\">    <span class=\"keyword\">while</span> head.n:</span><br><span class=\"line\">        <span class=\"built_in\">next</span> = head.n <span class=\"comment\"># 把头直连的节点摘出来</span></span><br><span class=\"line\">        head.n = head.n.n <span class=\"comment\"># 摘出来之后前后连上</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span>.n = dummy <span class=\"comment\"># 摘出来的节点放到最前</span></span><br><span class=\"line\">        dummy = <span class=\"built_in\">next</span> <span class=\"comment\"># 摘出来的节点变为最前</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表第m到第n的节点</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedListMN</span>(<span class=\"params\">head,m,n</span>):</span></span><br><span class=\"line\">    dummy = ListNode(<span class=\"number\">0</span>)</span><br><span class=\"line\">    dummy.n = head</span><br><span class=\"line\">    prev = dummy <span class=\"comment\"># 记录前面节点</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m-<span class=\"number\">1</span>): <span class=\"comment\"># 头结点走到m节点</span></span><br><span class=\"line\">        prev = prev.n</span><br><span class=\"line\">        head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n-m): <span class=\"comment\"># head之后有n-m个节点要塞到前面</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span> = head.n <span class=\"comment\"># 把头直连的节点摘出来</span></span><br><span class=\"line\">        head.n = head.n.n <span class=\"comment\"># 摘出来之后前后连上</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span>.n = prev.n <span class=\"comment\"># 摘出来的节点放到需要翻转的节点最前(也就是prev之后)</span></span><br><span class=\"line\">        prev.n = <span class=\"built_in\">next</span> <span class=\"comment\"># prev连接最新翻转过来的节点</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy.n   </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表【回溯法】</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedListRecall</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head: <span class=\"comment\"># 空链</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head.n: <span class=\"comment\"># 最后节点</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head </span><br><span class=\"line\">    <span class=\"built_in\">next</span> = reverseLinkedListRecall(head.n) <span class=\"comment\"># 取得最后节点只作最后返回用</span></span><br><span class=\"line\">    head.n.n = head <span class=\"comment\"># 翻转连接 左右都直连head head连接None</span></span><br><span class=\"line\">    head.n = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">next</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, v, n=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">        self.v = v</span><br><span class=\"line\">        self.n = n</span><br><span class=\"line\">x=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">y=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">x.n=y</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.v)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.n.v)</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>链表相关汇总</p>\n</blockquote>","more":"<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#【去重】有序重复链表元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">distinctLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    dummy = head <span class=\"comment\"># 虚构指针指向头节点</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> head:</span><br><span class=\"line\">        <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.v == head.n.v:</span><br><span class=\"line\">            head.n = head.n.n</span><br><span class=\"line\">        head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy </span><br><span class=\"line\"><span class=\"comment\"># 【删除】有序重复链表元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">deleteLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    dummy = ListNode(<span class=\"number\">0</span>)</span><br><span class=\"line\">    dummy.n = head <span class=\"comment\"># 虚构指针连接头结点</span></span><br><span class=\"line\">    head = dummy  <span class=\"comment\"># 头节点指向虚构节点</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.n.n: </span><br><span class=\"line\">        <span class=\"keyword\">if</span> head.n.v == head.n.n.v: <span class=\"comment\"># 如果重复</span></span><br><span class=\"line\">            dv = head.n.v <span class=\"comment\"># 记录重复的值</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> head.n <span class=\"keyword\">and</span> head.n.v == dv: <span class=\"comment\"># 所有重复值</span></span><br><span class=\"line\">                head.n = head.n.n</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy.n    </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedList</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\">    dummy = head</span><br><span class=\"line\">    <span class=\"keyword\">while</span> head.n:</span><br><span class=\"line\">        <span class=\"built_in\">next</span> = head.n <span class=\"comment\"># 把头直连的节点摘出来</span></span><br><span class=\"line\">        head.n = head.n.n <span class=\"comment\"># 摘出来之后前后连上</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span>.n = dummy <span class=\"comment\"># 摘出来的节点放到最前</span></span><br><span class=\"line\">        dummy = <span class=\"built_in\">next</span> <span class=\"comment\"># 摘出来的节点变为最前</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表第m到第n的节点</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedListMN</span>(<span class=\"params\">head,m,n</span>):</span></span><br><span class=\"line\">    dummy = ListNode(<span class=\"number\">0</span>)</span><br><span class=\"line\">    dummy.n = head</span><br><span class=\"line\">    prev = dummy <span class=\"comment\"># 记录前面节点</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(m-<span class=\"number\">1</span>): <span class=\"comment\"># 头结点走到m节点</span></span><br><span class=\"line\">        prev = prev.n</span><br><span class=\"line\">        head = head.n</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n-m): <span class=\"comment\"># head之后有n-m个节点要塞到前面</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span> = head.n <span class=\"comment\"># 把头直连的节点摘出来</span></span><br><span class=\"line\">        head.n = head.n.n <span class=\"comment\"># 摘出来之后前后连上</span></span><br><span class=\"line\">        <span class=\"built_in\">next</span>.n = prev.n <span class=\"comment\"># 摘出来的节点放到需要翻转的节点最前(也就是prev之后)</span></span><br><span class=\"line\">        prev.n = <span class=\"built_in\">next</span> <span class=\"comment\"># prev连接最新翻转过来的节点</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> dummy.n   </span><br><span class=\"line\"><span class=\"comment\"># 翻转链表【回溯法】</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">reverseLinkedListRecall</span>(<span class=\"params\">head</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head: <span class=\"comment\"># 空链</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> head.n: <span class=\"comment\"># 最后节点</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> head </span><br><span class=\"line\">    <span class=\"built_in\">next</span> = reverseLinkedListRecall(head.n) <span class=\"comment\"># 取得最后节点只作最后返回用</span></span><br><span class=\"line\">    head.n.n = head <span class=\"comment\"># 翻转连接 左右都直连head head连接None</span></span><br><span class=\"line\">    head.n = <span class=\"literal\">None</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"built_in\">next</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ListNode</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, v, n=<span class=\"literal\">None</span></span>):</span></span><br><span class=\"line\">        self.v = v</span><br><span class=\"line\">        self.n = n</span><br><span class=\"line\">x=ListNode(<span class=\"number\">1</span>)</span><br><span class=\"line\">y=ListNode(<span class=\"number\">2</span>)</span><br><span class=\"line\">x.n=y</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.v)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.n.v)</span><br></pre></td></tr></table></figure>"},{"title":"markdown-rule","date":"2022-01-23T09:13:37.000Z","_content":"> markdown语法示例。\n\n<!-- more -->\n\n# 一级标题\n## 二级标题\n### 三级标题\n#### 四级标题\n\n# 无序列表\n- 列表1\n  - 列表1.1\n  - 列表1.2\n- 列表2\n- 列表3\n- \n\n# 引用\n> 引用一段话\n\n# 斜体和粗体\n*斜体*\n\n**粗体**\n\n# 链接和图片\n[链接](http://note.youdao.com/)\n\n![图片](http://note.youdao.com/favicon.ico)\n\n# 分割线\n第一段\n***\n第二段\n\n# 表格\n| item | value |qty |\n| :--- | ---:  |:--:|\n|  w 1 | col 1 | 5  |\n| w 2  | col 1 | 12 |\n\n# 代码高亮\n``` python\ndef main:\n    print(\"123\")\n\n```\n\n# todolist\n- [x] 已完成项目\n    - [x] 已完成事项1\n    - [x] 已完成事项2\n- [ ] 代办事项1\n\n# 流程图\n```\ngraph TD\n    A[Christmas] -->B(Go shopping)\n    B -->C{Let me think}\n    C -->|One| D[Laptop]\n    C -->|Two| E[iphone]\n    C -->|Three| F[Car]\n```\n\n# 序列图\n```\nsequenceDiagram\n    loop every day\n        Alice->>John:Hello John, how are you?\n        Jojn-->>Alice:Great!\n    end\n```\n\n# 甘特图\n```\ngantt\ndateFormat YYYY-MM-DD\ntitle 产品计划表\nsection 初期阶段\n明确需求:2016-03-01,10d\nsection 中期阶段\n跟进开发:2016-03-11,15d\nsection 后期阶段\n走查测试:2016-03-20,9d\n```\n\n# 数学公式\nInline math:`$dfrac{\n\\tfrac{1}{2}[1-(\\tfrac{1}{2})^n] }{\n    1-\\tfrac{1}{2}} = s_n$`\n\n[参考文档](https://blog.csdn.net/coolyoung520/article/details/108960281)\n\n","source":"_posts/markdown-rule.md","raw":"---\ntitle: markdown-rule\ndate: 2022-01-23 17:13:37\ntags:\n---\n> markdown语法示例。\n\n<!-- more -->\n\n# 一级标题\n## 二级标题\n### 三级标题\n#### 四级标题\n\n# 无序列表\n- 列表1\n  - 列表1.1\n  - 列表1.2\n- 列表2\n- 列表3\n- \n\n# 引用\n> 引用一段话\n\n# 斜体和粗体\n*斜体*\n\n**粗体**\n\n# 链接和图片\n[链接](http://note.youdao.com/)\n\n![图片](http://note.youdao.com/favicon.ico)\n\n# 分割线\n第一段\n***\n第二段\n\n# 表格\n| item | value |qty |\n| :--- | ---:  |:--:|\n|  w 1 | col 1 | 5  |\n| w 2  | col 1 | 12 |\n\n# 代码高亮\n``` python\ndef main:\n    print(\"123\")\n\n```\n\n# todolist\n- [x] 已完成项目\n    - [x] 已完成事项1\n    - [x] 已完成事项2\n- [ ] 代办事项1\n\n# 流程图\n```\ngraph TD\n    A[Christmas] -->B(Go shopping)\n    B -->C{Let me think}\n    C -->|One| D[Laptop]\n    C -->|Two| E[iphone]\n    C -->|Three| F[Car]\n```\n\n# 序列图\n```\nsequenceDiagram\n    loop every day\n        Alice->>John:Hello John, how are you?\n        Jojn-->>Alice:Great!\n    end\n```\n\n# 甘特图\n```\ngantt\ndateFormat YYYY-MM-DD\ntitle 产品计划表\nsection 初期阶段\n明确需求:2016-03-01,10d\nsection 中期阶段\n跟进开发:2016-03-11,15d\nsection 后期阶段\n走查测试:2016-03-20,9d\n```\n\n# 数学公式\nInline math:`$dfrac{\n\\tfrac{1}{2}[1-(\\tfrac{1}{2})^n] }{\n    1-\\tfrac{1}{2}} = s_n$`\n\n[参考文档](https://blog.csdn.net/coolyoung520/article/details/108960281)\n\n","slug":"markdown-rule","published":1,"updated":"2022-12-19T01:30:52.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh8000b16gn4wibeqru","content":"<blockquote>\n<p>markdown语法示例。</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><h1 id=\"无序列表\"><a href=\"#无序列表\" class=\"headerlink\" title=\"无序列表\"></a>无序列表</h1><ul>\n<li>列表1<ul>\n<li>列表1.1</li>\n<li>列表1.2</li>\n</ul>\n</li>\n<li>列表2</li>\n<li>列表3</li>\n<li></li>\n</ul>\n<h1 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h1><blockquote>\n<p>引用一段话</p>\n</blockquote>\n<h1 id=\"斜体和粗体\"><a href=\"#斜体和粗体\" class=\"headerlink\" title=\"斜体和粗体\"></a>斜体和粗体</h1><p><em>斜体</em></p>\n<p><strong>粗体</strong></p>\n<h1 id=\"链接和图片\"><a href=\"#链接和图片\" class=\"headerlink\" title=\"链接和图片\"></a>链接和图片</h1><p><a href=\"http://note.youdao.com/\">链接</a></p>\n<p><img src=\"http://note.youdao.com/favicon.ico\" alt=\"图片\"></p>\n<h1 id=\"分割线\"><a href=\"#分割线\" class=\"headerlink\" title=\"分割线\"></a>分割线</h1><p>第一段</p>\n<hr>\n<p>第二段</p>\n<h1 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h1><table>\n<thead>\n<tr>\n<th align=\"left\">item</th>\n<th align=\"right\">value</th>\n<th align=\"center\">qty</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">w 1</td>\n<td align=\"right\">col 1</td>\n<td align=\"center\">5</td>\n</tr>\n<tr>\n<td align=\"left\">w 2</td>\n<td align=\"right\">col 1</td>\n<td align=\"center\">12</td>\n</tr>\n</tbody></table>\n<h1 id=\"代码高亮\"><a href=\"#代码高亮\" class=\"headerlink\" title=\"代码高亮\"></a>代码高亮</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span>:</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;123&quot;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"todolist\"><a href=\"#todolist\" class=\"headerlink\" title=\"todolist\"></a>todolist</h1><ul>\n<li><input checked disabled type=\"checkbox\"> 已完成项目<ul>\n<li><input checked disabled type=\"checkbox\"> 已完成事项1</li>\n<li><input checked disabled type=\"checkbox\"> 已完成事项2</li>\n</ul>\n</li>\n<li><input disabled type=\"checkbox\"> 代办事项1</li>\n</ul>\n<h1 id=\"流程图\"><a href=\"#流程图\" class=\"headerlink\" title=\"流程图\"></a>流程图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph TD</span><br><span class=\"line\">    A[Christmas] --&gt;B(Go shopping)</span><br><span class=\"line\">    B --&gt;C&#123;Let me think&#125;</span><br><span class=\"line\">    C --&gt;|One| D[Laptop]</span><br><span class=\"line\">    C --&gt;|Two| E[iphone]</span><br><span class=\"line\">    C --&gt;|Three| F[Car]</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"序列图\"><a href=\"#序列图\" class=\"headerlink\" title=\"序列图\"></a>序列图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sequenceDiagram</span><br><span class=\"line\">    loop every day</span><br><span class=\"line\">        Alice-&gt;&gt;John:Hello John, how are you?</span><br><span class=\"line\">        Jojn--&gt;&gt;Alice:Great!</span><br><span class=\"line\">    end</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"甘特图\"><a href=\"#甘特图\" class=\"headerlink\" title=\"甘特图\"></a>甘特图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gantt</span><br><span class=\"line\">dateFormat YYYY-MM-DD</span><br><span class=\"line\">title 产品计划表</span><br><span class=\"line\">section 初期阶段</span><br><span class=\"line\">明确需求:2016-03-01,10d</span><br><span class=\"line\">section 中期阶段</span><br><span class=\"line\">跟进开发:2016-03-11,15d</span><br><span class=\"line\">section 后期阶段</span><br><span class=\"line\">走查测试:2016-03-20,9d</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"数学公式\"><a href=\"#数学公式\" class=\"headerlink\" title=\"数学公式\"></a>数学公式</h1><p>Inline math:<code>$dfrac&#123; \\tfrac&#123;1&#125;&#123;2&#125;[1-(\\tfrac&#123;1&#125;&#123;2&#125;)^n] &#125;&#123;     1-\\tfrac&#123;1&#125;&#123;2&#125;&#125; = s_n$</code></p>\n<p><a href=\"https://blog.csdn.net/coolyoung520/article/details/108960281\">参考文档</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>markdown语法示例。</p>\n</blockquote>","more":"<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><h1 id=\"无序列表\"><a href=\"#无序列表\" class=\"headerlink\" title=\"无序列表\"></a>无序列表</h1><ul>\n<li>列表1<ul>\n<li>列表1.1</li>\n<li>列表1.2</li>\n</ul>\n</li>\n<li>列表2</li>\n<li>列表3</li>\n<li></li>\n</ul>\n<h1 id=\"引用\"><a href=\"#引用\" class=\"headerlink\" title=\"引用\"></a>引用</h1><blockquote>\n<p>引用一段话</p>\n</blockquote>\n<h1 id=\"斜体和粗体\"><a href=\"#斜体和粗体\" class=\"headerlink\" title=\"斜体和粗体\"></a>斜体和粗体</h1><p><em>斜体</em></p>\n<p><strong>粗体</strong></p>\n<h1 id=\"链接和图片\"><a href=\"#链接和图片\" class=\"headerlink\" title=\"链接和图片\"></a>链接和图片</h1><p><a href=\"http://note.youdao.com/\">链接</a></p>\n<p><img src=\"http://note.youdao.com/favicon.ico\" alt=\"图片\"></p>\n<h1 id=\"分割线\"><a href=\"#分割线\" class=\"headerlink\" title=\"分割线\"></a>分割线</h1><p>第一段</p>\n<hr>\n<p>第二段</p>\n<h1 id=\"表格\"><a href=\"#表格\" class=\"headerlink\" title=\"表格\"></a>表格</h1><table>\n<thead>\n<tr>\n<th align=\"left\">item</th>\n<th align=\"right\">value</th>\n<th align=\"center\">qty</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">w 1</td>\n<td align=\"right\">col 1</td>\n<td align=\"center\">5</td>\n</tr>\n<tr>\n<td align=\"left\">w 2</td>\n<td align=\"right\">col 1</td>\n<td align=\"center\">12</td>\n</tr>\n</tbody></table>\n<h1 id=\"代码高亮\"><a href=\"#代码高亮\" class=\"headerlink\" title=\"代码高亮\"></a>代码高亮</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span>:</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;123&quot;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"todolist\"><a href=\"#todolist\" class=\"headerlink\" title=\"todolist\"></a>todolist</h1><ul>\n<li><input checked disabled type=\"checkbox\"> 已完成项目<ul>\n<li><input checked disabled type=\"checkbox\"> 已完成事项1</li>\n<li><input checked disabled type=\"checkbox\"> 已完成事项2</li>\n</ul>\n</li>\n<li><input disabled type=\"checkbox\"> 代办事项1</li>\n</ul>\n<h1 id=\"流程图\"><a href=\"#流程图\" class=\"headerlink\" title=\"流程图\"></a>流程图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">graph TD</span><br><span class=\"line\">    A[Christmas] --&gt;B(Go shopping)</span><br><span class=\"line\">    B --&gt;C&#123;Let me think&#125;</span><br><span class=\"line\">    C --&gt;|One| D[Laptop]</span><br><span class=\"line\">    C --&gt;|Two| E[iphone]</span><br><span class=\"line\">    C --&gt;|Three| F[Car]</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"序列图\"><a href=\"#序列图\" class=\"headerlink\" title=\"序列图\"></a>序列图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sequenceDiagram</span><br><span class=\"line\">    loop every day</span><br><span class=\"line\">        Alice-&gt;&gt;John:Hello John, how are you?</span><br><span class=\"line\">        Jojn--&gt;&gt;Alice:Great!</span><br><span class=\"line\">    end</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"甘特图\"><a href=\"#甘特图\" class=\"headerlink\" title=\"甘特图\"></a>甘特图</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gantt</span><br><span class=\"line\">dateFormat YYYY-MM-DD</span><br><span class=\"line\">title 产品计划表</span><br><span class=\"line\">section 初期阶段</span><br><span class=\"line\">明确需求:2016-03-01,10d</span><br><span class=\"line\">section 中期阶段</span><br><span class=\"line\">跟进开发:2016-03-11,15d</span><br><span class=\"line\">section 后期阶段</span><br><span class=\"line\">走查测试:2016-03-20,9d</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"数学公式\"><a href=\"#数学公式\" class=\"headerlink\" title=\"数学公式\"></a>数学公式</h1><p>Inline math:<code>$dfrac&#123; \\tfrac&#123;1&#125;&#123;2&#125;[1-(\\tfrac&#123;1&#125;&#123;2&#125;)^n] &#125;&#123;     1-\\tfrac&#123;1&#125;&#123;2&#125;&#125; = s_n$</code></p>\n<p><a href=\"https://blog.csdn.net/coolyoung520/article/details/108960281\">参考文档</a></p>"},{"title":"pytorch相关","date":"2022-01-23T07:08:10.000Z","_content":"> pytorch学习ing...\n\n<!-- more -->\n\n```python\n# 安装 https://pytorch.org/get-started/locally/\n# cuda 11.0\npip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n```\n\n","source":"_posts/pytorch-log.md","raw":"---\ntitle: pytorch相关\ndate: 2022-01-23 15:08:10\ntags: pytorch\n---\n> pytorch学习ing...\n\n<!-- more -->\n\n```python\n# 安装 https://pytorch.org/get-started/locally/\n# cuda 11.0\npip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n```\n\n","slug":"pytorch-log","published":1,"updated":"2022-12-19T01:30:52.684Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh8000d16gn3yvka122","content":"<blockquote>\n<p>pytorch学习ing…</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 安装 https://pytorch.org/get-started/locally/</span></span><br><span class=\"line\"><span class=\"comment\"># cuda 11.0</span></span><br><span class=\"line\">pip3 install torch==<span class=\"number\">1.10</span><span class=\"number\">.1</span>+cu113 torchvision==<span class=\"number\">0.11</span><span class=\"number\">.2</span>+cu113 torchaudio==<span class=\"number\">0.10</span><span class=\"number\">.1</span>+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>pytorch学习ing…</p>\n</blockquote>","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 安装 https://pytorch.org/get-started/locally/</span></span><br><span class=\"line\"><span class=\"comment\"># cuda 11.0</span></span><br><span class=\"line\">pip3 install torch==<span class=\"number\">1.10</span><span class=\"number\">.1</span>+cu113 torchvision==<span class=\"number\">0.11</span><span class=\"number\">.2</span>+cu113 torchaudio==<span class=\"number\">0.10</span><span class=\"number\">.1</span>+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html</span><br></pre></td></tr></table></figure>"},{"title":"Hexo相关","_content":"\n![测试图片](./hello-world/2022020901.jpeg)\n\n> Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!-- more -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### CLEAN\n\n```bash\n$ hexo clean \n```\n\n\n\n### Run server\n\n``` bash\n$ hexo server / hexo s\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate / hexo g\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy / hexo d\n```\n\n### 补充\n\n>Q: github 403 \n>A: hexo用户和git默认用户不一致，增加新用户并在～/.ssh/config配置，另外在_config.yml中https改为ssh\n>\n>Q: next主题设置失败\n>A: 原因是hexo在5.0之后把swig给删除了需要自己手动安装 npm i hexo-renderer-swig\n>\n>参考：https://zhuanlan.zhihu.com/p/26625249\n>\n>Q:hexo多机部署主题丢失\n>\n>参考：https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hexo相关\n---\n\n![测试图片](./hello-world/2022020901.jpeg)\n\n> Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!-- more -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### CLEAN\n\n```bash\n$ hexo clean \n```\n\n\n\n### Run server\n\n``` bash\n$ hexo server / hexo s\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate / hexo g\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy / hexo d\n```\n\n### 补充\n\n>Q: github 403 \n>A: hexo用户和git默认用户不一致，增加新用户并在～/.ssh/config配置，另外在_config.yml中https改为ssh\n>\n>Q: next主题设置失败\n>A: 原因是hexo在5.0之后把swig给删除了需要自己手动安装 npm i hexo-renderer-swig\n>\n>参考：https://zhuanlan.zhihu.com/p/26625249\n>\n>Q:hexo多机部署主题丢失\n>\n>参考：https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2022-12-19T01:30:52.669Z","updated":"2022-12-19T01:30:52.669Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh9000f16gn5y4jccx7","content":"<p><img src=\"/2022/12/19/hello-world/2022020901.jpeg\" alt=\"测试图片\"></p>\n<blockquote>\n<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"CLEAN\"><a href=\"#CLEAN\" class=\"headerlink\" title=\"CLEAN\"></a>CLEAN</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean </span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server / hexo s</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate / hexo g</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy / hexo d</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h3><blockquote>\n<p>Q: github 403<br>A: hexo用户和git默认用户不一致，增加新用户并在～/.ssh/config配置，另外在_config.yml中https改为ssh</p>\n<p>Q: next主题设置失败<br>A: 原因是hexo在5.0之后把swig给删除了需要自己手动安装 npm i hexo-renderer-swig</p>\n<p>参考：<a href=\"https://zhuanlan.zhihu.com/p/26625249\">https://zhuanlan.zhihu.com/p/26625249</a></p>\n<p>Q:hexo多机部署主题丢失</p>\n<p>参考：<a href=\"https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/\">https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/</a></p>\n</blockquote>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"<p><img src=\"/2022/12/19/hello-world/2022020901.jpeg\" alt=\"测试图片\"></p>\n<blockquote>\n<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n</blockquote>","more":"<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"CLEAN\"><a href=\"#CLEAN\" class=\"headerlink\" title=\"CLEAN\"></a>CLEAN</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean </span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server / hexo s</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate / hexo g</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy / hexo d</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"补充\"><a href=\"#补充\" class=\"headerlink\" title=\"补充\"></a>补充</h3><blockquote>\n<p>Q: github 403<br>A: hexo用户和git默认用户不一致，增加新用户并在～/.ssh/config配置，另外在_config.yml中https改为ssh</p>\n<p>Q: next主题设置失败<br>A: 原因是hexo在5.0之后把swig给删除了需要自己手动安装 npm i hexo-renderer-swig</p>\n<p>参考：<a href=\"https://zhuanlan.zhihu.com/p/26625249\">https://zhuanlan.zhihu.com/p/26625249</a></p>\n<p>Q:hexo多机部署主题丢失</p>\n<p>参考：<a href=\"https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/\">https://mindawei.github.io/2018/05/01/%E5%A4%9A%E6%9C%BA%E4%BD%BF%E7%94%A8Hexo%E5%8D%9A%E5%AE%A2/</a></p>\n</blockquote>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>"},{"title":"Hive存储","date":"2022-11-23T02:23:43.000Z","Tags":"数据","_content":"\n> Hive存储数据格式对比\n\n<!-- more -->\n\n## Hive存储\n\n### 概览\n\n* 行式存储\n  * 文本格式（TextFile）\n  * 二进制序列化文件（SequenceFile）\n\n* 列式存储\n  * 行列式文件（RCFile）\n  * 优化的行列式文件（ORCFile）\n  * Parquet\n\n### 列式存储优势\n\n  - 查询时只读指定列，查询速度快\n  - 压缩时相同数据类型，压缩比高\n\n### 典型分析\n\n#### TextFile\n\nTextFile直接行式存储，可以直接load方式加载数据，加载速度快；\n\nTextFile反序列化必须逐个字符判断是不是分隔符或行结束符号，开销大；\n\n#### ORCFile\n\n* 1.ORC扩展了RCFile的压缩，除了**游程编码**(run-length)，引入**字典编码**和**BIT编码**；\n\n* 2.每个TASK只输出单个文件，减少NameNode负载；\n\n* 3.支持复杂数据类型，datetime、decimal、struct、list、map等；\n\n* 4.文件可切分，查询时输入数据量少；\n\n> 游程编码：变动长度编码法，AAAABBBCCDEEEE->4A3B2C1D4E;\n>\n> 字典编码：最后存储字典值，每个字典值长度以及字段在字典位置；\n>\n> BIT编码：null则存0否则1，null实际编码不需要存储，不占用存储空间；\n\n结构： \n\n  每个ORC文件横向切分多个stripe，每个stripe列式存储，每个stripe默认250MB；\n\n![orc结构](./hive-data/orc.png)\n\n* 条带(stripe)：ORC文件存储数据；\n* 文件注脚(file footer)：包含了文件中stripe的列表，每个stripe的行数，以及每个列的数据类型；每个列的最小值，最大值，行计数，求和等聚合信息。\n* postscript：含有压缩参数和压缩大小相关信息。\n\nstripe：\n\n* Index data：条带统计信息，数据在条带中位置索引信息。\n* rows data：数据存储，多个**行组**组成，数据以流(stream)形式存储。\n* stripe footer：保存数据所在文件目录。\n\nrows data存储两部分数据，metadata stream和data stream：\n\n* metadata stream：描述每个行组的元数据信息；\n* data stream：存储数据；\n\nORC三级索引：\n\n* 文件级：记录文件中所有stripe的位置信息，以及文件中所存储的每列数据的统计信息；\n* 条带级：记录每个stripe所存储数据的统计信息；\n* 行组级：记录每个行组所存储数据的统计信息，在stripe中，10000行构成一个行组；\n\n三级索引查找过程：\n\n  查询ORC文件类型的表时，会先对比文件级索引列统计信息，找到满足条件的文件；接着，根据条带级索引信息，找到满足条件的条带快，之后在根据行组级索引，找到满足条件的行组；避免遍历全部，减少磁盘和网络I/O。\n\n数据类型（所有类型都接受NULL值）：\n\n* 整型：包含boolean(1 bit)、tiny(8 bit)、smallint(16 bit)、int(32 bit)、bigint(64 bit)\n* 浮点型：包含float和double\n* 字符串类型：包含string、char和varchar\n* 二进制类型：包含binary\n* 日期和时间类型：包含timestamp和date\n* 复杂类型：包含struct、list、map和union类型\n\nACID事务支持：\n\n  Hive 0.14版本之前，只能新增或者删除整块分区或表，而不能对表的单个记录进行修改；\n\n  Hive 0.14版本之后，ORC文件能够保证原子性、一致性、隔离性和持久性的ACID事务能够被支持，支持数据更新；\n\n  Hive事务适用于大批量数据更新，不建议频繁小批量更新；\n\n压缩：\n\n  可选的类型有 NONE、ZLIB 和 SNAPPY，默认值是 ZLIB\n\n#### Parquet\n\n结构：\n\n![parquet结构](./hive-data/parquet.png)\n\n  Parquet存储数据时，同时存储多级元数据，文件级元数据、列块级元数据、页级元数据；\n\n* 文件级元数据：表结构信息；文件记录数；行组数、每个行组数据总量和记录数；每个行组列块的文件偏移量；\n\n* 列块级元数据：压缩前后数据大小和压缩编码；数据页、索引页的偏移量；列块的数据记录数；\n\n* 页级元数据：编码信息和数据记录数；\n\nParquet针对嵌套式结构支持好，ORC多层嵌套表达起来比较复杂，性能损失大；\n\n参考：\n\nhttps://blog.51cto.com/u_14932245/4608206\n","source":"_posts/hive-data.md","raw":"---\ntitle: Hive存储\ndate: 2022-11-23 10:23:43\nTags: 数据\n---\n\n> Hive存储数据格式对比\n\n<!-- more -->\n\n## Hive存储\n\n### 概览\n\n* 行式存储\n  * 文本格式（TextFile）\n  * 二进制序列化文件（SequenceFile）\n\n* 列式存储\n  * 行列式文件（RCFile）\n  * 优化的行列式文件（ORCFile）\n  * Parquet\n\n### 列式存储优势\n\n  - 查询时只读指定列，查询速度快\n  - 压缩时相同数据类型，压缩比高\n\n### 典型分析\n\n#### TextFile\n\nTextFile直接行式存储，可以直接load方式加载数据，加载速度快；\n\nTextFile反序列化必须逐个字符判断是不是分隔符或行结束符号，开销大；\n\n#### ORCFile\n\n* 1.ORC扩展了RCFile的压缩，除了**游程编码**(run-length)，引入**字典编码**和**BIT编码**；\n\n* 2.每个TASK只输出单个文件，减少NameNode负载；\n\n* 3.支持复杂数据类型，datetime、decimal、struct、list、map等；\n\n* 4.文件可切分，查询时输入数据量少；\n\n> 游程编码：变动长度编码法，AAAABBBCCDEEEE->4A3B2C1D4E;\n>\n> 字典编码：最后存储字典值，每个字典值长度以及字段在字典位置；\n>\n> BIT编码：null则存0否则1，null实际编码不需要存储，不占用存储空间；\n\n结构： \n\n  每个ORC文件横向切分多个stripe，每个stripe列式存储，每个stripe默认250MB；\n\n![orc结构](./hive-data/orc.png)\n\n* 条带(stripe)：ORC文件存储数据；\n* 文件注脚(file footer)：包含了文件中stripe的列表，每个stripe的行数，以及每个列的数据类型；每个列的最小值，最大值，行计数，求和等聚合信息。\n* postscript：含有压缩参数和压缩大小相关信息。\n\nstripe：\n\n* Index data：条带统计信息，数据在条带中位置索引信息。\n* rows data：数据存储，多个**行组**组成，数据以流(stream)形式存储。\n* stripe footer：保存数据所在文件目录。\n\nrows data存储两部分数据，metadata stream和data stream：\n\n* metadata stream：描述每个行组的元数据信息；\n* data stream：存储数据；\n\nORC三级索引：\n\n* 文件级：记录文件中所有stripe的位置信息，以及文件中所存储的每列数据的统计信息；\n* 条带级：记录每个stripe所存储数据的统计信息；\n* 行组级：记录每个行组所存储数据的统计信息，在stripe中，10000行构成一个行组；\n\n三级索引查找过程：\n\n  查询ORC文件类型的表时，会先对比文件级索引列统计信息，找到满足条件的文件；接着，根据条带级索引信息，找到满足条件的条带快，之后在根据行组级索引，找到满足条件的行组；避免遍历全部，减少磁盘和网络I/O。\n\n数据类型（所有类型都接受NULL值）：\n\n* 整型：包含boolean(1 bit)、tiny(8 bit)、smallint(16 bit)、int(32 bit)、bigint(64 bit)\n* 浮点型：包含float和double\n* 字符串类型：包含string、char和varchar\n* 二进制类型：包含binary\n* 日期和时间类型：包含timestamp和date\n* 复杂类型：包含struct、list、map和union类型\n\nACID事务支持：\n\n  Hive 0.14版本之前，只能新增或者删除整块分区或表，而不能对表的单个记录进行修改；\n\n  Hive 0.14版本之后，ORC文件能够保证原子性、一致性、隔离性和持久性的ACID事务能够被支持，支持数据更新；\n\n  Hive事务适用于大批量数据更新，不建议频繁小批量更新；\n\n压缩：\n\n  可选的类型有 NONE、ZLIB 和 SNAPPY，默认值是 ZLIB\n\n#### Parquet\n\n结构：\n\n![parquet结构](./hive-data/parquet.png)\n\n  Parquet存储数据时，同时存储多级元数据，文件级元数据、列块级元数据、页级元数据；\n\n* 文件级元数据：表结构信息；文件记录数；行组数、每个行组数据总量和记录数；每个行组列块的文件偏移量；\n\n* 列块级元数据：压缩前后数据大小和压缩编码；数据页、索引页的偏移量；列块的数据记录数；\n\n* 页级元数据：编码信息和数据记录数；\n\nParquet针对嵌套式结构支持好，ORC多层嵌套表达起来比较复杂，性能损失大；\n\n参考：\n\nhttps://blog.51cto.com/u_14932245/4608206\n","slug":"hive-data","published":1,"updated":"2022-12-19T01:30:52.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffh9000h16gnbtod0o29","content":"<blockquote>\n<p>Hive存储数据格式对比</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h2 id=\"Hive存储\"><a href=\"#Hive存储\" class=\"headerlink\" title=\"Hive存储\"></a>Hive存储</h2><h3 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h3><ul>\n<li><p>行式存储</p>\n<ul>\n<li>文本格式（TextFile）</li>\n<li>二进制序列化文件（SequenceFile）</li>\n</ul>\n</li>\n<li><p>列式存储</p>\n<ul>\n<li>行列式文件（RCFile）</li>\n<li>优化的行列式文件（ORCFile）</li>\n<li>Parquet</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"列式存储优势\"><a href=\"#列式存储优势\" class=\"headerlink\" title=\"列式存储优势\"></a>列式存储优势</h3><ul>\n<li>查询时只读指定列，查询速度快</li>\n<li>压缩时相同数据类型，压缩比高</li>\n</ul>\n<h3 id=\"典型分析\"><a href=\"#典型分析\" class=\"headerlink\" title=\"典型分析\"></a>典型分析</h3><h4 id=\"TextFile\"><a href=\"#TextFile\" class=\"headerlink\" title=\"TextFile\"></a>TextFile</h4><p>TextFile直接行式存储，可以直接load方式加载数据，加载速度快；</p>\n<p>TextFile反序列化必须逐个字符判断是不是分隔符或行结束符号，开销大；</p>\n<h4 id=\"ORCFile\"><a href=\"#ORCFile\" class=\"headerlink\" title=\"ORCFile\"></a>ORCFile</h4><ul>\n<li><p>1.ORC扩展了RCFile的压缩，除了<strong>游程编码</strong>(run-length)，引入<strong>字典编码</strong>和<strong>BIT编码</strong>；</p>\n</li>\n<li><p>2.每个TASK只输出单个文件，减少NameNode负载；</p>\n</li>\n<li><p>3.支持复杂数据类型，datetime、decimal、struct、list、map等；</p>\n</li>\n<li><p>4.文件可切分，查询时输入数据量少；</p>\n</li>\n</ul>\n<blockquote>\n<p>游程编码：变动长度编码法，AAAABBBCCDEEEE-&gt;4A3B2C1D4E;</p>\n<p>字典编码：最后存储字典值，每个字典值长度以及字段在字典位置；</p>\n<p>BIT编码：null则存0否则1，null实际编码不需要存储，不占用存储空间；</p>\n</blockquote>\n<p>结构： </p>\n<p>  每个ORC文件横向切分多个stripe，每个stripe列式存储，每个stripe默认250MB；</p>\n<p><img src=\"/2022/11/23/hive-data/orc.png\" alt=\"orc结构\"></p>\n<ul>\n<li>条带(stripe)：ORC文件存储数据；</li>\n<li>文件注脚(file footer)：包含了文件中stripe的列表，每个stripe的行数，以及每个列的数据类型；每个列的最小值，最大值，行计数，求和等聚合信息。</li>\n<li>postscript：含有压缩参数和压缩大小相关信息。</li>\n</ul>\n<p>stripe：</p>\n<ul>\n<li>Index data：条带统计信息，数据在条带中位置索引信息。</li>\n<li>rows data：数据存储，多个<strong>行组</strong>组成，数据以流(stream)形式存储。</li>\n<li>stripe footer：保存数据所在文件目录。</li>\n</ul>\n<p>rows data存储两部分数据，metadata stream和data stream：</p>\n<ul>\n<li>metadata stream：描述每个行组的元数据信息；</li>\n<li>data stream：存储数据；</li>\n</ul>\n<p>ORC三级索引：</p>\n<ul>\n<li>文件级：记录文件中所有stripe的位置信息，以及文件中所存储的每列数据的统计信息；</li>\n<li>条带级：记录每个stripe所存储数据的统计信息；</li>\n<li>行组级：记录每个行组所存储数据的统计信息，在stripe中，10000行构成一个行组；</li>\n</ul>\n<p>三级索引查找过程：</p>\n<p>  查询ORC文件类型的表时，会先对比文件级索引列统计信息，找到满足条件的文件；接着，根据条带级索引信息，找到满足条件的条带快，之后在根据行组级索引，找到满足条件的行组；避免遍历全部，减少磁盘和网络I/O。</p>\n<p>数据类型（所有类型都接受NULL值）：</p>\n<ul>\n<li>整型：包含boolean(1 bit)、tiny(8 bit)、smallint(16 bit)、int(32 bit)、bigint(64 bit)</li>\n<li>浮点型：包含float和double</li>\n<li>字符串类型：包含string、char和varchar</li>\n<li>二进制类型：包含binary</li>\n<li>日期和时间类型：包含timestamp和date</li>\n<li>复杂类型：包含struct、list、map和union类型</li>\n</ul>\n<p>ACID事务支持：</p>\n<p>  Hive 0.14版本之前，只能新增或者删除整块分区或表，而不能对表的单个记录进行修改；</p>\n<p>  Hive 0.14版本之后，ORC文件能够保证原子性、一致性、隔离性和持久性的ACID事务能够被支持，支持数据更新；</p>\n<p>  Hive事务适用于大批量数据更新，不建议频繁小批量更新；</p>\n<p>压缩：</p>\n<p>  可选的类型有 NONE、ZLIB 和 SNAPPY，默认值是 ZLIB</p>\n<h4 id=\"Parquet\"><a href=\"#Parquet\" class=\"headerlink\" title=\"Parquet\"></a>Parquet</h4><p>结构：</p>\n<p><img src=\"/2022/11/23/hive-data/parquet.png\" alt=\"parquet结构\"></p>\n<p>  Parquet存储数据时，同时存储多级元数据，文件级元数据、列块级元数据、页级元数据；</p>\n<ul>\n<li><p>文件级元数据：表结构信息；文件记录数；行组数、每个行组数据总量和记录数；每个行组列块的文件偏移量；</p>\n</li>\n<li><p>列块级元数据：压缩前后数据大小和压缩编码；数据页、索引页的偏移量；列块的数据记录数；</p>\n</li>\n<li><p>页级元数据：编码信息和数据记录数；</p>\n</li>\n</ul>\n<p>Parquet针对嵌套式结构支持好，ORC多层嵌套表达起来比较复杂，性能损失大；</p>\n<p>参考：</p>\n<p><a href=\"https://blog.51cto.com/u_14932245/4608206\">https://blog.51cto.com/u_14932245/4608206</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Hive存储数据格式对比</p>\n</blockquote>","more":"<h2 id=\"Hive存储\"><a href=\"#Hive存储\" class=\"headerlink\" title=\"Hive存储\"></a>Hive存储</h2><h3 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h3><ul>\n<li><p>行式存储</p>\n<ul>\n<li>文本格式（TextFile）</li>\n<li>二进制序列化文件（SequenceFile）</li>\n</ul>\n</li>\n<li><p>列式存储</p>\n<ul>\n<li>行列式文件（RCFile）</li>\n<li>优化的行列式文件（ORCFile）</li>\n<li>Parquet</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"列式存储优势\"><a href=\"#列式存储优势\" class=\"headerlink\" title=\"列式存储优势\"></a>列式存储优势</h3><ul>\n<li>查询时只读指定列，查询速度快</li>\n<li>压缩时相同数据类型，压缩比高</li>\n</ul>\n<h3 id=\"典型分析\"><a href=\"#典型分析\" class=\"headerlink\" title=\"典型分析\"></a>典型分析</h3><h4 id=\"TextFile\"><a href=\"#TextFile\" class=\"headerlink\" title=\"TextFile\"></a>TextFile</h4><p>TextFile直接行式存储，可以直接load方式加载数据，加载速度快；</p>\n<p>TextFile反序列化必须逐个字符判断是不是分隔符或行结束符号，开销大；</p>\n<h4 id=\"ORCFile\"><a href=\"#ORCFile\" class=\"headerlink\" title=\"ORCFile\"></a>ORCFile</h4><ul>\n<li><p>1.ORC扩展了RCFile的压缩，除了<strong>游程编码</strong>(run-length)，引入<strong>字典编码</strong>和<strong>BIT编码</strong>；</p>\n</li>\n<li><p>2.每个TASK只输出单个文件，减少NameNode负载；</p>\n</li>\n<li><p>3.支持复杂数据类型，datetime、decimal、struct、list、map等；</p>\n</li>\n<li><p>4.文件可切分，查询时输入数据量少；</p>\n</li>\n</ul>\n<blockquote>\n<p>游程编码：变动长度编码法，AAAABBBCCDEEEE-&gt;4A3B2C1D4E;</p>\n<p>字典编码：最后存储字典值，每个字典值长度以及字段在字典位置；</p>\n<p>BIT编码：null则存0否则1，null实际编码不需要存储，不占用存储空间；</p>\n</blockquote>\n<p>结构： </p>\n<p>  每个ORC文件横向切分多个stripe，每个stripe列式存储，每个stripe默认250MB；</p>\n<p><img src=\"/2022/11/23/hive-data/orc.png\" alt=\"orc结构\"></p>\n<ul>\n<li>条带(stripe)：ORC文件存储数据；</li>\n<li>文件注脚(file footer)：包含了文件中stripe的列表，每个stripe的行数，以及每个列的数据类型；每个列的最小值，最大值，行计数，求和等聚合信息。</li>\n<li>postscript：含有压缩参数和压缩大小相关信息。</li>\n</ul>\n<p>stripe：</p>\n<ul>\n<li>Index data：条带统计信息，数据在条带中位置索引信息。</li>\n<li>rows data：数据存储，多个<strong>行组</strong>组成，数据以流(stream)形式存储。</li>\n<li>stripe footer：保存数据所在文件目录。</li>\n</ul>\n<p>rows data存储两部分数据，metadata stream和data stream：</p>\n<ul>\n<li>metadata stream：描述每个行组的元数据信息；</li>\n<li>data stream：存储数据；</li>\n</ul>\n<p>ORC三级索引：</p>\n<ul>\n<li>文件级：记录文件中所有stripe的位置信息，以及文件中所存储的每列数据的统计信息；</li>\n<li>条带级：记录每个stripe所存储数据的统计信息；</li>\n<li>行组级：记录每个行组所存储数据的统计信息，在stripe中，10000行构成一个行组；</li>\n</ul>\n<p>三级索引查找过程：</p>\n<p>  查询ORC文件类型的表时，会先对比文件级索引列统计信息，找到满足条件的文件；接着，根据条带级索引信息，找到满足条件的条带快，之后在根据行组级索引，找到满足条件的行组；避免遍历全部，减少磁盘和网络I/O。</p>\n<p>数据类型（所有类型都接受NULL值）：</p>\n<ul>\n<li>整型：包含boolean(1 bit)、tiny(8 bit)、smallint(16 bit)、int(32 bit)、bigint(64 bit)</li>\n<li>浮点型：包含float和double</li>\n<li>字符串类型：包含string、char和varchar</li>\n<li>二进制类型：包含binary</li>\n<li>日期和时间类型：包含timestamp和date</li>\n<li>复杂类型：包含struct、list、map和union类型</li>\n</ul>\n<p>ACID事务支持：</p>\n<p>  Hive 0.14版本之前，只能新增或者删除整块分区或表，而不能对表的单个记录进行修改；</p>\n<p>  Hive 0.14版本之后，ORC文件能够保证原子性、一致性、隔离性和持久性的ACID事务能够被支持，支持数据更新；</p>\n<p>  Hive事务适用于大批量数据更新，不建议频繁小批量更新；</p>\n<p>压缩：</p>\n<p>  可选的类型有 NONE、ZLIB 和 SNAPPY，默认值是 ZLIB</p>\n<h4 id=\"Parquet\"><a href=\"#Parquet\" class=\"headerlink\" title=\"Parquet\"></a>Parquet</h4><p>结构：</p>\n<p><img src=\"/2022/11/23/hive-data/parquet.png\" alt=\"parquet结构\"></p>\n<p>  Parquet存储数据时，同时存储多级元数据，文件级元数据、列块级元数据、页级元数据；</p>\n<ul>\n<li><p>文件级元数据：表结构信息；文件记录数；行组数、每个行组数据总量和记录数；每个行组列块的文件偏移量；</p>\n</li>\n<li><p>列块级元数据：压缩前后数据大小和压缩编码；数据页、索引页的偏移量；列块的数据记录数；</p>\n</li>\n<li><p>页级元数据：编码信息和数据记录数；</p>\n</li>\n</ul>\n<p>Parquet针对嵌套式结构支持好，ORC多层嵌套表达起来比较复杂，性能损失大；</p>\n<p>参考：</p>\n<p><a href=\"https://blog.51cto.com/u_14932245/4608206\">https://blog.51cto.com/u_14932245/4608206</a></p>"},{"title":"hive-sql","date":"2022-12-05T07:01:21.000Z","_content":"\n> Hive sql 汇总\n\n<!-- more -->\n\n#### 连续7天登陆\n\n```sql\nselect user_id, count(1) as cn \nfrom (\n    select user_id, login_date, row_number() over (partition by user_id order by login_date) as rn \n    from (\n        select user_id, login_date \n        from db.t \n        group by user_id, login_date\n    )\n) group by user_id, date_sub(login_date, rn)\nwhere cn>=7\n```\n\n#### 时间处理\n\n```sql\ndate_sub(from_unixtime(unix_timestamp(cast(imp_date as string), 'yyyyMMdd')),1)\n```\n\n#### 相邻间隔\n\n```sql\nselect t1.id, \nt1.ctime, \nt2.id, \nt2.ctime,\ntimediff(t2.ctime,t1.ctime) as diff -- abs(datediff())\nfrom t t1 inner join t t2 \non t1.id + 1 = t2.id\n```\n\n","source":"_posts/hive-sql.md","raw":"---\ntitle: hive-sql\ndate: 2022-12-05 15:01:21\ntags: sql\n---\n\n> Hive sql 汇总\n\n<!-- more -->\n\n#### 连续7天登陆\n\n```sql\nselect user_id, count(1) as cn \nfrom (\n    select user_id, login_date, row_number() over (partition by user_id order by login_date) as rn \n    from (\n        select user_id, login_date \n        from db.t \n        group by user_id, login_date\n    )\n) group by user_id, date_sub(login_date, rn)\nwhere cn>=7\n```\n\n#### 时间处理\n\n```sql\ndate_sub(from_unixtime(unix_timestamp(cast(imp_date as string), 'yyyyMMdd')),1)\n```\n\n#### 相邻间隔\n\n```sql\nselect t1.id, \nt1.ctime, \nt2.id, \nt2.ctime,\ntimediff(t2.ctime,t1.ctime) as diff -- abs(datediff())\nfrom t t1 inner join t t2 \non t1.id + 1 = t2.id\n```\n\n","slug":"hive-sql","published":1,"updated":"2022-12-19T01:30:52.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffha000j16gn6u8q4t3k","content":"<blockquote>\n<p>Hive sql 汇总</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h4 id=\"连续7天登陆\"><a href=\"#连续7天登陆\" class=\"headerlink\" title=\"连续7天登陆\"></a>连续7天登陆</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> user_id, <span class=\"built_in\">count</span>(<span class=\"number\">1</span>) <span class=\"keyword\">as</span> cn </span><br><span class=\"line\"><span class=\"keyword\">from</span> (</span><br><span class=\"line\">    <span class=\"keyword\">select</span> user_id, login_date, <span class=\"built_in\">row_number</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_id <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> login_date) <span class=\"keyword\">as</span> rn </span><br><span class=\"line\">    <span class=\"keyword\">from</span> (</span><br><span class=\"line\">        <span class=\"keyword\">select</span> user_id, login_date </span><br><span class=\"line\">        <span class=\"keyword\">from</span> db.t </span><br><span class=\"line\">        <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> user_id, login_date</span><br><span class=\"line\">    )</span><br><span class=\"line\">) <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> user_id, date_sub(login_date, rn)</span><br><span class=\"line\"><span class=\"keyword\">where</span> cn<span class=\"operator\">&gt;=</span><span class=\"number\">7</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"时间处理\"><a href=\"#时间处理\" class=\"headerlink\" title=\"时间处理\"></a>时间处理</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">date_sub(from_unixtime(unix_timestamp(<span class=\"built_in\">cast</span>(imp_date <span class=\"keyword\">as</span> string), <span class=\"string\">&#x27;yyyyMMdd&#x27;</span>)),<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"相邻间隔\"><a href=\"#相邻间隔\" class=\"headerlink\" title=\"相邻间隔\"></a>相邻间隔</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> t1.id, </span><br><span class=\"line\">t1.ctime, </span><br><span class=\"line\">t2.id, </span><br><span class=\"line\">t2.ctime,</span><br><span class=\"line\">timediff(t2.ctime,t1.ctime) <span class=\"keyword\">as</span> diff <span class=\"comment\">-- abs(datediff())</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> t t1 <span class=\"keyword\">inner</span> <span class=\"keyword\">join</span> t t2 </span><br><span class=\"line\"><span class=\"keyword\">on</span> t1.id <span class=\"operator\">+</span> <span class=\"number\">1</span> <span class=\"operator\">=</span> t2.id</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Hive sql 汇总</p>\n</blockquote>","more":"<h4 id=\"连续7天登陆\"><a href=\"#连续7天登陆\" class=\"headerlink\" title=\"连续7天登陆\"></a>连续7天登陆</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> user_id, <span class=\"built_in\">count</span>(<span class=\"number\">1</span>) <span class=\"keyword\">as</span> cn </span><br><span class=\"line\"><span class=\"keyword\">from</span> (</span><br><span class=\"line\">    <span class=\"keyword\">select</span> user_id, login_date, <span class=\"built_in\">row_number</span>() <span class=\"keyword\">over</span> (<span class=\"keyword\">partition</span> <span class=\"keyword\">by</span> user_id <span class=\"keyword\">order</span> <span class=\"keyword\">by</span> login_date) <span class=\"keyword\">as</span> rn </span><br><span class=\"line\">    <span class=\"keyword\">from</span> (</span><br><span class=\"line\">        <span class=\"keyword\">select</span> user_id, login_date </span><br><span class=\"line\">        <span class=\"keyword\">from</span> db.t </span><br><span class=\"line\">        <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> user_id, login_date</span><br><span class=\"line\">    )</span><br><span class=\"line\">) <span class=\"keyword\">group</span> <span class=\"keyword\">by</span> user_id, date_sub(login_date, rn)</span><br><span class=\"line\"><span class=\"keyword\">where</span> cn<span class=\"operator\">&gt;=</span><span class=\"number\">7</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"时间处理\"><a href=\"#时间处理\" class=\"headerlink\" title=\"时间处理\"></a>时间处理</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">date_sub(from_unixtime(unix_timestamp(<span class=\"built_in\">cast</span>(imp_date <span class=\"keyword\">as</span> string), <span class=\"string\">&#x27;yyyyMMdd&#x27;</span>)),<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"相邻间隔\"><a href=\"#相邻间隔\" class=\"headerlink\" title=\"相邻间隔\"></a>相邻间隔</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">select</span> t1.id, </span><br><span class=\"line\">t1.ctime, </span><br><span class=\"line\">t2.id, </span><br><span class=\"line\">t2.ctime,</span><br><span class=\"line\">timediff(t2.ctime,t1.ctime) <span class=\"keyword\">as</span> diff <span class=\"comment\">-- abs(datediff())</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> t t1 <span class=\"keyword\">inner</span> <span class=\"keyword\">join</span> t t2 </span><br><span class=\"line\"><span class=\"keyword\">on</span> t1.id <span class=\"operator\">+</span> <span class=\"number\">1</span> <span class=\"operator\">=</span> t2.id</span><br></pre></td></tr></table></figure>"},{"title":"sort-py","date":"2022-12-05T06:06:29.000Z","_content":"\n> 排序算法汇总(基于python)\n\n<!-- more -->\n\n### 实现\n\n```python\n### 1 冒泡排序\n### 双层循环，外层循环已冒泡元素个数，内层循环未冒泡元素与左侧对比\ndef bubbleSort(arr):\n    for i in range(len(arr)): \n        for j in range(1,len(arr)-i): \n            if arr[j-1]>arr[j]: \n                arr[j-1],arr[j] = arr[j],arr[j-1]\n    return arr\n### 2 选择排序\n### 双层循环，外层循环元素遍历，找到当前元素及之后元素最小值，交换当前元素和最小值元素\ndef selectSort(arr):\n    for i in range(len(arr)):\n        min = i\n        for j in range(i+1, len(arr)):\n            if arr[j]<arr[min]:\n                min = j\n        arr[i], arr[min] = arr[min], arr[i]\n    return arr\n### 3 插入排序\n### 双层循环，外层未排序好元素开始(首位元素认为已排序)，如果小于左边记录索引和值，从后向前遍历已排序元素找到合适位置插入\ndef insertSort(arr):\n    for i in range(1,len(arr)):\n        if arr[i]<arr[i-1]:\n            index = i # 记录索引\n            value = arr[i] # 记录值\n            # 找位置(逐个后移)\n            for j in range(i-1, -1, -1):\n                if value<arr[j]:\n                    arr[j+1]=arr[j]\n                    index = j # 记录当前索引\n                else:\n                    break\n            arr[index] = value # 插入\n    return(arr)\n### 4 希尔排序\n### 逐渐减小步长，以步长长度插入排序，直至步长为1\ndef shellSort(arr):\n    gap = len(arr)//2\n    while gap>0:\n        for i in range(gap, len(arr)):\n            index = i\n            value = arr[i]\n            while index>=gap and arr[index-gap]>value:\n                arr[index] = arr[index-gap]\n                index = index-gap\n            arr[index]=value\n        gap = gap//2\n    return arr\n#### 5 归并排序\n#### 逐步二分到长度为1 然后合并合并再合并\ndef mergeSort(arr):\n    if len(arr)<=1:\n        return arr\n    m = len(arr)//2\n    left = mergeSort(arr[:m])\n    right = mergeSort(arr[m:])\n    return merge(left,right)\ndef merge(left,right):\n    l,r = 0,0 # 双指针\n    result = []\n    while l<len(left) and r<len(right):\n        if left[l]<right[r]:\n            result.append(left[l])\n            l+=1\n        else:\n            result.append(right[r])\n            r+=1\n    result+=left[l:]\n    result+=right[r:]\n    return result\n#### 6 快速排序\n#### 三次遍历可以简化为一次 优化空间\ndef quickSort(arr):\n    if len(arr)<=1:\n        return arr\n    pivot = arr[0]\n    left = [x for x in arr if x < pivot]\n    right = [x for x in arr if x > pivot]\n    middle = [x for x in arr if x == pivot]\n    return quickSort(left) + middle + quickSort(right)\n#### 7 堆排序\n#### 先从非叶子结点开始递归堆化然后从后向前交换首尾去掉尾部从头开始重新堆化\ndef heapSort(arr):\n    for i in range((len(arr)-2)//2, -1, -1): # 减1是为了长度转索引 再减1除2是为了找到非叶子节点(没有子节点都是)\n        heapify(arr, i) # 堆化\n    result = []\n    for i in range(len(arr)-1, -1, -1):\n        arr[i], arr[0] = arr[0], arr[i] # 交换首尾\n        result.append(arr.pop(-1))  # 去除末尾\n        heapify(arr, 0) # 重新堆化\n    return result\ndef heapify(arr, i):\n    smallest = i\n    if 2*i+1<len(arr) and arr[2*i+1] < arr[smallest]:\n        smallest = 2*i+1\n    if 2*i+2<len(arr) and arr[2*i+2] < arr[smallest]:\n        smallest = 2*i+2\n    if not smallest==i: # 跟和子互换\n        arr[i], arr[smallest] = arr[smallest], arr[i]\n        heapify(arr, smallest) # 子作为跟递归堆化\n#### 8 计数排序\n#### 有点浪费空间\ndef countSort(arr):\n    count = [0]*(max(arr)-min(arr)+1)\n    for a in arr:\n        count[a-min(arr)]+=1\n    result = []\n    for i in range(len(count)):\n        result += [i+min(arr)] * count[i]\n    return result\narr = [1,5,3,2,0]\n# bubbleSort(arr)\n# selectSort(arr)\n# insertSort(arr)\n# shellSort(arr)\n# mergeSort(arr)\n# quickSort(arr)\n# heapSort(arr)\ncountSort(arr)\n```\n\n### 参考\n\nhttps://zhuanlan.zhihu.com/p/21839027\n","source":"_posts/sort-py.md","raw":"---\ntitle: sort-py\ndate: 2022-12-05 14:06:29\ntags: 算法\n---\n\n> 排序算法汇总(基于python)\n\n<!-- more -->\n\n### 实现\n\n```python\n### 1 冒泡排序\n### 双层循环，外层循环已冒泡元素个数，内层循环未冒泡元素与左侧对比\ndef bubbleSort(arr):\n    for i in range(len(arr)): \n        for j in range(1,len(arr)-i): \n            if arr[j-1]>arr[j]: \n                arr[j-1],arr[j] = arr[j],arr[j-1]\n    return arr\n### 2 选择排序\n### 双层循环，外层循环元素遍历，找到当前元素及之后元素最小值，交换当前元素和最小值元素\ndef selectSort(arr):\n    for i in range(len(arr)):\n        min = i\n        for j in range(i+1, len(arr)):\n            if arr[j]<arr[min]:\n                min = j\n        arr[i], arr[min] = arr[min], arr[i]\n    return arr\n### 3 插入排序\n### 双层循环，外层未排序好元素开始(首位元素认为已排序)，如果小于左边记录索引和值，从后向前遍历已排序元素找到合适位置插入\ndef insertSort(arr):\n    for i in range(1,len(arr)):\n        if arr[i]<arr[i-1]:\n            index = i # 记录索引\n            value = arr[i] # 记录值\n            # 找位置(逐个后移)\n            for j in range(i-1, -1, -1):\n                if value<arr[j]:\n                    arr[j+1]=arr[j]\n                    index = j # 记录当前索引\n                else:\n                    break\n            arr[index] = value # 插入\n    return(arr)\n### 4 希尔排序\n### 逐渐减小步长，以步长长度插入排序，直至步长为1\ndef shellSort(arr):\n    gap = len(arr)//2\n    while gap>0:\n        for i in range(gap, len(arr)):\n            index = i\n            value = arr[i]\n            while index>=gap and arr[index-gap]>value:\n                arr[index] = arr[index-gap]\n                index = index-gap\n            arr[index]=value\n        gap = gap//2\n    return arr\n#### 5 归并排序\n#### 逐步二分到长度为1 然后合并合并再合并\ndef mergeSort(arr):\n    if len(arr)<=1:\n        return arr\n    m = len(arr)//2\n    left = mergeSort(arr[:m])\n    right = mergeSort(arr[m:])\n    return merge(left,right)\ndef merge(left,right):\n    l,r = 0,0 # 双指针\n    result = []\n    while l<len(left) and r<len(right):\n        if left[l]<right[r]:\n            result.append(left[l])\n            l+=1\n        else:\n            result.append(right[r])\n            r+=1\n    result+=left[l:]\n    result+=right[r:]\n    return result\n#### 6 快速排序\n#### 三次遍历可以简化为一次 优化空间\ndef quickSort(arr):\n    if len(arr)<=1:\n        return arr\n    pivot = arr[0]\n    left = [x for x in arr if x < pivot]\n    right = [x for x in arr if x > pivot]\n    middle = [x for x in arr if x == pivot]\n    return quickSort(left) + middle + quickSort(right)\n#### 7 堆排序\n#### 先从非叶子结点开始递归堆化然后从后向前交换首尾去掉尾部从头开始重新堆化\ndef heapSort(arr):\n    for i in range((len(arr)-2)//2, -1, -1): # 减1是为了长度转索引 再减1除2是为了找到非叶子节点(没有子节点都是)\n        heapify(arr, i) # 堆化\n    result = []\n    for i in range(len(arr)-1, -1, -1):\n        arr[i], arr[0] = arr[0], arr[i] # 交换首尾\n        result.append(arr.pop(-1))  # 去除末尾\n        heapify(arr, 0) # 重新堆化\n    return result\ndef heapify(arr, i):\n    smallest = i\n    if 2*i+1<len(arr) and arr[2*i+1] < arr[smallest]:\n        smallest = 2*i+1\n    if 2*i+2<len(arr) and arr[2*i+2] < arr[smallest]:\n        smallest = 2*i+2\n    if not smallest==i: # 跟和子互换\n        arr[i], arr[smallest] = arr[smallest], arr[i]\n        heapify(arr, smallest) # 子作为跟递归堆化\n#### 8 计数排序\n#### 有点浪费空间\ndef countSort(arr):\n    count = [0]*(max(arr)-min(arr)+1)\n    for a in arr:\n        count[a-min(arr)]+=1\n    result = []\n    for i in range(len(count)):\n        result += [i+min(arr)] * count[i]\n    return result\narr = [1,5,3,2,0]\n# bubbleSort(arr)\n# selectSort(arr)\n# insertSort(arr)\n# shellSort(arr)\n# mergeSort(arr)\n# quickSort(arr)\n# heapSort(arr)\ncountSort(arr)\n```\n\n### 参考\n\nhttps://zhuanlan.zhihu.com/p/21839027\n","slug":"sort-py","published":1,"updated":"2022-12-19T01:30:52.684Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffha000l16gn86at1ifp","content":"<blockquote>\n<p>排序算法汇总(基于python)</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### 1 冒泡排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层循环已冒泡元素个数，内层循环未冒泡元素与左侧对比</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bubbleSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)): </span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"built_in\">len</span>(arr)-i): </span><br><span class=\"line\">            <span class=\"keyword\">if</span> arr[j-<span class=\"number\">1</span>]&gt;arr[j]: </span><br><span class=\"line\">                arr[j-<span class=\"number\">1</span>],arr[j] = arr[j],arr[j-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">### 2 选择排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层循环元素遍历，找到当前元素及之后元素最小值，交换当前元素和最小值元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">selectSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">        <span class=\"built_in\">min</span> = i</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i+<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> arr[j]&lt;arr[<span class=\"built_in\">min</span>]:</span><br><span class=\"line\">                <span class=\"built_in\">min</span> = j</span><br><span class=\"line\">        arr[i], arr[<span class=\"built_in\">min</span>] = arr[<span class=\"built_in\">min</span>], arr[i]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">### 3 插入排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层未排序好元素开始(首位元素认为已排序)，如果小于左边记录索引和值，从后向前遍历已排序元素找到合适位置插入</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[i]&lt;arr[i-<span class=\"number\">1</span>]:</span><br><span class=\"line\">            index = i <span class=\"comment\"># 记录索引</span></span><br><span class=\"line\">            value = arr[i] <span class=\"comment\"># 记录值</span></span><br><span class=\"line\">            <span class=\"comment\"># 找位置(逐个后移)</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i-<span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> value&lt;arr[j]:</span><br><span class=\"line\">                    arr[j+<span class=\"number\">1</span>]=arr[j]</span><br><span class=\"line\">                    index = j <span class=\"comment\"># 记录当前索引</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            arr[index] = value <span class=\"comment\"># 插入</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span>(arr)</span><br><span class=\"line\"><span class=\"comment\">### 4 希尔排序</span></span><br><span class=\"line\"><span class=\"comment\">### 逐渐减小步长，以步长长度插入排序，直至步长为1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">shellSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    gap = <span class=\"built_in\">len</span>(arr)//<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> gap&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gap, <span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">            index = i</span><br><span class=\"line\">            value = arr[i]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> index&gt;=gap <span class=\"keyword\">and</span> arr[index-gap]&gt;value:</span><br><span class=\"line\">                arr[index] = arr[index-gap]</span><br><span class=\"line\">                index = index-gap</span><br><span class=\"line\">            arr[index]=value</span><br><span class=\"line\">        gap = gap//<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">#### 5 归并排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 逐步二分到长度为1 然后合并合并再合并</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(arr)&lt;=<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr</span><br><span class=\"line\">    m = <span class=\"built_in\">len</span>(arr)//<span class=\"number\">2</span></span><br><span class=\"line\">    left = mergeSort(arr[:m])</span><br><span class=\"line\">    right = mergeSort(arr[m:])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(left,right)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">left,right</span>):</span></span><br><span class=\"line\">    l,r = <span class=\"number\">0</span>,<span class=\"number\">0</span> <span class=\"comment\"># 双指针</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> l&lt;<span class=\"built_in\">len</span>(left) <span class=\"keyword\">and</span> r&lt;<span class=\"built_in\">len</span>(right):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> left[l]&lt;right[r]:</span><br><span class=\"line\">            result.append(left[l])</span><br><span class=\"line\">            l+=<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            result.append(right[r])</span><br><span class=\"line\">            r+=<span class=\"number\">1</span></span><br><span class=\"line\">    result+=left[l:]</span><br><span class=\"line\">    result+=right[r:]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"><span class=\"comment\">#### 6 快速排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 三次遍历可以简化为一次 优化空间</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">quickSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(arr)&lt;=<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr</span><br><span class=\"line\">    pivot = arr[<span class=\"number\">0</span>]</span><br><span class=\"line\">    left = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x &lt; pivot]</span><br><span class=\"line\">    right = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x &gt; pivot]</span><br><span class=\"line\">    middle = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x == pivot]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> quickSort(left) + middle + quickSort(right)</span><br><span class=\"line\"><span class=\"comment\">#### 7 堆排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 先从非叶子结点开始递归堆化然后从后向前交换首尾去掉尾部从头开始重新堆化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heapSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>((<span class=\"built_in\">len</span>(arr)-<span class=\"number\">2</span>)//<span class=\"number\">2</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 减1是为了长度转索引 再减1除2是为了找到非叶子节点(没有子节点都是)</span></span><br><span class=\"line\">        heapify(arr, i) <span class=\"comment\"># 堆化</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)-<span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        arr[i], arr[<span class=\"number\">0</span>] = arr[<span class=\"number\">0</span>], arr[i] <span class=\"comment\"># 交换首尾</span></span><br><span class=\"line\">        result.append(arr.pop(-<span class=\"number\">1</span>))  <span class=\"comment\"># 去除末尾</span></span><br><span class=\"line\">        heapify(arr, <span class=\"number\">0</span>) <span class=\"comment\"># 重新堆化</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heapify</span>(<span class=\"params\">arr, i</span>):</span></span><br><span class=\"line\">    smallest = i</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"number\">2</span>*i+<span class=\"number\">1</span>&lt;<span class=\"built_in\">len</span>(arr) <span class=\"keyword\">and</span> arr[<span class=\"number\">2</span>*i+<span class=\"number\">1</span>] &lt; arr[smallest]:</span><br><span class=\"line\">        smallest = <span class=\"number\">2</span>*i+<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"number\">2</span>*i+<span class=\"number\">2</span>&lt;<span class=\"built_in\">len</span>(arr) <span class=\"keyword\">and</span> arr[<span class=\"number\">2</span>*i+<span class=\"number\">2</span>] &lt; arr[smallest]:</span><br><span class=\"line\">        smallest = <span class=\"number\">2</span>*i+<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> smallest==i: <span class=\"comment\"># 跟和子互换</span></span><br><span class=\"line\">        arr[i], arr[smallest] = arr[smallest], arr[i]</span><br><span class=\"line\">        heapify(arr, smallest) <span class=\"comment\"># 子作为跟递归堆化</span></span><br><span class=\"line\"><span class=\"comment\">#### 8 计数排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 有点浪费空间</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">countSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    count = [<span class=\"number\">0</span>]*(<span class=\"built_in\">max</span>(arr)-<span class=\"built_in\">min</span>(arr)+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> arr:</span><br><span class=\"line\">        count[a-<span class=\"built_in\">min</span>(arr)]+=<span class=\"number\">1</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(count)):</span><br><span class=\"line\">        result += [i+<span class=\"built_in\">min</span>(arr)] * count[i]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">arr = [<span class=\"number\">1</span>,<span class=\"number\">5</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># bubbleSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># selectSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># insertSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># shellSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># mergeSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># quickSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># heapSort(arr)</span></span><br><span class=\"line\">countSort(arr)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://zhuanlan.zhihu.com/p/21839027\">https://zhuanlan.zhihu.com/p/21839027</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>排序算法汇总(基于python)</p>\n</blockquote>","more":"<h3 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">### 1 冒泡排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层循环已冒泡元素个数，内层循环未冒泡元素与左侧对比</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bubbleSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)): </span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"built_in\">len</span>(arr)-i): </span><br><span class=\"line\">            <span class=\"keyword\">if</span> arr[j-<span class=\"number\">1</span>]&gt;arr[j]: </span><br><span class=\"line\">                arr[j-<span class=\"number\">1</span>],arr[j] = arr[j],arr[j-<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">### 2 选择排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层循环元素遍历，找到当前元素及之后元素最小值，交换当前元素和最小值元素</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">selectSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">        <span class=\"built_in\">min</span> = i</span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i+<span class=\"number\">1</span>, <span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> arr[j]&lt;arr[<span class=\"built_in\">min</span>]:</span><br><span class=\"line\">                <span class=\"built_in\">min</span> = j</span><br><span class=\"line\">        arr[i], arr[<span class=\"built_in\">min</span>] = arr[<span class=\"built_in\">min</span>], arr[i]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">### 3 插入排序</span></span><br><span class=\"line\"><span class=\"comment\">### 双层循环，外层未排序好元素开始(首位元素认为已排序)，如果小于左边记录索引和值，从后向前遍历已排序元素找到合适位置插入</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">insertSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> arr[i]&lt;arr[i-<span class=\"number\">1</span>]:</span><br><span class=\"line\">            index = i <span class=\"comment\"># 记录索引</span></span><br><span class=\"line\">            value = arr[i] <span class=\"comment\"># 记录值</span></span><br><span class=\"line\">            <span class=\"comment\"># 找位置(逐个后移)</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(i-<span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> value&lt;arr[j]:</span><br><span class=\"line\">                    arr[j+<span class=\"number\">1</span>]=arr[j]</span><br><span class=\"line\">                    index = j <span class=\"comment\"># 记录当前索引</span></span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    <span class=\"keyword\">break</span></span><br><span class=\"line\">            arr[index] = value <span class=\"comment\"># 插入</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span>(arr)</span><br><span class=\"line\"><span class=\"comment\">### 4 希尔排序</span></span><br><span class=\"line\"><span class=\"comment\">### 逐渐减小步长，以步长长度插入排序，直至步长为1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">shellSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    gap = <span class=\"built_in\">len</span>(arr)//<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> gap&gt;<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gap, <span class=\"built_in\">len</span>(arr)):</span><br><span class=\"line\">            index = i</span><br><span class=\"line\">            value = arr[i]</span><br><span class=\"line\">            <span class=\"keyword\">while</span> index&gt;=gap <span class=\"keyword\">and</span> arr[index-gap]&gt;value:</span><br><span class=\"line\">                arr[index] = arr[index-gap]</span><br><span class=\"line\">                index = index-gap</span><br><span class=\"line\">            arr[index]=value</span><br><span class=\"line\">        gap = gap//<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> arr</span><br><span class=\"line\"><span class=\"comment\">#### 5 归并排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 逐步二分到长度为1 然后合并合并再合并</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mergeSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(arr)&lt;=<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr</span><br><span class=\"line\">    m = <span class=\"built_in\">len</span>(arr)//<span class=\"number\">2</span></span><br><span class=\"line\">    left = mergeSort(arr[:m])</span><br><span class=\"line\">    right = mergeSort(arr[m:])</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(left,right)</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">left,right</span>):</span></span><br><span class=\"line\">    l,r = <span class=\"number\">0</span>,<span class=\"number\">0</span> <span class=\"comment\"># 双指针</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">while</span> l&lt;<span class=\"built_in\">len</span>(left) <span class=\"keyword\">and</span> r&lt;<span class=\"built_in\">len</span>(right):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> left[l]&lt;right[r]:</span><br><span class=\"line\">            result.append(left[l])</span><br><span class=\"line\">            l+=<span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            result.append(right[r])</span><br><span class=\"line\">            r+=<span class=\"number\">1</span></span><br><span class=\"line\">    result+=left[l:]</span><br><span class=\"line\">    result+=right[r:]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"><span class=\"comment\">#### 6 快速排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 三次遍历可以简化为一次 优化空间</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">quickSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(arr)&lt;=<span class=\"number\">1</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> arr</span><br><span class=\"line\">    pivot = arr[<span class=\"number\">0</span>]</span><br><span class=\"line\">    left = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x &lt; pivot]</span><br><span class=\"line\">    right = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x &gt; pivot]</span><br><span class=\"line\">    middle = [x <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> arr <span class=\"keyword\">if</span> x == pivot]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> quickSort(left) + middle + quickSort(right)</span><br><span class=\"line\"><span class=\"comment\">#### 7 堆排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 先从非叶子结点开始递归堆化然后从后向前交换首尾去掉尾部从头开始重新堆化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heapSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>((<span class=\"built_in\">len</span>(arr)-<span class=\"number\">2</span>)//<span class=\"number\">2</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>): <span class=\"comment\"># 减1是为了长度转索引 再减1除2是为了找到非叶子节点(没有子节点都是)</span></span><br><span class=\"line\">        heapify(arr, i) <span class=\"comment\"># 堆化</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(arr)-<span class=\"number\">1</span>, -<span class=\"number\">1</span>, -<span class=\"number\">1</span>):</span><br><span class=\"line\">        arr[i], arr[<span class=\"number\">0</span>] = arr[<span class=\"number\">0</span>], arr[i] <span class=\"comment\"># 交换首尾</span></span><br><span class=\"line\">        result.append(arr.pop(-<span class=\"number\">1</span>))  <span class=\"comment\"># 去除末尾</span></span><br><span class=\"line\">        heapify(arr, <span class=\"number\">0</span>) <span class=\"comment\"># 重新堆化</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">heapify</span>(<span class=\"params\">arr, i</span>):</span></span><br><span class=\"line\">    smallest = i</span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"number\">2</span>*i+<span class=\"number\">1</span>&lt;<span class=\"built_in\">len</span>(arr) <span class=\"keyword\">and</span> arr[<span class=\"number\">2</span>*i+<span class=\"number\">1</span>] &lt; arr[smallest]:</span><br><span class=\"line\">        smallest = <span class=\"number\">2</span>*i+<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"number\">2</span>*i+<span class=\"number\">2</span>&lt;<span class=\"built_in\">len</span>(arr) <span class=\"keyword\">and</span> arr[<span class=\"number\">2</span>*i+<span class=\"number\">2</span>] &lt; arr[smallest]:</span><br><span class=\"line\">        smallest = <span class=\"number\">2</span>*i+<span class=\"number\">2</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> smallest==i: <span class=\"comment\"># 跟和子互换</span></span><br><span class=\"line\">        arr[i], arr[smallest] = arr[smallest], arr[i]</span><br><span class=\"line\">        heapify(arr, smallest) <span class=\"comment\"># 子作为跟递归堆化</span></span><br><span class=\"line\"><span class=\"comment\">#### 8 计数排序</span></span><br><span class=\"line\"><span class=\"comment\">#### 有点浪费空间</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">countSort</span>(<span class=\"params\">arr</span>):</span></span><br><span class=\"line\">    count = [<span class=\"number\">0</span>]*(<span class=\"built_in\">max</span>(arr)-<span class=\"built_in\">min</span>(arr)+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> arr:</span><br><span class=\"line\">        count[a-<span class=\"built_in\">min</span>(arr)]+=<span class=\"number\">1</span></span><br><span class=\"line\">    result = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(count)):</span><br><span class=\"line\">        result += [i+<span class=\"built_in\">min</span>(arr)] * count[i]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">arr = [<span class=\"number\">1</span>,<span class=\"number\">5</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># bubbleSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># selectSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># insertSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># shellSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># mergeSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># quickSort(arr)</span></span><br><span class=\"line\"><span class=\"comment\"># heapSort(arr)</span></span><br><span class=\"line\">countSort(arr)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://zhuanlan.zhihu.com/p/21839027\">https://zhuanlan.zhihu.com/p/21839027</a></p>"},{"title":"spark-role","date":"2022-12-15T02:45:04.000Z","_content":"\n> Spark角色\n\n<!-- more -->\n\n| 角色名称        | 解释                                                         |\n| --------------- | ------------------------------------------------------------ |\n| Master          | 常驻master守护进程，负责管理worker节点，从master节点提交应用 |\n| Worker          | 常驻worker守护进程，与master节点通信，并且管理executor进程，运行一个或多个Executor进程，相当于计算节点 |\n| Client          | 用户提交作业的客户端                                         |\n| Driver          | 负责控制一个应用的执行，运行Application的main函数和初始化SparkContext，Driver将Task和Task所依赖的file和jar(序列化后)传递给对应的Worker机器执行 |\n| Executor        | executor进程宿主在worker节点上，一个worker可以有多个executor，每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的RDD提供内存式存储，RDD是直接缓存在executor进程内的，因此任务可以运行时充分利用缓存加速运算。一个container对应一个JVM进程(也就是一个executor) |\n| Job             | action的触发会生成一个job，job会提交给DAGScheduler，分解成Stage |\n| Stage           | DAGscheduler根据shuffle将job划分为不同的stage，同一个stage包含多个task，这些task有相同shuffle dependencies |\n| Task            | 被送到executor上的工作单元，task简单来说就是在一个数据partition上单个数据处理流程 |\n| Cluster Manager | 在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn)    |\n| SparkContext    | 整个应用的上下文，控制程序的生命周期                         |\n| DAGScheduler    | 根据Job构建基于Stage的DAG工作流，并提交Stage给TaskScheduler  |\n| TaskScheduler   | 将task发给Executor执行                                       |\n| SparkEnv        | 线程级别的上下文，存储运行时的重要组件的引用                 |\n\n![sparkonyarn](./spark-role/sparksubmit.png)\n","source":"_posts/spark-role.md","raw":"---\ntitle: spark-role\ndate: 2022-12-15 10:45:04\ntags: Spark\n---\n\n> Spark角色\n\n<!-- more -->\n\n| 角色名称        | 解释                                                         |\n| --------------- | ------------------------------------------------------------ |\n| Master          | 常驻master守护进程，负责管理worker节点，从master节点提交应用 |\n| Worker          | 常驻worker守护进程，与master节点通信，并且管理executor进程，运行一个或多个Executor进程，相当于计算节点 |\n| Client          | 用户提交作业的客户端                                         |\n| Driver          | 负责控制一个应用的执行，运行Application的main函数和初始化SparkContext，Driver将Task和Task所依赖的file和jar(序列化后)传递给对应的Worker机器执行 |\n| Executor        | executor进程宿主在worker节点上，一个worker可以有多个executor，每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的RDD提供内存式存储，RDD是直接缓存在executor进程内的，因此任务可以运行时充分利用缓存加速运算。一个container对应一个JVM进程(也就是一个executor) |\n| Job             | action的触发会生成一个job，job会提交给DAGScheduler，分解成Stage |\n| Stage           | DAGscheduler根据shuffle将job划分为不同的stage，同一个stage包含多个task，这些task有相同shuffle dependencies |\n| Task            | 被送到executor上的工作单元，task简单来说就是在一个数据partition上单个数据处理流程 |\n| Cluster Manager | 在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn)    |\n| SparkContext    | 整个应用的上下文，控制程序的生命周期                         |\n| DAGScheduler    | 根据Job构建基于Stage的DAG工作流，并提交Stage给TaskScheduler  |\n| TaskScheduler   | 将task发给Executor执行                                       |\n| SparkEnv        | 线程级别的上下文，存储运行时的重要组件的引用                 |\n\n![sparkonyarn](./spark-role/sparksubmit.png)\n","slug":"spark-role","published":1,"updated":"2022-12-19T01:30:52.693Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffha000m16gn0zk88msb","content":"<blockquote>\n<p>Spark角色</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<table>\n<thead>\n<tr>\n<th>角色名称</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Master</td>\n<td>常驻master守护进程，负责管理worker节点，从master节点提交应用</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>常驻worker守护进程，与master节点通信，并且管理executor进程，运行一个或多个Executor进程，相当于计算节点</td>\n</tr>\n<tr>\n<td>Client</td>\n<td>用户提交作业的客户端</td>\n</tr>\n<tr>\n<td>Driver</td>\n<td>负责控制一个应用的执行，运行Application的main函数和初始化SparkContext，Driver将Task和Task所依赖的file和jar(序列化后)传递给对应的Worker机器执行</td>\n</tr>\n<tr>\n<td>Executor</td>\n<td>executor进程宿主在worker节点上，一个worker可以有多个executor，每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的RDD提供内存式存储，RDD是直接缓存在executor进程内的，因此任务可以运行时充分利用缓存加速运算。一个container对应一个JVM进程(也就是一个executor)</td>\n</tr>\n<tr>\n<td>Job</td>\n<td>action的触发会生成一个job，job会提交给DAGScheduler，分解成Stage</td>\n</tr>\n<tr>\n<td>Stage</td>\n<td>DAGscheduler根据shuffle将job划分为不同的stage，同一个stage包含多个task，这些task有相同shuffle dependencies</td>\n</tr>\n<tr>\n<td>Task</td>\n<td>被送到executor上的工作单元，task简单来说就是在一个数据partition上单个数据处理流程</td>\n</tr>\n<tr>\n<td>Cluster Manager</td>\n<td>在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn)</td>\n</tr>\n<tr>\n<td>SparkContext</td>\n<td>整个应用的上下文，控制程序的生命周期</td>\n</tr>\n<tr>\n<td>DAGScheduler</td>\n<td>根据Job构建基于Stage的DAG工作流，并提交Stage给TaskScheduler</td>\n</tr>\n<tr>\n<td>TaskScheduler</td>\n<td>将task发给Executor执行</td>\n</tr>\n<tr>\n<td>SparkEnv</td>\n<td>线程级别的上下文，存储运行时的重要组件的引用</td>\n</tr>\n</tbody></table>\n<p><img src=\"/2022/12/15/spark-role/sparksubmit.png\" alt=\"sparkonyarn\"></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Spark角色</p>\n</blockquote>","more":"<table>\n<thead>\n<tr>\n<th>角色名称</th>\n<th>解释</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Master</td>\n<td>常驻master守护进程，负责管理worker节点，从master节点提交应用</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>常驻worker守护进程，与master节点通信，并且管理executor进程，运行一个或多个Executor进程，相当于计算节点</td>\n</tr>\n<tr>\n<td>Client</td>\n<td>用户提交作业的客户端</td>\n</tr>\n<tr>\n<td>Driver</td>\n<td>负责控制一个应用的执行，运行Application的main函数和初始化SparkContext，Driver将Task和Task所依赖的file和jar(序列化后)传递给对应的Worker机器执行</td>\n</tr>\n<tr>\n<td>Executor</td>\n<td>executor进程宿主在worker节点上，一个worker可以有多个executor，每个executor持有一个线程池，每个线程可以执行一个task，executor执行完task以后将结果返回driver，每个executor执行的task都属于同一个应用。此外executor还有一个功能就是为应用程序中要求缓存的RDD提供内存式存储，RDD是直接缓存在executor进程内的，因此任务可以运行时充分利用缓存加速运算。一个container对应一个JVM进程(也就是一个executor)</td>\n</tr>\n<tr>\n<td>Job</td>\n<td>action的触发会生成一个job，job会提交给DAGScheduler，分解成Stage</td>\n</tr>\n<tr>\n<td>Stage</td>\n<td>DAGscheduler根据shuffle将job划分为不同的stage，同一个stage包含多个task，这些task有相同shuffle dependencies</td>\n</tr>\n<tr>\n<td>Task</td>\n<td>被送到executor上的工作单元，task简单来说就是在一个数据partition上单个数据处理流程</td>\n</tr>\n<tr>\n<td>Cluster Manager</td>\n<td>在集群上获取资源的外部服务(例如：Standalone、Mesos、Yarn)</td>\n</tr>\n<tr>\n<td>SparkContext</td>\n<td>整个应用的上下文，控制程序的生命周期</td>\n</tr>\n<tr>\n<td>DAGScheduler</td>\n<td>根据Job构建基于Stage的DAG工作流，并提交Stage给TaskScheduler</td>\n</tr>\n<tr>\n<td>TaskScheduler</td>\n<td>将task发给Executor执行</td>\n</tr>\n<tr>\n<td>SparkEnv</td>\n<td>线程级别的上下文，存储运行时的重要组件的引用</td>\n</tr>\n</tbody></table>\n<p><img src=\"/2022/12/15/spark-role/sparksubmit.png\" alt=\"sparkonyarn\"></p>"},{"title":"Spark-SQL","date":"2022-11-25T06:50:13.000Z","_content":"\n> Spark SQL执行原理\n\n<!-- more -->\n\n## Spark-SQL\n\n### 原理\n\n#### 了解Catalyst\n\n  Spark SQL核心：\n\n* 解析(Parser)\n* 优化(Optimizer)\n* 执行(Execution)\n\n  五大组件：\n\n* Parser模块：将sql解析为抽象语法树(AST)\n\n  依赖第三方类库ANTLR\n\n  ![parser](./spark-sql/parser.png)\n\n* Analyzer模块：遍历AST，对AST节点数据类型和函数绑定，根据元数据信息Catalog解析数据表字段\n\n  会判断表名、字段名是否存在\n\n  ![analyzer](./spark-sql/analyzer.png)\n\n* Optimizer模块：Catalyst核心，RBO(基于规则优化)和CBO(基于代价优化)\n\n  谓词下推：过滤操作下推join之前，减少join数据量，减少耗时；\n\n  ![predicate-pushdown](./spark-sql/predicate_pushdown.png)\n\n  常量累加：减少每行重复常量计算，直接使用结果；\n\n  ![constant_fold](./spark-sql/constant_fold.png)\n\n  列值裁剪：裁剪不需要的列，减少网络、内存消耗，提高列式存储扫描效率；\n\n* SparkPlanner模块：优化后的逻辑执行计划(OptimizedLogicalPlan)转为物理计划PhysicalPlan\n\n  例如，Spark会针对join算子制定真正执行的计划：\n\n  * BroadcastHashJoin(小join大)：\n\n    小表分发到大表所在节点，广播算法可以是先发给driver然后分发给executor，在executor执行小表映射，大表试探；\n\n    SparkSQL规定BroadcastHashJoin执行条件，小表小于参数spark.sql.autoBroadcastJoinThreshold，默认10MB；\n\n  * ShuffleHashJoin(大join小)\n\n    两张表中的数据充分布到集群所有节点，称为shuffle，节点上执行hash join算法；\n\n  * SortMergeJoin(大join大)\n\n    先shuffle，节点上两表sort，双指针遍历，匹配merge\n\n  > hash join：\n  >\n  > 1）确定build table和probe table，通常build小probe大\n  >\n  > 2）build table根据key做hash，生成hash table缓存到内存，内存不够dump外存\n  >\n  > 3）扫描probe table相同hash映射，匹配则join到一起 \n\n* CostModel模块：根据性能统计数据，选择最佳物理执行计划，基于CBO\n\n  选择代价最小的物理执行计划\n\n### 总结\n\n * 选择最优物理执行计划，生成java字节码，将SQL转化为DAG，以RDD形式进行执行；\n\n   ![summary](./spark-sql/summary.png)\n\n### 参考\n\nhttps://cloud.tencent.com/developer/article/2008340\n\nhttps://blog.csdn.net/lp284558195/article/details/80717219\n","source":"_posts/spark-sql.md","raw":"---\ntitle: Spark-SQL\ndate: 2022-11-25 14:50:13\ntags: 大数据\n---\n\n> Spark SQL执行原理\n\n<!-- more -->\n\n## Spark-SQL\n\n### 原理\n\n#### 了解Catalyst\n\n  Spark SQL核心：\n\n* 解析(Parser)\n* 优化(Optimizer)\n* 执行(Execution)\n\n  五大组件：\n\n* Parser模块：将sql解析为抽象语法树(AST)\n\n  依赖第三方类库ANTLR\n\n  ![parser](./spark-sql/parser.png)\n\n* Analyzer模块：遍历AST，对AST节点数据类型和函数绑定，根据元数据信息Catalog解析数据表字段\n\n  会判断表名、字段名是否存在\n\n  ![analyzer](./spark-sql/analyzer.png)\n\n* Optimizer模块：Catalyst核心，RBO(基于规则优化)和CBO(基于代价优化)\n\n  谓词下推：过滤操作下推join之前，减少join数据量，减少耗时；\n\n  ![predicate-pushdown](./spark-sql/predicate_pushdown.png)\n\n  常量累加：减少每行重复常量计算，直接使用结果；\n\n  ![constant_fold](./spark-sql/constant_fold.png)\n\n  列值裁剪：裁剪不需要的列，减少网络、内存消耗，提高列式存储扫描效率；\n\n* SparkPlanner模块：优化后的逻辑执行计划(OptimizedLogicalPlan)转为物理计划PhysicalPlan\n\n  例如，Spark会针对join算子制定真正执行的计划：\n\n  * BroadcastHashJoin(小join大)：\n\n    小表分发到大表所在节点，广播算法可以是先发给driver然后分发给executor，在executor执行小表映射，大表试探；\n\n    SparkSQL规定BroadcastHashJoin执行条件，小表小于参数spark.sql.autoBroadcastJoinThreshold，默认10MB；\n\n  * ShuffleHashJoin(大join小)\n\n    两张表中的数据充分布到集群所有节点，称为shuffle，节点上执行hash join算法；\n\n  * SortMergeJoin(大join大)\n\n    先shuffle，节点上两表sort，双指针遍历，匹配merge\n\n  > hash join：\n  >\n  > 1）确定build table和probe table，通常build小probe大\n  >\n  > 2）build table根据key做hash，生成hash table缓存到内存，内存不够dump外存\n  >\n  > 3）扫描probe table相同hash映射，匹配则join到一起 \n\n* CostModel模块：根据性能统计数据，选择最佳物理执行计划，基于CBO\n\n  选择代价最小的物理执行计划\n\n### 总结\n\n * 选择最优物理执行计划，生成java字节码，将SQL转化为DAG，以RDD形式进行执行；\n\n   ![summary](./spark-sql/summary.png)\n\n### 参考\n\nhttps://cloud.tencent.com/developer/article/2008340\n\nhttps://blog.csdn.net/lp284558195/article/details/80717219\n","slug":"spark-sql","published":1,"updated":"2022-12-19T01:30:52.694Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffhb000p16gnfwgi2hj4","content":"<blockquote>\n<p>Spark SQL执行原理</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h2 id=\"Spark-SQL\"><a href=\"#Spark-SQL\" class=\"headerlink\" title=\"Spark-SQL\"></a>Spark-SQL</h2><h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><h4 id=\"了解Catalyst\"><a href=\"#了解Catalyst\" class=\"headerlink\" title=\"了解Catalyst\"></a>了解Catalyst</h4><p>  Spark SQL核心：</p>\n<ul>\n<li><p>解析(Parser)</p>\n</li>\n<li><p>优化(Optimizer)</p>\n</li>\n<li><p>执行(Execution)</p>\n<p>五大组件：</p>\n</li>\n<li><p>Parser模块：将sql解析为抽象语法树(AST)</p>\n<p>依赖第三方类库ANTLR</p>\n<p><img src=\"/2022/11/25/spark-sql/parser.png\" alt=\"parser\"></p>\n</li>\n<li><p>Analyzer模块：遍历AST，对AST节点数据类型和函数绑定，根据元数据信息Catalog解析数据表字段</p>\n<p>会判断表名、字段名是否存在</p>\n<p><img src=\"/2022/11/25/spark-sql/analyzer.png\" alt=\"analyzer\"></p>\n</li>\n<li><p>Optimizer模块：Catalyst核心，RBO(基于规则优化)和CBO(基于代价优化)</p>\n<p>谓词下推：过滤操作下推join之前，减少join数据量，减少耗时；</p>\n<p><img src=\"/2022/11/25/spark-sql/predicate_pushdown.png\" alt=\"predicate-pushdown\"></p>\n<p>常量累加：减少每行重复常量计算，直接使用结果；</p>\n<p><img src=\"/2022/11/25/spark-sql/constant_fold.png\" alt=\"constant_fold\"></p>\n<p>列值裁剪：裁剪不需要的列，减少网络、内存消耗，提高列式存储扫描效率；</p>\n</li>\n<li><p>SparkPlanner模块：优化后的逻辑执行计划(OptimizedLogicalPlan)转为物理计划PhysicalPlan</p>\n<p>例如，Spark会针对join算子制定真正执行的计划：</p>\n<ul>\n<li><p>BroadcastHashJoin(小join大)：</p>\n<p>小表分发到大表所在节点，广播算法可以是先发给driver然后分发给executor，在executor执行小表映射，大表试探；</p>\n<p>SparkSQL规定BroadcastHashJoin执行条件，小表小于参数spark.sql.autoBroadcastJoinThreshold，默认10MB；</p>\n</li>\n<li><p>ShuffleHashJoin(大join小)</p>\n<p>两张表中的数据充分布到集群所有节点，称为shuffle，节点上执行hash join算法；</p>\n</li>\n<li><p>SortMergeJoin(大join大)</p>\n<p>先shuffle，节点上两表sort，双指针遍历，匹配merge</p>\n</li>\n</ul>\n<blockquote>\n<p>hash join：</p>\n<p>1）确定build table和probe table，通常build小probe大</p>\n<p>2）build table根据key做hash，生成hash table缓存到内存，内存不够dump外存</p>\n<p>3）扫描probe table相同hash映射，匹配则join到一起 </p>\n</blockquote>\n</li>\n<li><p>CostModel模块：根据性能统计数据，选择最佳物理执行计划，基于CBO</p>\n<p>选择代价最小的物理执行计划</p>\n</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li><p>选择最优物理执行计划，生成java字节码，将SQL转化为DAG，以RDD形式进行执行；</p>\n<p><img src=\"/2022/11/25/spark-sql/summary.png\" alt=\"summary\"></p>\n</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://cloud.tencent.com/developer/article/2008340\">https://cloud.tencent.com/developer/article/2008340</a></p>\n<p><a href=\"https://blog.csdn.net/lp284558195/article/details/80717219\">https://blog.csdn.net/lp284558195/article/details/80717219</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Spark SQL执行原理</p>\n</blockquote>","more":"<h2 id=\"Spark-SQL\"><a href=\"#Spark-SQL\" class=\"headerlink\" title=\"Spark-SQL\"></a>Spark-SQL</h2><h3 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h3><h4 id=\"了解Catalyst\"><a href=\"#了解Catalyst\" class=\"headerlink\" title=\"了解Catalyst\"></a>了解Catalyst</h4><p>  Spark SQL核心：</p>\n<ul>\n<li><p>解析(Parser)</p>\n</li>\n<li><p>优化(Optimizer)</p>\n</li>\n<li><p>执行(Execution)</p>\n<p>五大组件：</p>\n</li>\n<li><p>Parser模块：将sql解析为抽象语法树(AST)</p>\n<p>依赖第三方类库ANTLR</p>\n<p><img src=\"/2022/11/25/spark-sql/parser.png\" alt=\"parser\"></p>\n</li>\n<li><p>Analyzer模块：遍历AST，对AST节点数据类型和函数绑定，根据元数据信息Catalog解析数据表字段</p>\n<p>会判断表名、字段名是否存在</p>\n<p><img src=\"/2022/11/25/spark-sql/analyzer.png\" alt=\"analyzer\"></p>\n</li>\n<li><p>Optimizer模块：Catalyst核心，RBO(基于规则优化)和CBO(基于代价优化)</p>\n<p>谓词下推：过滤操作下推join之前，减少join数据量，减少耗时；</p>\n<p><img src=\"/2022/11/25/spark-sql/predicate_pushdown.png\" alt=\"predicate-pushdown\"></p>\n<p>常量累加：减少每行重复常量计算，直接使用结果；</p>\n<p><img src=\"/2022/11/25/spark-sql/constant_fold.png\" alt=\"constant_fold\"></p>\n<p>列值裁剪：裁剪不需要的列，减少网络、内存消耗，提高列式存储扫描效率；</p>\n</li>\n<li><p>SparkPlanner模块：优化后的逻辑执行计划(OptimizedLogicalPlan)转为物理计划PhysicalPlan</p>\n<p>例如，Spark会针对join算子制定真正执行的计划：</p>\n<ul>\n<li><p>BroadcastHashJoin(小join大)：</p>\n<p>小表分发到大表所在节点，广播算法可以是先发给driver然后分发给executor，在executor执行小表映射，大表试探；</p>\n<p>SparkSQL规定BroadcastHashJoin执行条件，小表小于参数spark.sql.autoBroadcastJoinThreshold，默认10MB；</p>\n</li>\n<li><p>ShuffleHashJoin(大join小)</p>\n<p>两张表中的数据充分布到集群所有节点，称为shuffle，节点上执行hash join算法；</p>\n</li>\n<li><p>SortMergeJoin(大join大)</p>\n<p>先shuffle，节点上两表sort，双指针遍历，匹配merge</p>\n</li>\n</ul>\n<blockquote>\n<p>hash join：</p>\n<p>1）确定build table和probe table，通常build小probe大</p>\n<p>2）build table根据key做hash，生成hash table缓存到内存，内存不够dump外存</p>\n<p>3）扫描probe table相同hash映射，匹配则join到一起 </p>\n</blockquote>\n</li>\n<li><p>CostModel模块：根据性能统计数据，选择最佳物理执行计划，基于CBO</p>\n<p>选择代价最小的物理执行计划</p>\n</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li><p>选择最优物理执行计划，生成java字节码，将SQL转化为DAG，以RDD形式进行执行；</p>\n<p><img src=\"/2022/11/25/spark-sql/summary.png\" alt=\"summary\"></p>\n</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://cloud.tencent.com/developer/article/2008340\">https://cloud.tencent.com/developer/article/2008340</a></p>\n<p><a href=\"https://blog.csdn.net/lp284558195/article/details/80717219\">https://blog.csdn.net/lp284558195/article/details/80717219</a></p>"},{"title":"spark-yarn","date":"2022-12-15T02:39:52.000Z","_content":"\n> Spark on Yarn执行流程\n\n<!-- more -->\n\nSparkOnYarn集群，以HDFS为文件系统，以Yarn为资源管理和资源分配建立Spark集群，集群3个节点，1个master节点，2个slave节点\n\n![sparkonyarn](./spark-yarn/sparkonyarn.png)\n\n### Yarn-Cluster模式\n\n> 1.第一阶段：Spark的**Driver作为一个ApplicationMaster**在Yarn集群中启动\n>\n> 2.第二阶段：ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控运行过程，直到运行完成\n\n![yarncluster](./spark-yarn/yarncluster.png)\n\n流程说明：\n\n* Spark Yarn Client向Yarn提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；\n* ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，启动ApplicationMaster进行SparkContext等的初始化；\n* ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序运行状态，然后它将采用轮训的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；\n* 一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这点同Standalone，但是SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑；\n* ApplicationMaster中的SparkContext分配给Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行时的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重启；\n* 应用程序运行结束，ApplicationMaster向ResourceManager申请注销和关闭\n\n### Yarn-Client模式\n\n![yarnclient](./spark-yarn/yarnclient.png)\n\n* Spark Yarn Client向Yarn的ResourceManager申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TaskScheduler、SparkEnv对象等，由于选择的是Yarn-Client模式，程序会选择YarnClientScheduler和YarnClientSchedulerBackend；\n* ResourceManager收到请求后，会在集群在选择一个NodeManager，为该程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与Yarn-Cluser区别在于ApplicationMaster不运行SparkContext，只与SparkContext进行联系资源的分派；\n* Client的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源；\n* 一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向client中的SparkContext注册并申请Task；\n* Client的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而在任务失败时重新启动任务；\n* 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销和关闭；\n\n### 两者区别\n\n在Yarn中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container；\n\n| 比较项             | YARN-Client模式                                              | YARN-Cluster模式                                             |\n| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Application Master | Application Master仅仅向Yarn请求Executor，Client会和请求的Container通信来调度它们工作； | Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。用户提交作业之后，可以关掉Client，作业会继续在YARN运行； |\n| Driver             | 运行在Client中                                               | 运行在AM(Application Master)中                               |\n| client关闭         | 关闭client，任务就直接结束                                   | 提交任务后可以关闭client，不影响集群程序的运行               |\n| 使用场景           | 适合交互和调试环境                                           | 适合生产环境                                                 |\n| 监控日志           | 直接查看                                                     | yarn logs -applicationId xxxxxx                              |\n| 优点               | 便于调试和查看监控日志                                       | 可以直接关闭client，不影响集群程序的运行                     |\n| 缺点               | 1.由于存在大量数据在Driver和集群中进行交互，会在运行过程中产生大量网络数据传输，网络开销加大；2.client关闭，任务结束； | 不便于交互和查看监控日志                                     |\n\n### 参考\n\nsparkhttps://blog.csdn.net/bocai8058/article/details/83051242\n\nshufflehttps://blog.csdn.net/bocai8058/article/details/83051403\n\nyarnhttps://blog.csdn.net/bocai8058/article/details/119300198\n\n算子https://blog.csdn.net/bocai8058/article/details/95651583\n","source":"_posts/spark-yarn.md","raw":"---\ntitle: spark-yarn\ndate: 2022-12-15 10:39:52\ntags: Spark\n---\n\n> Spark on Yarn执行流程\n\n<!-- more -->\n\nSparkOnYarn集群，以HDFS为文件系统，以Yarn为资源管理和资源分配建立Spark集群，集群3个节点，1个master节点，2个slave节点\n\n![sparkonyarn](./spark-yarn/sparkonyarn.png)\n\n### Yarn-Cluster模式\n\n> 1.第一阶段：Spark的**Driver作为一个ApplicationMaster**在Yarn集群中启动\n>\n> 2.第二阶段：ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控运行过程，直到运行完成\n\n![yarncluster](./spark-yarn/yarncluster.png)\n\n流程说明：\n\n* Spark Yarn Client向Yarn提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；\n* ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，启动ApplicationMaster进行SparkContext等的初始化；\n* ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序运行状态，然后它将采用轮训的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；\n* 一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这点同Standalone，但是SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑；\n* ApplicationMaster中的SparkContext分配给Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行时的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重启；\n* 应用程序运行结束，ApplicationMaster向ResourceManager申请注销和关闭\n\n### Yarn-Client模式\n\n![yarnclient](./spark-yarn/yarnclient.png)\n\n* Spark Yarn Client向Yarn的ResourceManager申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TaskScheduler、SparkEnv对象等，由于选择的是Yarn-Client模式，程序会选择YarnClientScheduler和YarnClientSchedulerBackend；\n* ResourceManager收到请求后，会在集群在选择一个NodeManager，为该程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与Yarn-Cluser区别在于ApplicationMaster不运行SparkContext，只与SparkContext进行联系资源的分派；\n* Client的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源；\n* 一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向client中的SparkContext注册并申请Task；\n* Client的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而在任务失败时重新启动任务；\n* 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销和关闭；\n\n### 两者区别\n\n在Yarn中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container；\n\n| 比较项             | YARN-Client模式                                              | YARN-Cluster模式                                             |\n| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Application Master | Application Master仅仅向Yarn请求Executor，Client会和请求的Container通信来调度它们工作； | Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。用户提交作业之后，可以关掉Client，作业会继续在YARN运行； |\n| Driver             | 运行在Client中                                               | 运行在AM(Application Master)中                               |\n| client关闭         | 关闭client，任务就直接结束                                   | 提交任务后可以关闭client，不影响集群程序的运行               |\n| 使用场景           | 适合交互和调试环境                                           | 适合生产环境                                                 |\n| 监控日志           | 直接查看                                                     | yarn logs -applicationId xxxxxx                              |\n| 优点               | 便于调试和查看监控日志                                       | 可以直接关闭client，不影响集群程序的运行                     |\n| 缺点               | 1.由于存在大量数据在Driver和集群中进行交互，会在运行过程中产生大量网络数据传输，网络开销加大；2.client关闭，任务结束； | 不便于交互和查看监控日志                                     |\n\n### 参考\n\nsparkhttps://blog.csdn.net/bocai8058/article/details/83051242\n\nshufflehttps://blog.csdn.net/bocai8058/article/details/83051403\n\nyarnhttps://blog.csdn.net/bocai8058/article/details/119300198\n\n算子https://blog.csdn.net/bocai8058/article/details/95651583\n","slug":"spark-yarn","published":1,"updated":"2022-12-19T01:30:52.697Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffhc001116gngqhhamxk","content":"<blockquote>\n<p>Spark on Yarn执行流程</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<p>SparkOnYarn集群，以HDFS为文件系统，以Yarn为资源管理和资源分配建立Spark集群，集群3个节点，1个master节点，2个slave节点</p>\n<p><img src=\"/2022/12/15/spark-yarn/sparkonyarn.png\" alt=\"sparkonyarn\"></p>\n<h3 id=\"Yarn-Cluster模式\"><a href=\"#Yarn-Cluster模式\" class=\"headerlink\" title=\"Yarn-Cluster模式\"></a>Yarn-Cluster模式</h3><blockquote>\n<p>1.第一阶段：Spark的<strong>Driver作为一个ApplicationMaster</strong>在Yarn集群中启动</p>\n<p>2.第二阶段：ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控运行过程，直到运行完成</p>\n</blockquote>\n<p><img src=\"/2022/12/15/spark-yarn/yarncluster.png\" alt=\"yarncluster\"></p>\n<p>流程说明：</p>\n<ul>\n<li>Spark Yarn Client向Yarn提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；</li>\n<li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，启动ApplicationMaster进行SparkContext等的初始化；</li>\n<li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序运行状态，然后它将采用轮训的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；</li>\n<li>一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这点同Standalone，但是SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑；</li>\n<li>ApplicationMaster中的SparkContext分配给Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行时的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重启；</li>\n<li>应用程序运行结束，ApplicationMaster向ResourceManager申请注销和关闭</li>\n</ul>\n<h3 id=\"Yarn-Client模式\"><a href=\"#Yarn-Client模式\" class=\"headerlink\" title=\"Yarn-Client模式\"></a>Yarn-Client模式</h3><p><img src=\"/2022/12/15/spark-yarn/yarnclient.png\" alt=\"yarnclient\"></p>\n<ul>\n<li>Spark Yarn Client向Yarn的ResourceManager申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TaskScheduler、SparkEnv对象等，由于选择的是Yarn-Client模式，程序会选择YarnClientScheduler和YarnClientSchedulerBackend；</li>\n<li>ResourceManager收到请求后，会在集群在选择一个NodeManager，为该程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与Yarn-Cluser区别在于ApplicationMaster不运行SparkContext，只与SparkContext进行联系资源的分派；</li>\n<li>Client的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源；</li>\n<li>一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向client中的SparkContext注册并申请Task；</li>\n<li>Client的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而在任务失败时重新启动任务；</li>\n<li>应用程序运行完成后，Client的SparkContext向ResourceManager申请注销和关闭；</li>\n</ul>\n<h3 id=\"两者区别\"><a href=\"#两者区别\" class=\"headerlink\" title=\"两者区别\"></a>两者区别</h3><p>在Yarn中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container；</p>\n<table>\n<thead>\n<tr>\n<th>比较项</th>\n<th>YARN-Client模式</th>\n<th>YARN-Cluster模式</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Application Master</td>\n<td>Application Master仅仅向Yarn请求Executor，Client会和请求的Container通信来调度它们工作；</td>\n<td>Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。用户提交作业之后，可以关掉Client，作业会继续在YARN运行；</td>\n</tr>\n<tr>\n<td>Driver</td>\n<td>运行在Client中</td>\n<td>运行在AM(Application Master)中</td>\n</tr>\n<tr>\n<td>client关闭</td>\n<td>关闭client，任务就直接结束</td>\n<td>提交任务后可以关闭client，不影响集群程序的运行</td>\n</tr>\n<tr>\n<td>使用场景</td>\n<td>适合交互和调试环境</td>\n<td>适合生产环境</td>\n</tr>\n<tr>\n<td>监控日志</td>\n<td>直接查看</td>\n<td>yarn logs -applicationId xxxxxx</td>\n</tr>\n<tr>\n<td>优点</td>\n<td>便于调试和查看监控日志</td>\n<td>可以直接关闭client，不影响集群程序的运行</td>\n</tr>\n<tr>\n<td>缺点</td>\n<td>1.由于存在大量数据在Driver和集群中进行交互，会在运行过程中产生大量网络数据传输，网络开销加大；2.client关闭，任务结束；</td>\n<td>不便于交互和查看监控日志</td>\n</tr>\n</tbody></table>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>spark<a href=\"https://blog.csdn.net/bocai8058/article/details/83051242\">https://blog.csdn.net/bocai8058/article/details/83051242</a></p>\n<p>shuffle<a href=\"https://blog.csdn.net/bocai8058/article/details/83051403\">https://blog.csdn.net/bocai8058/article/details/83051403</a></p>\n<p>yarn<a href=\"https://blog.csdn.net/bocai8058/article/details/119300198\">https://blog.csdn.net/bocai8058/article/details/119300198</a></p>\n<p>算子<a href=\"https://blog.csdn.net/bocai8058/article/details/95651583\">https://blog.csdn.net/bocai8058/article/details/95651583</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Spark on Yarn执行流程</p>\n</blockquote>","more":"<p>SparkOnYarn集群，以HDFS为文件系统，以Yarn为资源管理和资源分配建立Spark集群，集群3个节点，1个master节点，2个slave节点</p>\n<p><img src=\"/2022/12/15/spark-yarn/sparkonyarn.png\" alt=\"sparkonyarn\"></p>\n<h3 id=\"Yarn-Cluster模式\"><a href=\"#Yarn-Cluster模式\" class=\"headerlink\" title=\"Yarn-Cluster模式\"></a>Yarn-Cluster模式</h3><blockquote>\n<p>1.第一阶段：Spark的<strong>Driver作为一个ApplicationMaster</strong>在Yarn集群中启动</p>\n<p>2.第二阶段：ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控运行过程，直到运行完成</p>\n</blockquote>\n<p><img src=\"/2022/12/15/spark-yarn/yarncluster.png\" alt=\"yarncluster\"></p>\n<p>流程说明：</p>\n<ul>\n<li>Spark Yarn Client向Yarn提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等；</li>\n<li>ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，启动ApplicationMaster进行SparkContext等的初始化；</li>\n<li>ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序运行状态，然后它将采用轮训的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束；</li>\n<li>一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这点同Standalone，但是SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑；</li>\n<li>ApplicationMaster中的SparkContext分配给Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行时的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重启；</li>\n<li>应用程序运行结束，ApplicationMaster向ResourceManager申请注销和关闭</li>\n</ul>\n<h3 id=\"Yarn-Client模式\"><a href=\"#Yarn-Client模式\" class=\"headerlink\" title=\"Yarn-Client模式\"></a>Yarn-Client模式</h3><p><img src=\"/2022/12/15/spark-yarn/yarnclient.png\" alt=\"yarnclient\"></p>\n<ul>\n<li>Spark Yarn Client向Yarn的ResourceManager申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TaskScheduler、SparkEnv对象等，由于选择的是Yarn-Client模式，程序会选择YarnClientScheduler和YarnClientSchedulerBackend；</li>\n<li>ResourceManager收到请求后，会在集群在选择一个NodeManager，为该程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与Yarn-Cluser区别在于ApplicationMaster不运行SparkContext，只与SparkContext进行联系资源的分派；</li>\n<li>Client的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源；</li>\n<li>一旦ApplicationMaster申请到资源(也就是Container)后，便与对应的NodeManager通信，要求它在获得Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向client中的SparkContext注册并申请Task；</li>\n<li>Client的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，让Client随时掌握各个任务的运行状态，从而在任务失败时重新启动任务；</li>\n<li>应用程序运行完成后，Client的SparkContext向ResourceManager申请注销和关闭；</li>\n</ul>\n<h3 id=\"两者区别\"><a href=\"#两者区别\" class=\"headerlink\" title=\"两者区别\"></a>两者区别</h3><p>在Yarn中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container；</p>\n<table>\n<thead>\n<tr>\n<th>比较项</th>\n<th>YARN-Client模式</th>\n<th>YARN-Cluster模式</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Application Master</td>\n<td>Application Master仅仅向Yarn请求Executor，Client会和请求的Container通信来调度它们工作；</td>\n<td>Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。用户提交作业之后，可以关掉Client，作业会继续在YARN运行；</td>\n</tr>\n<tr>\n<td>Driver</td>\n<td>运行在Client中</td>\n<td>运行在AM(Application Master)中</td>\n</tr>\n<tr>\n<td>client关闭</td>\n<td>关闭client，任务就直接结束</td>\n<td>提交任务后可以关闭client，不影响集群程序的运行</td>\n</tr>\n<tr>\n<td>使用场景</td>\n<td>适合交互和调试环境</td>\n<td>适合生产环境</td>\n</tr>\n<tr>\n<td>监控日志</td>\n<td>直接查看</td>\n<td>yarn logs -applicationId xxxxxx</td>\n</tr>\n<tr>\n<td>优点</td>\n<td>便于调试和查看监控日志</td>\n<td>可以直接关闭client，不影响集群程序的运行</td>\n</tr>\n<tr>\n<td>缺点</td>\n<td>1.由于存在大量数据在Driver和集群中进行交互，会在运行过程中产生大量网络数据传输，网络开销加大；2.client关闭，任务结束；</td>\n<td>不便于交互和查看监控日志</td>\n</tr>\n</tbody></table>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>spark<a href=\"https://blog.csdn.net/bocai8058/article/details/83051242\">https://blog.csdn.net/bocai8058/article/details/83051242</a></p>\n<p>shuffle<a href=\"https://blog.csdn.net/bocai8058/article/details/83051403\">https://blog.csdn.net/bocai8058/article/details/83051403</a></p>\n<p>yarn<a href=\"https://blog.csdn.net/bocai8058/article/details/119300198\">https://blog.csdn.net/bocai8058/article/details/119300198</a></p>\n<p>算子<a href=\"https://blog.csdn.net/bocai8058/article/details/95651583\">https://blog.csdn.net/bocai8058/article/details/95651583</a></p>"},{"title":"spark-opt","date":"2022-12-09T07:55:51.000Z","_content":"\n> Spark汇总\n\n<!-- more -->\n\n### Spark优化原则\n\n#### 原则1:避免创建重复的RDD\n\n#### 原则2:尽可能复用同一个RDD\n\n#### 原则3:对多次使用的RDD进行持久化\n\ncache() 使用**非序列化**的方法将RDD中的数据全部尝试持久化到内存中；\n\npersist() 选择持久化级别、方式进行持久化。_SER后缀表示序列化后保存，每个Partition会被序列化一个大的字节数组，然后持久化到内存或磁盘，减少数据占用内存过多，避免频繁GC；\n\n| 级别                | 含义                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| MEMORY_ONLY         | 默认，放内存，同cache()，内存不够就不持久化，算子操作时候从源头计算一遍； |\n| MEMORY_AND_RISK     | 内存不够放磁盘，算子操作时从磁盘中取出来用；                 |\n| MEMORY_ONLY_SER     | 同MEMORY_ONLY，区别是序列化，节省内存避免频繁GC；            |\n| MEMORY_AND_DISK_SER | 同MEMORY_ONLY_SER，区别是序列化，节省内存避免频繁GC；        |\n| DISK_ONLY           | 放磁盘；                                                     |\n| *_2                 | 上面的所有策略+_2后缀表示每个持久化数据都有副本放其他节点，容灾； |\n\n#### 原则4:尽量避免使用shuffle类算子\n\nshuffle解释：将分布在集群多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作，比如，reduceByKey、join、distinct、repartition等，应该尽量使用map类操作替代;\n\nshuffle过程：各个节点相同key都会先写磁盘，其他节点通过网络传输拉取各个节点磁盘文件中相同key，节点内存不够溢写磁盘，大量的磁盘IO和网络传输导致shuffle性能较差；\n\n```scala\n// broadcast+map替代join\nval rdd3 = rdd1.join(rdd2)\n// broadcast+map不会产生shuffle 适用于rdd2数据量较少 <2G \nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data) // 每个Executer都有一份rdd2了\nval rdd3 = rdd1.map(rdd2DataBroadcast...)// 找到相同key然后处理\n```\n\n#### 原则5:使用map-side预聚合的shuffle操作\n\n#### 原则6:使用高性能算子\n\ngroupByKey是直接shuffle，reduceByKey和aggregateByKey先预聚合再shuffle，区别是后者可以指定初始值并且partition内部和之间的聚合操作可以不同，如果相同可以用foldByKey，\n\n```scala\nobject AggByKeyOpt {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName(\"test\").setMaster(\"local\")\n    val sc = new SparkContext(sparkConf)\n    val data = Seq((1,3),(1,2),(1,4),(2,3))\n    val rdd = sc.parallelize(data, 2)\n    //合并不同partition中的值，a，b得数据类型为zeroValue的数据类型\n    def combOp(a:String,b:String):String={\n      println(\"combOp: \"+a+\"\\t\"+b)\n      a+b\n    }\n    //合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型\n    def seqOp(a:String,b:Int):String={\n      println(\"SeqOp:\"+a+\"\\t\"+b)\n      a+b\n    }\n    rdd.foreach(println)\n    val aggregateByKeyRDD=rdd.aggregateByKey(\"100\")(seqOp, combOp)\n    aggregateByKeyRDD.foreach(println)\n    sc.stop()\n  }\n}\n```\n\nmapPartitions代替map，一次函数调用处理一个partition而非单条，容易OOM，如果内存不够垃圾回收无法回收太多对象；\n\nforeachPartitions代替foreach，对于每个partition如果连接mysql连接一次就可以；\n\nfilter之后使用coaleasce操作，将rdd压缩到更少partition使用更少task处理；\n\nrepartitionAndSortWithinPartitions代替repartition与sort操作，shuffle和sort同时进行，性能更高；\n\n#### 原则7:广播大变量\n\n大变量，100M以上，默认情况下会复制多份传输到task中，大量变量副本消耗网络传输，在各个节点Executor中占用过多内存频繁GC，广播的话，每个Executor只保留一份副本，Executor中的task共用；\n\n```scala\nval list1 = ...\nval list1Broadcast = sc.broadcast(list1)\nrdd1.map(list1Broadcast...)\n```\n\n#### 原则8 使用Kryo优化序列化性能\n\nSpark中三个地方涉及到了序列化：\n\n* 算子函数使用外部变量时，变量会被序列化后进行网络传输；\n* 自定义类型作为RDD的范型时，会进行序列化，要求自定义类实现Serializable接口；\n* 使用可序列化的持久化策略时，例如MEMORY_ONLY_SER；\n\nSpark默认使用Java序列化机制，ObjectOutputStream/ObjectInputStream API进行序列化和反序列化，使用Kryo性能提高10倍；\n\n```scala\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n```\n\n#### 原则9:优化数据结构\n\n三种数据类型比较耗费内存:1)对象，每个Java对象都有对象头、引用等额外信息，比较占用内存；2)字符串，每个字符串内部都有一个字符数组以及长度等信息；3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry；\n\n官方建议，使用原始类型比如Int、long代替字符串，数组代替集合类型，减少内存占用，降低GC频率，提升性能；\n\n### Spark作业基本运行原理\n\n![spark-submit](./spark-opt/spark-submit.png)\n\nspark-submit提交Spark作业，启动一个对应Driver进程，根据部署模式不同，本地启动或者集群某个工作节点启动。Driver会占用一定数量的内存和CPU core。Driver进程首先向资源管理集群(Yarn等)申请资源，资源指的是Executor进行，资源管理集群会根据资源参数，在各个工作节点上启动一定数量的Executor进程，每个Executor进程会占用一定数量的内存和CPU core；\n\n资源到位，Driver开始将Spark作业拆分多个stage，并为每个stage创建一批task，然后将task分配到Executor执行，task是最小的计算单元，负责执行一摸一样的计算逻辑(代码片段)，stage的所有task执行完毕之后，中间结果写入本地磁盘，Driver开始调度运行下一个stage，输入数据是上一个stage中间结果，直到全部执行；\n\nSpark根据shuffle类算子进行stage划分，每个stage会通过网络传输拉取需要自己处理的所有相同key，然后进行聚合操作，这就是shuffle；\n\ncache/persist会把task计算出来的数据保存到Executor进程的内存或者所在节点的磁盘文件中；\n\nExecutor内存模型：20%执行task逻辑，20%执行shuffle操作，60%执行RDD持久化，详见参考(3)；\n\ntask执行速度和CPU core直接相关，task执行独占CPU core；\n\n#### num-executors\n\nSpark作业总共要用多少Executor进程执行，太少无法充分利用集群资源，太多队列无法给予，一般50-100；\n\n#### executor-memory\n\nExecutor内存大小决定Spark作业性能，太小容易OOM，一般4G-8G；\n\n#### executor-cores\n\n每个Executor进程的CPU core数量，越多执行越快，一般2-4；\n\n#### driver-memory\n\n设置Driver进程内存，如果使用collect算子会将RDD数据拉到Driver上进行处理需要更多内存否则OOM，否则，一般1G；\n\n#### spark.default.parallelism\n\n每个stage默认task数量，默认是根据底层HDFS block(hadoop2.x是128M)数量，通常默认很少(几十)，官网建议num-executors*executor-cores的2-3倍，一般500-1000；\n\n#### spark.storage.memoryFraction\n\nRDD持久化数据在Executor内存占比，默认0.6，如果内存不够就写磁盘，持久化操作多就提高，shuffle类操作多就降低，频繁GC说明执行task逻辑内存不够也降低(通过spark web ui查看gc耗时)；\n\n#### spark.shuffle.memoryFraction\n\nshuffle过程中拉到上个stage输出后，进行聚。合操作能使用的Executor内存比例，默认0.2，超过溢写磁盘，降低性能。如果RDD持久化操作少，shuffle操作多建议提高，如果作业频繁GC导致运行缓慢，意味着task逻辑执行内存不够，建议降低；\n\n```bash\n# 示例\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n### Spark数据倾斜处理\n\n#### 现象\n\n1）个别task执行慢\n\n2）突然OOM异常\n\n#### 原理\n\n进行shuffle时候，某个key对应数据量特别大，spark运行进度是由运行时间最长的task决定；\n\n#### 定位\n\n可能触发shuffle操作算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup(对两个RDD中的KV元素,每个RDD中相同key中的元素分别聚合成一个集合)、repartition等；\n\nSpark Web UI查看stage各个task分配的数据量、运行时间；\n\n推算stage与代码关系：shuffle类算子前后会分两个stage，如下，reduceByKey前后会划分两个stage，stage0执行textFile到map以及shuffle write操作，每个task处理的数据中，相同key会写入同一个磁盘文件；stage1执行reduceByKey到collect操作，首先执行shuffle read操作，会从stage0各个task所在节点拉取需要key，然后对相同key进行聚合或者join操作，在这里是对相同key的value累加，最后执行collect算子，将所有数据拉到Driver上打印输出；\n\n查看key分布：直接通过sparksql查看key分布，如果是RDD文件可以通过RDD.countByKey()之后collect/take打印；\n\n```scala\n// 单词计数\nval conf = new SparkConf()\nval sc = new SparkContext(conf)\nval lines = sc.textFile(\"hdfs://...\")\nval words = lines.flatMap(_.split(\" \"))\nval pairs = words.map((_,1))\nval wordCounts = pairs.reduceByKey(_ + _) // \nwordCounts.collect().foreach(println(_))\n```\n\n#### 解决方案1：使用Hive ETL预处理数据\n\n预先聚合或join，将数据倾斜发生提前，减少spark shuffle时间，治标不治本，还是会发生倾斜；\n\n#### 解决方案2：过滤少数导致倾斜的key\n\n#### 解决方案3：提高shuffle操作的并行度\n\nreduceByKey(1000)，增加shuffle read task并行度，spark.sql.shuffle.partitions就代表该并行度，默认值200，200个task不够，会导致单task分配过多数据，增多可以减少单task执行时间(原来单task分配5个key，增加之后只分1个key)。实现简单，但是效果有限，因为如果单key对应数据太多没法再分；\n\n#### 解决方案4：两阶段聚合(局部聚合+全局聚合)\n\n首先局部聚合，key+随机前缀，然后执行reduceByKey等操作，之后去掉key随机前缀，在进行全局聚合，得到最终结果；适用于聚合类shuffle操作，不适用于join类shuffle操作；\n\n#### 解决方案5：将reduce join转为map join\n\n适用于join类操作时其中一个RDD或表较小(<2G)，使用broadcast变量完全规避shuffle，使用map进行连接；将较小的RDD直接通过collect算子拉取到Driver端内存再分发到Executor；\n\n```java\nList<Tuple2<Long, Row>> rdd1Data = rdd1.collect();\nfinal Broadcast<List<Tuple2<Long, Row>>> rdd1DataBroadcast = sc.broadcast(rdd1Data); // 广播\nJavaPairRDD<String,Tuple2<String,Row>> joinedRdd = rdd2.mapToPair(\n\tnew PairFunction<Tuple2<Long,String>,String,Tuple2<String,Row>>(){\n    private static final long serialVersionUID = 1L;\n    @override\n    public Tuple2<String, Tuple2<String, Row>> call(Tuple2<Long, String> tuple) throw Exception{\n      List<Tuple2<Long, Row>> rdd1Data = rdd1DataBroadcast.value(); // 获取广播变量\n      Map<Long, Row> rdd1DataMap = new HashMap<Long, Row>(); // 广播变量转化为Map\n      for(Tuple2<Long, Row> data: rdd1Data){\n        rdd1DataMap.put(data._1, data._2);\n      }\n      // 获取当前rdd的key和value\n      String key = tuple._1;\n      String value = tuple._2;\n      Row rdd1Value = rdd1DataMap.get(key); // 获取join到数据\n      return new Tuple2<String, String>(key, new Tuple2<String, Row>(value, rdd1Value));\n    }\n  }\n)\n```\n\n#### 解决方案6：采样倾斜key并分拆join操作\n\n左表/RDD少数key数量大右表/RDD分布均匀时，左边sample算子采样出数量大的n个key，然后将拆分，加上n以内随机数前缀，右边也过滤出这些key也加上n以内前缀膨胀n倍，左右拆分出来的做join，未拆分出来的也做join，然后union结果，就是最终的结果；适用于倾斜key少的时候；\n\n#### 解决方案7：随机前缀和扩容RDD进行join\n\n适用于在join操作时，RDD中有大量key导致数据倾斜，分拆没意义，左边所有都加n以内随机前缀，右边稳定扩容n倍，然后左右join；\n\n#### 解决方案8：多种方案组合使用\n\n针对复杂场景，需要多种方案组合使用，针对多环节数据倾斜spark作业，可以先预处理，其次可以shuffle操作提高并行度，最后针对不同的聚合或join操作，才用两段聚合、随机前缀等方案；\n\n### Shuffle调优\n\n#### HashShuffleManager运行原理\n\n假设前提：每个Executor只分配1个CPU core，也就是说无论这个Executor上分配多少个task线程，同一时间只能执行一个task线程；\n\n##### 未经优化的HashShuffleManager\n\nshuffle write：当前stage的每个task要为下个stage创建多少份磁盘文件？下个stage有多少task就要创建多少！下个stage有100个task，当前stage的每个task就要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，那么每个Executor执行5个task，每个Executor会创建500个磁盘文件；\n\nshuffle read：通常是一个stage刚开始要做的事，该stage中的每个task从上个stage拉取相同key，网络传输到自己所在节点，然后聚合或连接；此时上个stage中的task已经分好了给下个stage每个task的磁盘文件，直接拉取；\n\nshuffle read的拉取过程是一边拉取一边聚合的，每个shuffle read task都有自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的Map进行聚合等操作，聚合完一批再拉下一批；\n\n![hashshufflemanager](./spark-opt/hashshufflemanager.png)\n\n##### 优化后的HashShuffleManager\n\n设置参数spark.shuffle.consolidateFiles=true，默认是false。出现shuffleFileGroup概念，CPU core决定可以并行task数量，每个Executor上磁盘文件数此时取决于CPU core数量*stage的task数量，consolidate机制使不同task可以复用同一批磁盘文件。减少磁盘文件数量，提升shuffle write性能；\n\n![hashshufflemanageropt](./spark-opt/hashshufflemanageropt.png)\n\n\n\n#### SortShuffleManager运行原理\n\nSortShuffleManager分为普通运行机制和bypass运行机制，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数值时(默认200)，就启用bypass机制；\n\n##### 普通运行机制\n\n排序之后分批(1万条)溢写磁盘，写入磁盘是通过java的BufferedOutputStream实现，先缓冲内存，内存满了之后溢写磁盘，每个task最终只有一个磁盘文件，但是会有一个索引文件，标识下游各个task需要数据的start offset和end offset；\n\n![sortshufflemanager](./spark-opt/sortshufflemanager.png)\n\n##### bypass运行机制\n\n触发条件：shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，不是聚合类的shuffle算子(例如reduceByKey)；\n\n此时task会为每个下游task通过hash创建一个临时磁盘文件，类似于HashShuffleManager未优化版本，但是最终会做一个磁盘文件的合并，所以相对来说shuffle read性能会好些；\n\n相较于SortShuffleManager普通机制，磁盘写机制不同，不会进行排序；\n\n![bypass](./spark-opt/bypass.png)\n\n#### spark.shuffle.file.buffer\n\n默认值：32k\n\n参数说明：设置shuffle write task的BufferedOutputStream的buffer缓冲大小，将数据写到磁盘文件之前，会先写到buffer缓冲中，待缓冲写满之后，才会溢写磁盘；\n\n调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如64k)，从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。合理调节，1%-5%性能提升；\n\n#### spark.reducer.maxSizeInFlight\n\n默认值：48M\n\n参数说明：该参数用于设置shuffle read task的buffer缓冲大小，这个buffer缓冲决定了每次拉取多少数据\n\n调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如96m)，从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。合理调节，1%-5%性能提升；\n\n #### spark.shuffle.io.maxRetries\n\n默认值：3\n\n参数说明：shuffle read task从shuffle write task所在节点拉取自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数代表可以重试的最大次数，如果在指定次数之内拉取还没成功，就可能导致作业执行失败。\n\n调优建议：对于那些包含特别耗时的shuffle操作作业，建议增加重试最大次数(比如60次)，以避免JVM的full gc或者网络不稳定等因素导致数据拉取失败。对于超大数据量(数十亿-上百亿)的shuffle过程，调节该参数可以大幅度提升稳定性；\n\n #### spark.shuffle.io.retryWait\n\n默认值：5s\n\n参数说明：代表每次重试拉取数据的等待间隔，默认是5s。\n\n调优建议：建议加大间隔时长，以增加shuffle操作的稳定性。\n\n#### spark.shuffle.memoryFraction\n\n默认值：0.2\n\n参数说明：该参数代表Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。\n\n调优建议：如果内存充足，而且很少使用持久化操作，建议提高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足聚合过程中频繁读写磁盘。实践过程中，合理调节，10%性能提升；\n\n #### spark.shuffle.manager\n\n默认值：sort\n\n参数说明：该参数用于设置ShuffleManager的类型。Spark1.5之后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2之前的默认选项，但是spark 1.2之后版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高；\n\n调优建议：SortShuffleManager默认会对数据进行排序，如果业务逻辑需要该排序机制的话，则使用默认的SortShuffleManager就可以，如果不需要对数据进行排序，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。tungsten-sort慎用～\n\n#### spark.shuffle.sort.bypassMergeThreshold\n\n默认值：200\n\n参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值时，则shuffle write不会进行排序操作，而是按照未经优化的HashShuffleManager的方式写数据，最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件；\n\n调优建议：当使用SortShuffleManager时，如果的确不需要排序操作，调大参数，大于shuffle read task的数量，会自动启用bypass机制，map-side就不会进行排序了，减少了排序性能开销，但是依然会产生大量磁盘文件，因此shuffle write性能有待提高；\n\n #### spark.shuffle.consolidateFiles\n\n默认值：false\n\n参数说明：如果使用了HashShuffleManager，该参数有效，如果设置了true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大减少磁盘IO开销，提升性能；\n\n调优建议：如果的确不需要SortShuffleManager排序机制，除了使用bypass还可以将spark.shuffle.manager设置为hash，使用HashShuffleManager，同时开启consolidate机制。实践表示，性能比开启bypass的SortShuffleManager要高出10%-30%；\n\n### 参考\n\n(1)https://tech.meituan.com/2016/04/29/spark-tuning-basic.html\n\n(2)https://tech.meituan.com/2016/05/12/spark-tuning-pro.html\n\n(3)https://bbs.huaweicloud.com/blogs/325349\n","source":"_posts/spark-opt.md","raw":"---\ntitle: spark-opt\ndate: 2022-12-09 15:55:51\ntags: Spark\n---\n\n> Spark汇总\n\n<!-- more -->\n\n### Spark优化原则\n\n#### 原则1:避免创建重复的RDD\n\n#### 原则2:尽可能复用同一个RDD\n\n#### 原则3:对多次使用的RDD进行持久化\n\ncache() 使用**非序列化**的方法将RDD中的数据全部尝试持久化到内存中；\n\npersist() 选择持久化级别、方式进行持久化。_SER后缀表示序列化后保存，每个Partition会被序列化一个大的字节数组，然后持久化到内存或磁盘，减少数据占用内存过多，避免频繁GC；\n\n| 级别                | 含义                                                         |\n| ------------------- | ------------------------------------------------------------ |\n| MEMORY_ONLY         | 默认，放内存，同cache()，内存不够就不持久化，算子操作时候从源头计算一遍； |\n| MEMORY_AND_RISK     | 内存不够放磁盘，算子操作时从磁盘中取出来用；                 |\n| MEMORY_ONLY_SER     | 同MEMORY_ONLY，区别是序列化，节省内存避免频繁GC；            |\n| MEMORY_AND_DISK_SER | 同MEMORY_ONLY_SER，区别是序列化，节省内存避免频繁GC；        |\n| DISK_ONLY           | 放磁盘；                                                     |\n| *_2                 | 上面的所有策略+_2后缀表示每个持久化数据都有副本放其他节点，容灾； |\n\n#### 原则4:尽量避免使用shuffle类算子\n\nshuffle解释：将分布在集群多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作，比如，reduceByKey、join、distinct、repartition等，应该尽量使用map类操作替代;\n\nshuffle过程：各个节点相同key都会先写磁盘，其他节点通过网络传输拉取各个节点磁盘文件中相同key，节点内存不够溢写磁盘，大量的磁盘IO和网络传输导致shuffle性能较差；\n\n```scala\n// broadcast+map替代join\nval rdd3 = rdd1.join(rdd2)\n// broadcast+map不会产生shuffle 适用于rdd2数据量较少 <2G \nval rdd2Data = rdd2.collect()\nval rdd2DataBroadcast = sc.broadcast(rdd2Data) // 每个Executer都有一份rdd2了\nval rdd3 = rdd1.map(rdd2DataBroadcast...)// 找到相同key然后处理\n```\n\n#### 原则5:使用map-side预聚合的shuffle操作\n\n#### 原则6:使用高性能算子\n\ngroupByKey是直接shuffle，reduceByKey和aggregateByKey先预聚合再shuffle，区别是后者可以指定初始值并且partition内部和之间的聚合操作可以不同，如果相同可以用foldByKey，\n\n```scala\nobject AggByKeyOpt {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName(\"test\").setMaster(\"local\")\n    val sc = new SparkContext(sparkConf)\n    val data = Seq((1,3),(1,2),(1,4),(2,3))\n    val rdd = sc.parallelize(data, 2)\n    //合并不同partition中的值，a，b得数据类型为zeroValue的数据类型\n    def combOp(a:String,b:String):String={\n      println(\"combOp: \"+a+\"\\t\"+b)\n      a+b\n    }\n    //合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型\n    def seqOp(a:String,b:Int):String={\n      println(\"SeqOp:\"+a+\"\\t\"+b)\n      a+b\n    }\n    rdd.foreach(println)\n    val aggregateByKeyRDD=rdd.aggregateByKey(\"100\")(seqOp, combOp)\n    aggregateByKeyRDD.foreach(println)\n    sc.stop()\n  }\n}\n```\n\nmapPartitions代替map，一次函数调用处理一个partition而非单条，容易OOM，如果内存不够垃圾回收无法回收太多对象；\n\nforeachPartitions代替foreach，对于每个partition如果连接mysql连接一次就可以；\n\nfilter之后使用coaleasce操作，将rdd压缩到更少partition使用更少task处理；\n\nrepartitionAndSortWithinPartitions代替repartition与sort操作，shuffle和sort同时进行，性能更高；\n\n#### 原则7:广播大变量\n\n大变量，100M以上，默认情况下会复制多份传输到task中，大量变量副本消耗网络传输，在各个节点Executor中占用过多内存频繁GC，广播的话，每个Executor只保留一份副本，Executor中的task共用；\n\n```scala\nval list1 = ...\nval list1Broadcast = sc.broadcast(list1)\nrdd1.map(list1Broadcast...)\n```\n\n#### 原则8 使用Kryo优化序列化性能\n\nSpark中三个地方涉及到了序列化：\n\n* 算子函数使用外部变量时，变量会被序列化后进行网络传输；\n* 自定义类型作为RDD的范型时，会进行序列化，要求自定义类实现Serializable接口；\n* 使用可序列化的持久化策略时，例如MEMORY_ONLY_SER；\n\nSpark默认使用Java序列化机制，ObjectOutputStream/ObjectInputStream API进行序列化和反序列化，使用Kryo性能提高10倍；\n\n```scala\n// 创建SparkConf对象。\nval conf = new SparkConf().setMaster(...).setAppName(...)\n// 设置序列化器为KryoSerializer。\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n// 注册要序列化的自定义类型。\nconf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))\n```\n\n#### 原则9:优化数据结构\n\n三种数据类型比较耗费内存:1)对象，每个Java对象都有对象头、引用等额外信息，比较占用内存；2)字符串，每个字符串内部都有一个字符数组以及长度等信息；3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry；\n\n官方建议，使用原始类型比如Int、long代替字符串，数组代替集合类型，减少内存占用，降低GC频率，提升性能；\n\n### Spark作业基本运行原理\n\n![spark-submit](./spark-opt/spark-submit.png)\n\nspark-submit提交Spark作业，启动一个对应Driver进程，根据部署模式不同，本地启动或者集群某个工作节点启动。Driver会占用一定数量的内存和CPU core。Driver进程首先向资源管理集群(Yarn等)申请资源，资源指的是Executor进行，资源管理集群会根据资源参数，在各个工作节点上启动一定数量的Executor进程，每个Executor进程会占用一定数量的内存和CPU core；\n\n资源到位，Driver开始将Spark作业拆分多个stage，并为每个stage创建一批task，然后将task分配到Executor执行，task是最小的计算单元，负责执行一摸一样的计算逻辑(代码片段)，stage的所有task执行完毕之后，中间结果写入本地磁盘，Driver开始调度运行下一个stage，输入数据是上一个stage中间结果，直到全部执行；\n\nSpark根据shuffle类算子进行stage划分，每个stage会通过网络传输拉取需要自己处理的所有相同key，然后进行聚合操作，这就是shuffle；\n\ncache/persist会把task计算出来的数据保存到Executor进程的内存或者所在节点的磁盘文件中；\n\nExecutor内存模型：20%执行task逻辑，20%执行shuffle操作，60%执行RDD持久化，详见参考(3)；\n\ntask执行速度和CPU core直接相关，task执行独占CPU core；\n\n#### num-executors\n\nSpark作业总共要用多少Executor进程执行，太少无法充分利用集群资源，太多队列无法给予，一般50-100；\n\n#### executor-memory\n\nExecutor内存大小决定Spark作业性能，太小容易OOM，一般4G-8G；\n\n#### executor-cores\n\n每个Executor进程的CPU core数量，越多执行越快，一般2-4；\n\n#### driver-memory\n\n设置Driver进程内存，如果使用collect算子会将RDD数据拉到Driver上进行处理需要更多内存否则OOM，否则，一般1G；\n\n#### spark.default.parallelism\n\n每个stage默认task数量，默认是根据底层HDFS block(hadoop2.x是128M)数量，通常默认很少(几十)，官网建议num-executors*executor-cores的2-3倍，一般500-1000；\n\n#### spark.storage.memoryFraction\n\nRDD持久化数据在Executor内存占比，默认0.6，如果内存不够就写磁盘，持久化操作多就提高，shuffle类操作多就降低，频繁GC说明执行task逻辑内存不够也降低(通过spark web ui查看gc耗时)；\n\n#### spark.shuffle.memoryFraction\n\nshuffle过程中拉到上个stage输出后，进行聚。合操作能使用的Executor内存比例，默认0.2，超过溢写磁盘，降低性能。如果RDD持久化操作少，shuffle操作多建议提高，如果作业频繁GC导致运行缓慢，意味着task逻辑执行内存不够，建议降低；\n\n```bash\n# 示例\n./bin/spark-submit \\\n  --master yarn-cluster \\\n  --num-executors 100 \\\n  --executor-memory 6G \\\n  --executor-cores 4 \\\n  --driver-memory 1G \\\n  --conf spark.default.parallelism=1000 \\\n  --conf spark.storage.memoryFraction=0.5 \\\n  --conf spark.shuffle.memoryFraction=0.3 \\\n```\n\n### Spark数据倾斜处理\n\n#### 现象\n\n1）个别task执行慢\n\n2）突然OOM异常\n\n#### 原理\n\n进行shuffle时候，某个key对应数据量特别大，spark运行进度是由运行时间最长的task决定；\n\n#### 定位\n\n可能触发shuffle操作算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup(对两个RDD中的KV元素,每个RDD中相同key中的元素分别聚合成一个集合)、repartition等；\n\nSpark Web UI查看stage各个task分配的数据量、运行时间；\n\n推算stage与代码关系：shuffle类算子前后会分两个stage，如下，reduceByKey前后会划分两个stage，stage0执行textFile到map以及shuffle write操作，每个task处理的数据中，相同key会写入同一个磁盘文件；stage1执行reduceByKey到collect操作，首先执行shuffle read操作，会从stage0各个task所在节点拉取需要key，然后对相同key进行聚合或者join操作，在这里是对相同key的value累加，最后执行collect算子，将所有数据拉到Driver上打印输出；\n\n查看key分布：直接通过sparksql查看key分布，如果是RDD文件可以通过RDD.countByKey()之后collect/take打印；\n\n```scala\n// 单词计数\nval conf = new SparkConf()\nval sc = new SparkContext(conf)\nval lines = sc.textFile(\"hdfs://...\")\nval words = lines.flatMap(_.split(\" \"))\nval pairs = words.map((_,1))\nval wordCounts = pairs.reduceByKey(_ + _) // \nwordCounts.collect().foreach(println(_))\n```\n\n#### 解决方案1：使用Hive ETL预处理数据\n\n预先聚合或join，将数据倾斜发生提前，减少spark shuffle时间，治标不治本，还是会发生倾斜；\n\n#### 解决方案2：过滤少数导致倾斜的key\n\n#### 解决方案3：提高shuffle操作的并行度\n\nreduceByKey(1000)，增加shuffle read task并行度，spark.sql.shuffle.partitions就代表该并行度，默认值200，200个task不够，会导致单task分配过多数据，增多可以减少单task执行时间(原来单task分配5个key，增加之后只分1个key)。实现简单，但是效果有限，因为如果单key对应数据太多没法再分；\n\n#### 解决方案4：两阶段聚合(局部聚合+全局聚合)\n\n首先局部聚合，key+随机前缀，然后执行reduceByKey等操作，之后去掉key随机前缀，在进行全局聚合，得到最终结果；适用于聚合类shuffle操作，不适用于join类shuffle操作；\n\n#### 解决方案5：将reduce join转为map join\n\n适用于join类操作时其中一个RDD或表较小(<2G)，使用broadcast变量完全规避shuffle，使用map进行连接；将较小的RDD直接通过collect算子拉取到Driver端内存再分发到Executor；\n\n```java\nList<Tuple2<Long, Row>> rdd1Data = rdd1.collect();\nfinal Broadcast<List<Tuple2<Long, Row>>> rdd1DataBroadcast = sc.broadcast(rdd1Data); // 广播\nJavaPairRDD<String,Tuple2<String,Row>> joinedRdd = rdd2.mapToPair(\n\tnew PairFunction<Tuple2<Long,String>,String,Tuple2<String,Row>>(){\n    private static final long serialVersionUID = 1L;\n    @override\n    public Tuple2<String, Tuple2<String, Row>> call(Tuple2<Long, String> tuple) throw Exception{\n      List<Tuple2<Long, Row>> rdd1Data = rdd1DataBroadcast.value(); // 获取广播变量\n      Map<Long, Row> rdd1DataMap = new HashMap<Long, Row>(); // 广播变量转化为Map\n      for(Tuple2<Long, Row> data: rdd1Data){\n        rdd1DataMap.put(data._1, data._2);\n      }\n      // 获取当前rdd的key和value\n      String key = tuple._1;\n      String value = tuple._2;\n      Row rdd1Value = rdd1DataMap.get(key); // 获取join到数据\n      return new Tuple2<String, String>(key, new Tuple2<String, Row>(value, rdd1Value));\n    }\n  }\n)\n```\n\n#### 解决方案6：采样倾斜key并分拆join操作\n\n左表/RDD少数key数量大右表/RDD分布均匀时，左边sample算子采样出数量大的n个key，然后将拆分，加上n以内随机数前缀，右边也过滤出这些key也加上n以内前缀膨胀n倍，左右拆分出来的做join，未拆分出来的也做join，然后union结果，就是最终的结果；适用于倾斜key少的时候；\n\n#### 解决方案7：随机前缀和扩容RDD进行join\n\n适用于在join操作时，RDD中有大量key导致数据倾斜，分拆没意义，左边所有都加n以内随机前缀，右边稳定扩容n倍，然后左右join；\n\n#### 解决方案8：多种方案组合使用\n\n针对复杂场景，需要多种方案组合使用，针对多环节数据倾斜spark作业，可以先预处理，其次可以shuffle操作提高并行度，最后针对不同的聚合或join操作，才用两段聚合、随机前缀等方案；\n\n### Shuffle调优\n\n#### HashShuffleManager运行原理\n\n假设前提：每个Executor只分配1个CPU core，也就是说无论这个Executor上分配多少个task线程，同一时间只能执行一个task线程；\n\n##### 未经优化的HashShuffleManager\n\nshuffle write：当前stage的每个task要为下个stage创建多少份磁盘文件？下个stage有多少task就要创建多少！下个stage有100个task，当前stage的每个task就要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，那么每个Executor执行5个task，每个Executor会创建500个磁盘文件；\n\nshuffle read：通常是一个stage刚开始要做的事，该stage中的每个task从上个stage拉取相同key，网络传输到自己所在节点，然后聚合或连接；此时上个stage中的task已经分好了给下个stage每个task的磁盘文件，直接拉取；\n\nshuffle read的拉取过程是一边拉取一边聚合的，每个shuffle read task都有自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的Map进行聚合等操作，聚合完一批再拉下一批；\n\n![hashshufflemanager](./spark-opt/hashshufflemanager.png)\n\n##### 优化后的HashShuffleManager\n\n设置参数spark.shuffle.consolidateFiles=true，默认是false。出现shuffleFileGroup概念，CPU core决定可以并行task数量，每个Executor上磁盘文件数此时取决于CPU core数量*stage的task数量，consolidate机制使不同task可以复用同一批磁盘文件。减少磁盘文件数量，提升shuffle write性能；\n\n![hashshufflemanageropt](./spark-opt/hashshufflemanageropt.png)\n\n\n\n#### SortShuffleManager运行原理\n\nSortShuffleManager分为普通运行机制和bypass运行机制，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数值时(默认200)，就启用bypass机制；\n\n##### 普通运行机制\n\n排序之后分批(1万条)溢写磁盘，写入磁盘是通过java的BufferedOutputStream实现，先缓冲内存，内存满了之后溢写磁盘，每个task最终只有一个磁盘文件，但是会有一个索引文件，标识下游各个task需要数据的start offset和end offset；\n\n![sortshufflemanager](./spark-opt/sortshufflemanager.png)\n\n##### bypass运行机制\n\n触发条件：shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，不是聚合类的shuffle算子(例如reduceByKey)；\n\n此时task会为每个下游task通过hash创建一个临时磁盘文件，类似于HashShuffleManager未优化版本，但是最终会做一个磁盘文件的合并，所以相对来说shuffle read性能会好些；\n\n相较于SortShuffleManager普通机制，磁盘写机制不同，不会进行排序；\n\n![bypass](./spark-opt/bypass.png)\n\n#### spark.shuffle.file.buffer\n\n默认值：32k\n\n参数说明：设置shuffle write task的BufferedOutputStream的buffer缓冲大小，将数据写到磁盘文件之前，会先写到buffer缓冲中，待缓冲写满之后，才会溢写磁盘；\n\n调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如64k)，从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。合理调节，1%-5%性能提升；\n\n#### spark.reducer.maxSizeInFlight\n\n默认值：48M\n\n参数说明：该参数用于设置shuffle read task的buffer缓冲大小，这个buffer缓冲决定了每次拉取多少数据\n\n调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如96m)，从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。合理调节，1%-5%性能提升；\n\n #### spark.shuffle.io.maxRetries\n\n默认值：3\n\n参数说明：shuffle read task从shuffle write task所在节点拉取自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数代表可以重试的最大次数，如果在指定次数之内拉取还没成功，就可能导致作业执行失败。\n\n调优建议：对于那些包含特别耗时的shuffle操作作业，建议增加重试最大次数(比如60次)，以避免JVM的full gc或者网络不稳定等因素导致数据拉取失败。对于超大数据量(数十亿-上百亿)的shuffle过程，调节该参数可以大幅度提升稳定性；\n\n #### spark.shuffle.io.retryWait\n\n默认值：5s\n\n参数说明：代表每次重试拉取数据的等待间隔，默认是5s。\n\n调优建议：建议加大间隔时长，以增加shuffle操作的稳定性。\n\n#### spark.shuffle.memoryFraction\n\n默认值：0.2\n\n参数说明：该参数代表Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。\n\n调优建议：如果内存充足，而且很少使用持久化操作，建议提高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足聚合过程中频繁读写磁盘。实践过程中，合理调节，10%性能提升；\n\n #### spark.shuffle.manager\n\n默认值：sort\n\n参数说明：该参数用于设置ShuffleManager的类型。Spark1.5之后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2之前的默认选项，但是spark 1.2之后版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高；\n\n调优建议：SortShuffleManager默认会对数据进行排序，如果业务逻辑需要该排序机制的话，则使用默认的SortShuffleManager就可以，如果不需要对数据进行排序，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。tungsten-sort慎用～\n\n#### spark.shuffle.sort.bypassMergeThreshold\n\n默认值：200\n\n参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值时，则shuffle write不会进行排序操作，而是按照未经优化的HashShuffleManager的方式写数据，最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件；\n\n调优建议：当使用SortShuffleManager时，如果的确不需要排序操作，调大参数，大于shuffle read task的数量，会自动启用bypass机制，map-side就不会进行排序了，减少了排序性能开销，但是依然会产生大量磁盘文件，因此shuffle write性能有待提高；\n\n #### spark.shuffle.consolidateFiles\n\n默认值：false\n\n参数说明：如果使用了HashShuffleManager，该参数有效，如果设置了true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大减少磁盘IO开销，提升性能；\n\n调优建议：如果的确不需要SortShuffleManager排序机制，除了使用bypass还可以将spark.shuffle.manager设置为hash，使用HashShuffleManager，同时开启consolidate机制。实践表示，性能比开启bypass的SortShuffleManager要高出10%-30%；\n\n### 参考\n\n(1)https://tech.meituan.com/2016/04/29/spark-tuning-basic.html\n\n(2)https://tech.meituan.com/2016/05/12/spark-tuning-pro.html\n\n(3)https://bbs.huaweicloud.com/blogs/325349\n","slug":"spark-opt","published":1,"updated":"2022-12-19T01:30:52.684Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbu4ffhd001216gncy4ohc18","content":"<blockquote>\n<p>Spark汇总</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h3 id=\"Spark优化原则\"><a href=\"#Spark优化原则\" class=\"headerlink\" title=\"Spark优化原则\"></a>Spark优化原则</h3><h4 id=\"原则1-避免创建重复的RDD\"><a href=\"#原则1-避免创建重复的RDD\" class=\"headerlink\" title=\"原则1:避免创建重复的RDD\"></a>原则1:避免创建重复的RDD</h4><h4 id=\"原则2-尽可能复用同一个RDD\"><a href=\"#原则2-尽可能复用同一个RDD\" class=\"headerlink\" title=\"原则2:尽可能复用同一个RDD\"></a>原则2:尽可能复用同一个RDD</h4><h4 id=\"原则3-对多次使用的RDD进行持久化\"><a href=\"#原则3-对多次使用的RDD进行持久化\" class=\"headerlink\" title=\"原则3:对多次使用的RDD进行持久化\"></a>原则3:对多次使用的RDD进行持久化</h4><p>cache() 使用<strong>非序列化</strong>的方法将RDD中的数据全部尝试持久化到内存中；</p>\n<p>persist() 选择持久化级别、方式进行持久化。_SER后缀表示序列化后保存，每个Partition会被序列化一个大的字节数组，然后持久化到内存或磁盘，减少数据占用内存过多，避免频繁GC；</p>\n<table>\n<thead>\n<tr>\n<th>级别</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MEMORY_ONLY</td>\n<td>默认，放内存，同cache()，内存不够就不持久化，算子操作时候从源头计算一遍；</td>\n</tr>\n<tr>\n<td>MEMORY_AND_RISK</td>\n<td>内存不够放磁盘，算子操作时从磁盘中取出来用；</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_SER</td>\n<td>同MEMORY_ONLY，区别是序列化，节省内存避免频繁GC；</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK_SER</td>\n<td>同MEMORY_ONLY_SER，区别是序列化，节省内存避免频繁GC；</td>\n</tr>\n<tr>\n<td>DISK_ONLY</td>\n<td>放磁盘；</td>\n</tr>\n<tr>\n<td>*_2</td>\n<td>上面的所有策略+_2后缀表示每个持久化数据都有副本放其他节点，容灾；</td>\n</tr>\n</tbody></table>\n<h4 id=\"原则4-尽量避免使用shuffle类算子\"><a href=\"#原则4-尽量避免使用shuffle类算子\" class=\"headerlink\" title=\"原则4:尽量避免使用shuffle类算子\"></a>原则4:尽量避免使用shuffle类算子</h4><p>shuffle解释：将分布在集群多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作，比如，reduceByKey、join、distinct、repartition等，应该尽量使用map类操作替代;</p>\n<p>shuffle过程：各个节点相同key都会先写磁盘，其他节点通过网络传输拉取各个节点磁盘文件中相同key，节点内存不够溢写磁盘，大量的磁盘IO和网络传输导致shuffle性能较差；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// broadcast+map替代join</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class=\"line\"><span class=\"comment\">// broadcast+map不会产生shuffle 适用于rdd2数据量较少 &lt;2G </span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd2Data = rdd2.collect()</span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data) <span class=\"comment\">// 每个Executer都有一份rdd2了</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)<span class=\"comment\">// 找到相同key然后处理</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则5-使用map-side预聚合的shuffle操作\"><a href=\"#原则5-使用map-side预聚合的shuffle操作\" class=\"headerlink\" title=\"原则5:使用map-side预聚合的shuffle操作\"></a>原则5:使用map-side预聚合的shuffle操作</h4><h4 id=\"原则6-使用高性能算子\"><a href=\"#原则6-使用高性能算子\" class=\"headerlink\" title=\"原则6:使用高性能算子\"></a>原则6:使用高性能算子</h4><p>groupByKey是直接shuffle，reduceByKey和aggregateByKey先预聚合再shuffle，区别是后者可以指定初始值并且partition内部和之间的聚合操作可以不同，如果相同可以用foldByKey，</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">AggByKeyOpt</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">&quot;test&quot;</span>).setMaster(<span class=\"string\">&quot;local&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> data = <span class=\"type\">Seq</span>((<span class=\"number\">1</span>,<span class=\"number\">3</span>),(<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">1</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> rdd = sc.parallelize(data, <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\">//合并不同partition中的值，a，b得数据类型为zeroValue的数据类型</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combOp</span></span>(a:<span class=\"type\">String</span>,b:<span class=\"type\">String</span>):<span class=\"type\">String</span>=&#123;</span><br><span class=\"line\">      println(<span class=\"string\">&quot;combOp: &quot;</span>+a+<span class=\"string\">&quot;\\t&quot;</span>+b)</span><br><span class=\"line\">      a+b</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">seqOp</span></span>(a:<span class=\"type\">String</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">String</span>=&#123;</span><br><span class=\"line\">      println(<span class=\"string\">&quot;SeqOp:&quot;</span>+a+<span class=\"string\">&quot;\\t&quot;</span>+b)</span><br><span class=\"line\">      a+b</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    rdd.foreach(println)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregateByKeyRDD=rdd.aggregateByKey(<span class=\"string\">&quot;100&quot;</span>)(seqOp, combOp)</span><br><span class=\"line\">    aggregateByKeyRDD.foreach(println)</span><br><span class=\"line\">    sc.stop()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>mapPartitions代替map，一次函数调用处理一个partition而非单条，容易OOM，如果内存不够垃圾回收无法回收太多对象；</p>\n<p>foreachPartitions代替foreach，对于每个partition如果连接mysql连接一次就可以；</p>\n<p>filter之后使用coaleasce操作，将rdd压缩到更少partition使用更少task处理；</p>\n<p>repartitionAndSortWithinPartitions代替repartition与sort操作，shuffle和sort同时进行，性能更高；</p>\n<h4 id=\"原则7-广播大变量\"><a href=\"#原则7-广播大变量\" class=\"headerlink\" title=\"原则7:广播大变量\"></a>原则7:广播大变量</h4><p>大变量，100M以上，默认情况下会复制多份传输到task中，大量变量副本消耗网络传输，在各个节点Executor中占用过多内存频繁GC，广播的话，每个Executor只保留一份副本，Executor中的task共用；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> list1 = ...</span><br><span class=\"line\"><span class=\"keyword\">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class=\"line\">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则8-使用Kryo优化序列化性能\"><a href=\"#原则8-使用Kryo优化序列化性能\" class=\"headerlink\" title=\"原则8 使用Kryo优化序列化性能\"></a>原则8 使用Kryo优化序列化性能</h4><p>Spark中三个地方涉及到了序列化：</p>\n<ul>\n<li>算子函数使用外部变量时，变量会被序列化后进行网络传输；</li>\n<li>自定义类型作为RDD的范型时，会进行序列化，要求自定义类实现Serializable接口；</li>\n<li>使用可序列化的持久化策略时，例如MEMORY_ONLY_SER；</li>\n</ul>\n<p>Spark默认使用Java序列化机制，ObjectOutputStream/ObjectInputStream API进行序列化和反序列化，使用Kryo性能提高10倍；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 创建SparkConf对象。</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class=\"line\"><span class=\"comment\">// 设置序列化器为KryoSerializer。</span></span><br><span class=\"line\">conf.set(<span class=\"string\">&quot;spark.serializer&quot;</span>, <span class=\"string\">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">// 注册要序列化的自定义类型。</span></span><br><span class=\"line\">conf.registerKryoClasses(<span class=\"type\">Array</span>(classOf[<span class=\"type\">MyClass1</span>], classOf[<span class=\"type\">MyClass2</span>]))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则9-优化数据结构\"><a href=\"#原则9-优化数据结构\" class=\"headerlink\" title=\"原则9:优化数据结构\"></a>原则9:优化数据结构</h4><p>三种数据类型比较耗费内存:1)对象，每个Java对象都有对象头、引用等额外信息，比较占用内存；2)字符串，每个字符串内部都有一个字符数组以及长度等信息；3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry；</p>\n<p>官方建议，使用原始类型比如Int、long代替字符串，数组代替集合类型，减少内存占用，降低GC频率，提升性能；</p>\n<h3 id=\"Spark作业基本运行原理\"><a href=\"#Spark作业基本运行原理\" class=\"headerlink\" title=\"Spark作业基本运行原理\"></a>Spark作业基本运行原理</h3><p><img src=\"/2022/12/09/spark-opt/spark-submit.png\" alt=\"spark-submit\"></p>\n<p>spark-submit提交Spark作业，启动一个对应Driver进程，根据部署模式不同，本地启动或者集群某个工作节点启动。Driver会占用一定数量的内存和CPU core。Driver进程首先向资源管理集群(Yarn等)申请资源，资源指的是Executor进行，资源管理集群会根据资源参数，在各个工作节点上启动一定数量的Executor进程，每个Executor进程会占用一定数量的内存和CPU core；</p>\n<p>资源到位，Driver开始将Spark作业拆分多个stage，并为每个stage创建一批task，然后将task分配到Executor执行，task是最小的计算单元，负责执行一摸一样的计算逻辑(代码片段)，stage的所有task执行完毕之后，中间结果写入本地磁盘，Driver开始调度运行下一个stage，输入数据是上一个stage中间结果，直到全部执行；</p>\n<p>Spark根据shuffle类算子进行stage划分，每个stage会通过网络传输拉取需要自己处理的所有相同key，然后进行聚合操作，这就是shuffle；</p>\n<p>cache/persist会把task计算出来的数据保存到Executor进程的内存或者所在节点的磁盘文件中；</p>\n<p>Executor内存模型：20%执行task逻辑，20%执行shuffle操作，60%执行RDD持久化，详见参考(3)；</p>\n<p>task执行速度和CPU core直接相关，task执行独占CPU core；</p>\n<h4 id=\"num-executors\"><a href=\"#num-executors\" class=\"headerlink\" title=\"num-executors\"></a>num-executors</h4><p>Spark作业总共要用多少Executor进程执行，太少无法充分利用集群资源，太多队列无法给予，一般50-100；</p>\n<h4 id=\"executor-memory\"><a href=\"#executor-memory\" class=\"headerlink\" title=\"executor-memory\"></a>executor-memory</h4><p>Executor内存大小决定Spark作业性能，太小容易OOM，一般4G-8G；</p>\n<h4 id=\"executor-cores\"><a href=\"#executor-cores\" class=\"headerlink\" title=\"executor-cores\"></a>executor-cores</h4><p>每个Executor进程的CPU core数量，越多执行越快，一般2-4；</p>\n<h4 id=\"driver-memory\"><a href=\"#driver-memory\" class=\"headerlink\" title=\"driver-memory\"></a>driver-memory</h4><p>设置Driver进程内存，如果使用collect算子会将RDD数据拉到Driver上进行处理需要更多内存否则OOM，否则，一般1G；</p>\n<h4 id=\"spark-default-parallelism\"><a href=\"#spark-default-parallelism\" class=\"headerlink\" title=\"spark.default.parallelism\"></a>spark.default.parallelism</h4><p>每个stage默认task数量，默认是根据底层HDFS block(hadoop2.x是128M)数量，通常默认很少(几十)，官网建议num-executors*executor-cores的2-3倍，一般500-1000；</p>\n<h4 id=\"spark-storage-memoryFraction\"><a href=\"#spark-storage-memoryFraction\" class=\"headerlink\" title=\"spark.storage.memoryFraction\"></a>spark.storage.memoryFraction</h4><p>RDD持久化数据在Executor内存占比，默认0.6，如果内存不够就写磁盘，持久化操作多就提高，shuffle类操作多就降低，频繁GC说明执行task逻辑内存不够也降低(通过spark web ui查看gc耗时)；</p>\n<h4 id=\"spark-shuffle-memoryFraction\"><a href=\"#spark-shuffle-memoryFraction\" class=\"headerlink\" title=\"spark.shuffle.memoryFraction\"></a>spark.shuffle.memoryFraction</h4><p>shuffle过程中拉到上个stage输出后，进行聚。合操作能使用的Executor内存比例，默认0.2，超过溢写磁盘，降低性能。如果RDD持久化操作少，shuffle操作多建议提高，如果作业频繁GC导致运行缓慢，意味着task逻辑执行内存不够，建议降低；</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 示例</span></span><br><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --master yarn-cluster \\</span><br><span class=\"line\">  --num-executors 100 \\</span><br><span class=\"line\">  --executor-memory 6G \\</span><br><span class=\"line\">  --executor-cores 4 \\</span><br><span class=\"line\">  --driver-memory 1G \\</span><br><span class=\"line\">  --conf spark.default.parallelism=1000 \\</span><br><span class=\"line\">  --conf spark.storage.memoryFraction=0.5 \\</span><br><span class=\"line\">  --conf spark.shuffle.memoryFraction=0.3 \\</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Spark数据倾斜处理\"><a href=\"#Spark数据倾斜处理\" class=\"headerlink\" title=\"Spark数据倾斜处理\"></a>Spark数据倾斜处理</h3><h4 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h4><p>1）个别task执行慢</p>\n<p>2）突然OOM异常</p>\n<h4 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>进行shuffle时候，某个key对应数据量特别大，spark运行进度是由运行时间最长的task决定；</p>\n<h4 id=\"定位\"><a href=\"#定位\" class=\"headerlink\" title=\"定位\"></a>定位</h4><p>可能触发shuffle操作算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup(对两个RDD中的KV元素,每个RDD中相同key中的元素分别聚合成一个集合)、repartition等；</p>\n<p>Spark Web UI查看stage各个task分配的数据量、运行时间；</p>\n<p>推算stage与代码关系：shuffle类算子前后会分两个stage，如下，reduceByKey前后会划分两个stage，stage0执行textFile到map以及shuffle write操作，每个task处理的数据中，相同key会写入同一个磁盘文件；stage1执行reduceByKey到collect操作，首先执行shuffle read操作，会从stage0各个task所在节点拉取需要key，然后对相同key进行聚合或者join操作，在这里是对相同key的value累加，最后执行collect算子，将所有数据拉到Driver上打印输出；</p>\n<p>查看key分布：直接通过sparksql查看key分布，如果是RDD文件可以通过RDD.countByKey()之后collect/take打印；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 单词计数</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\"><span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</span><br><span class=\"line\"><span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">&quot;hdfs://...&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> words = lines.flatMap(_.split(<span class=\"string\">&quot; &quot;</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs = words.map((_,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts = pairs.reduceByKey(_ + _) <span class=\"comment\">// </span></span><br><span class=\"line\">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"解决方案1：使用Hive-ETL预处理数据\"><a href=\"#解决方案1：使用Hive-ETL预处理数据\" class=\"headerlink\" title=\"解决方案1：使用Hive ETL预处理数据\"></a>解决方案1：使用Hive ETL预处理数据</h4><p>预先聚合或join，将数据倾斜发生提前，减少spark shuffle时间，治标不治本，还是会发生倾斜；</p>\n<h4 id=\"解决方案2：过滤少数导致倾斜的key\"><a href=\"#解决方案2：过滤少数导致倾斜的key\" class=\"headerlink\" title=\"解决方案2：过滤少数导致倾斜的key\"></a>解决方案2：过滤少数导致倾斜的key</h4><h4 id=\"解决方案3：提高shuffle操作的并行度\"><a href=\"#解决方案3：提高shuffle操作的并行度\" class=\"headerlink\" title=\"解决方案3：提高shuffle操作的并行度\"></a>解决方案3：提高shuffle操作的并行度</h4><p>reduceByKey(1000)，增加shuffle read task并行度，spark.sql.shuffle.partitions就代表该并行度，默认值200，200个task不够，会导致单task分配过多数据，增多可以减少单task执行时间(原来单task分配5个key，增加之后只分1个key)。实现简单，但是效果有限，因为如果单key对应数据太多没法再分；</p>\n<h4 id=\"解决方案4：两阶段聚合-局部聚合-全局聚合\"><a href=\"#解决方案4：两阶段聚合-局部聚合-全局聚合\" class=\"headerlink\" title=\"解决方案4：两阶段聚合(局部聚合+全局聚合)\"></a>解决方案4：两阶段聚合(局部聚合+全局聚合)</h4><p>首先局部聚合，key+随机前缀，然后执行reduceByKey等操作，之后去掉key随机前缀，在进行全局聚合，得到最终结果；适用于聚合类shuffle操作，不适用于join类shuffle操作；</p>\n<h4 id=\"解决方案5：将reduce-join转为map-join\"><a href=\"#解决方案5：将reduce-join转为map-join\" class=\"headerlink\" title=\"解决方案5：将reduce join转为map join\"></a>解决方案5：将reduce join转为map join</h4><p>适用于join类操作时其中一个RDD或表较小(&lt;2G)，使用broadcast变量完全规避shuffle，使用map进行连接；将较小的RDD直接通过collect算子拉取到Driver端内存再分发到Executor；</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect();</span><br><span class=\"line\"><span class=\"keyword\">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data); <span class=\"comment\">// 广播</span></span><br><span class=\"line\">JavaPairRDD&lt;String,Tuple2&lt;String,Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;,String,Tuple2&lt;String,Row&gt;&gt;()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">1L</span>;</span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) <span class=\"keyword\">throw</span> Exception&#123;</span><br><span class=\"line\">      List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); <span class=\"comment\">// 获取广播变量</span></span><br><span class=\"line\">      Map&lt;Long, Row&gt; rdd1DataMap = <span class=\"keyword\">new</span> HashMap&lt;Long, Row&gt;(); <span class=\"comment\">// 广播变量转化为Map</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span>(Tuple2&lt;Long, Row&gt; data: rdd1Data)&#123;</span><br><span class=\"line\">        rdd1DataMap.put(data._1, data._2);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">// 获取当前rdd的key和value</span></span><br><span class=\"line\">      String key = tuple._1;</span><br><span class=\"line\">      String value = tuple._2;</span><br><span class=\"line\">      Row rdd1Value = rdd1DataMap.get(key); <span class=\"comment\">// 获取join到数据</span></span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;String, String&gt;(key, <span class=\"keyword\">new</span> Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"解决方案6：采样倾斜key并分拆join操作\"><a href=\"#解决方案6：采样倾斜key并分拆join操作\" class=\"headerlink\" title=\"解决方案6：采样倾斜key并分拆join操作\"></a>解决方案6：采样倾斜key并分拆join操作</h4><p>左表/RDD少数key数量大右表/RDD分布均匀时，左边sample算子采样出数量大的n个key，然后将拆分，加上n以内随机数前缀，右边也过滤出这些key也加上n以内前缀膨胀n倍，左右拆分出来的做join，未拆分出来的也做join，然后union结果，就是最终的结果；适用于倾斜key少的时候；</p>\n<h4 id=\"解决方案7：随机前缀和扩容RDD进行join\"><a href=\"#解决方案7：随机前缀和扩容RDD进行join\" class=\"headerlink\" title=\"解决方案7：随机前缀和扩容RDD进行join\"></a>解决方案7：随机前缀和扩容RDD进行join</h4><p>适用于在join操作时，RDD中有大量key导致数据倾斜，分拆没意义，左边所有都加n以内随机前缀，右边稳定扩容n倍，然后左右join；</p>\n<h4 id=\"解决方案8：多种方案组合使用\"><a href=\"#解决方案8：多种方案组合使用\" class=\"headerlink\" title=\"解决方案8：多种方案组合使用\"></a>解决方案8：多种方案组合使用</h4><p>针对复杂场景，需要多种方案组合使用，针对多环节数据倾斜spark作业，可以先预处理，其次可以shuffle操作提高并行度，最后针对不同的聚合或join操作，才用两段聚合、随机前缀等方案；</p>\n<h3 id=\"Shuffle调优\"><a href=\"#Shuffle调优\" class=\"headerlink\" title=\"Shuffle调优\"></a>Shuffle调优</h3><h4 id=\"HashShuffleManager运行原理\"><a href=\"#HashShuffleManager运行原理\" class=\"headerlink\" title=\"HashShuffleManager运行原理\"></a>HashShuffleManager运行原理</h4><p>假设前提：每个Executor只分配1个CPU core，也就是说无论这个Executor上分配多少个task线程，同一时间只能执行一个task线程；</p>\n<h5 id=\"未经优化的HashShuffleManager\"><a href=\"#未经优化的HashShuffleManager\" class=\"headerlink\" title=\"未经优化的HashShuffleManager\"></a>未经优化的HashShuffleManager</h5><p>shuffle write：当前stage的每个task要为下个stage创建多少份磁盘文件？下个stage有多少task就要创建多少！下个stage有100个task，当前stage的每个task就要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，那么每个Executor执行5个task，每个Executor会创建500个磁盘文件；</p>\n<p>shuffle read：通常是一个stage刚开始要做的事，该stage中的每个task从上个stage拉取相同key，网络传输到自己所在节点，然后聚合或连接；此时上个stage中的task已经分好了给下个stage每个task的磁盘文件，直接拉取；</p>\n<p>shuffle read的拉取过程是一边拉取一边聚合的，每个shuffle read task都有自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的Map进行聚合等操作，聚合完一批再拉下一批；</p>\n<p><img src=\"/2022/12/09/spark-opt/hashshufflemanager.png\" alt=\"hashshufflemanager\"></p>\n<h5 id=\"优化后的HashShuffleManager\"><a href=\"#优化后的HashShuffleManager\" class=\"headerlink\" title=\"优化后的HashShuffleManager\"></a>优化后的HashShuffleManager</h5><p>设置参数spark.shuffle.consolidateFiles=true，默认是false。出现shuffleFileGroup概念，CPU core决定可以并行task数量，每个Executor上磁盘文件数此时取决于CPU core数量*stage的task数量，consolidate机制使不同task可以复用同一批磁盘文件。减少磁盘文件数量，提升shuffle write性能；</p>\n<p><img src=\"/2022/12/09/spark-opt/hashshufflemanageropt.png\" alt=\"hashshufflemanageropt\"></p>\n<h4 id=\"SortShuffleManager运行原理\"><a href=\"#SortShuffleManager运行原理\" class=\"headerlink\" title=\"SortShuffleManager运行原理\"></a>SortShuffleManager运行原理</h4><p>SortShuffleManager分为普通运行机制和bypass运行机制，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数值时(默认200)，就启用bypass机制；</p>\n<h5 id=\"普通运行机制\"><a href=\"#普通运行机制\" class=\"headerlink\" title=\"普通运行机制\"></a>普通运行机制</h5><p>排序之后分批(1万条)溢写磁盘，写入磁盘是通过java的BufferedOutputStream实现，先缓冲内存，内存满了之后溢写磁盘，每个task最终只有一个磁盘文件，但是会有一个索引文件，标识下游各个task需要数据的start offset和end offset；</p>\n<p><img src=\"/2022/12/09/spark-opt/sortshufflemanager.png\" alt=\"sortshufflemanager\"></p>\n<h5 id=\"bypass运行机制\"><a href=\"#bypass运行机制\" class=\"headerlink\" title=\"bypass运行机制\"></a>bypass运行机制</h5><p>触发条件：shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，不是聚合类的shuffle算子(例如reduceByKey)；</p>\n<p>此时task会为每个下游task通过hash创建一个临时磁盘文件，类似于HashShuffleManager未优化版本，但是最终会做一个磁盘文件的合并，所以相对来说shuffle read性能会好些；</p>\n<p>相较于SortShuffleManager普通机制，磁盘写机制不同，不会进行排序；</p>\n<p><img src=\"/2022/12/09/spark-opt/bypass.png\" alt=\"bypass\"></p>\n<h4 id=\"spark-shuffle-file-buffer\"><a href=\"#spark-shuffle-file-buffer\" class=\"headerlink\" title=\"spark.shuffle.file.buffer\"></a>spark.shuffle.file.buffer</h4><p>默认值：32k</p>\n<p>参数说明：设置shuffle write task的BufferedOutputStream的buffer缓冲大小，将数据写到磁盘文件之前，会先写到buffer缓冲中，待缓冲写满之后，才会溢写磁盘；</p>\n<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如64k)，从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。合理调节，1%-5%性能提升；</p>\n<h4 id=\"spark-reducer-maxSizeInFlight\"><a href=\"#spark-reducer-maxSizeInFlight\" class=\"headerlink\" title=\"spark.reducer.maxSizeInFlight\"></a>spark.reducer.maxSizeInFlight</h4><p>默认值：48M</p>\n<p>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，这个buffer缓冲决定了每次拉取多少数据</p>\n<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如96m)，从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。合理调节，1%-5%性能提升；</p>\n<h4 id=\"spark-shuffle-io-maxRetries\"><a href=\"#spark-shuffle-io-maxRetries\" class=\"headerlink\" title=\"spark.shuffle.io.maxRetries\"></a>spark.shuffle.io.maxRetries</h4><p>默认值：3</p>\n<p>参数说明：shuffle read task从shuffle write task所在节点拉取自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数代表可以重试的最大次数，如果在指定次数之内拉取还没成功，就可能导致作业执行失败。</p>\n<p>调优建议：对于那些包含特别耗时的shuffle操作作业，建议增加重试最大次数(比如60次)，以避免JVM的full gc或者网络不稳定等因素导致数据拉取失败。对于超大数据量(数十亿-上百亿)的shuffle过程，调节该参数可以大幅度提升稳定性；</p>\n<h4 id=\"spark-shuffle-io-retryWait\"><a href=\"#spark-shuffle-io-retryWait\" class=\"headerlink\" title=\"spark.shuffle.io.retryWait\"></a>spark.shuffle.io.retryWait</h4><p>默认值：5s</p>\n<p>参数说明：代表每次重试拉取数据的等待间隔，默认是5s。</p>\n<p>调优建议：建议加大间隔时长，以增加shuffle操作的稳定性。</p>\n<h4 id=\"spark-shuffle-memoryFraction-1\"><a href=\"#spark-shuffle-memoryFraction-1\" class=\"headerlink\" title=\"spark.shuffle.memoryFraction\"></a>spark.shuffle.memoryFraction</h4><p>默认值：0.2</p>\n<p>参数说明：该参数代表Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</p>\n<p>调优建议：如果内存充足，而且很少使用持久化操作，建议提高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足聚合过程中频繁读写磁盘。实践过程中，合理调节，10%性能提升；</p>\n<h4 id=\"spark-shuffle-manager\"><a href=\"#spark-shuffle-manager\" class=\"headerlink\" title=\"spark.shuffle.manager\"></a>spark.shuffle.manager</h4><p>默认值：sort</p>\n<p>参数说明：该参数用于设置ShuffleManager的类型。Spark1.5之后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2之前的默认选项，但是spark 1.2之后版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高；</p>\n<p>调优建议：SortShuffleManager默认会对数据进行排序，如果业务逻辑需要该排序机制的话，则使用默认的SortShuffleManager就可以，如果不需要对数据进行排序，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。tungsten-sort慎用～</p>\n<h4 id=\"spark-shuffle-sort-bypassMergeThreshold\"><a href=\"#spark-shuffle-sort-bypassMergeThreshold\" class=\"headerlink\" title=\"spark.shuffle.sort.bypassMergeThreshold\"></a>spark.shuffle.sort.bypassMergeThreshold</h4><p>默认值：200</p>\n<p>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值时，则shuffle write不会进行排序操作，而是按照未经优化的HashShuffleManager的方式写数据，最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件；</p>\n<p>调优建议：当使用SortShuffleManager时，如果的确不需要排序操作，调大参数，大于shuffle read task的数量，会自动启用bypass机制，map-side就不会进行排序了，减少了排序性能开销，但是依然会产生大量磁盘文件，因此shuffle write性能有待提高；</p>\n<h4 id=\"spark-shuffle-consolidateFiles\"><a href=\"#spark-shuffle-consolidateFiles\" class=\"headerlink\" title=\"spark.shuffle.consolidateFiles\"></a>spark.shuffle.consolidateFiles</h4><p>默认值：false</p>\n<p>参数说明：如果使用了HashShuffleManager，该参数有效，如果设置了true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大减少磁盘IO开销，提升性能；</p>\n<p>调优建议：如果的确不需要SortShuffleManager排序机制，除了使用bypass还可以将spark.shuffle.manager设置为hash，使用HashShuffleManager，同时开启consolidate机制。实践表示，性能比开启bypass的SortShuffleManager要高出10%-30%；</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>(1)<a href=\"https://tech.meituan.com/2016/04/29/spark-tuning-basic.html\">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a></p>\n<p>(2)<a href=\"https://tech.meituan.com/2016/05/12/spark-tuning-pro.html\">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>\n<p>(3)<a href=\"https://bbs.huaweicloud.com/blogs/325349\">https://bbs.huaweicloud.com/blogs/325349</a></p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>Spark汇总</p>\n</blockquote>","more":"<h3 id=\"Spark优化原则\"><a href=\"#Spark优化原则\" class=\"headerlink\" title=\"Spark优化原则\"></a>Spark优化原则</h3><h4 id=\"原则1-避免创建重复的RDD\"><a href=\"#原则1-避免创建重复的RDD\" class=\"headerlink\" title=\"原则1:避免创建重复的RDD\"></a>原则1:避免创建重复的RDD</h4><h4 id=\"原则2-尽可能复用同一个RDD\"><a href=\"#原则2-尽可能复用同一个RDD\" class=\"headerlink\" title=\"原则2:尽可能复用同一个RDD\"></a>原则2:尽可能复用同一个RDD</h4><h4 id=\"原则3-对多次使用的RDD进行持久化\"><a href=\"#原则3-对多次使用的RDD进行持久化\" class=\"headerlink\" title=\"原则3:对多次使用的RDD进行持久化\"></a>原则3:对多次使用的RDD进行持久化</h4><p>cache() 使用<strong>非序列化</strong>的方法将RDD中的数据全部尝试持久化到内存中；</p>\n<p>persist() 选择持久化级别、方式进行持久化。_SER后缀表示序列化后保存，每个Partition会被序列化一个大的字节数组，然后持久化到内存或磁盘，减少数据占用内存过多，避免频繁GC；</p>\n<table>\n<thead>\n<tr>\n<th>级别</th>\n<th>含义</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MEMORY_ONLY</td>\n<td>默认，放内存，同cache()，内存不够就不持久化，算子操作时候从源头计算一遍；</td>\n</tr>\n<tr>\n<td>MEMORY_AND_RISK</td>\n<td>内存不够放磁盘，算子操作时从磁盘中取出来用；</td>\n</tr>\n<tr>\n<td>MEMORY_ONLY_SER</td>\n<td>同MEMORY_ONLY，区别是序列化，节省内存避免频繁GC；</td>\n</tr>\n<tr>\n<td>MEMORY_AND_DISK_SER</td>\n<td>同MEMORY_ONLY_SER，区别是序列化，节省内存避免频繁GC；</td>\n</tr>\n<tr>\n<td>DISK_ONLY</td>\n<td>放磁盘；</td>\n</tr>\n<tr>\n<td>*_2</td>\n<td>上面的所有策略+_2后缀表示每个持久化数据都有副本放其他节点，容灾；</td>\n</tr>\n</tbody></table>\n<h4 id=\"原则4-尽量避免使用shuffle类算子\"><a href=\"#原则4-尽量避免使用shuffle类算子\" class=\"headerlink\" title=\"原则4:尽量避免使用shuffle类算子\"></a>原则4:尽量避免使用shuffle类算子</h4><p>shuffle解释：将分布在集群多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作，比如，reduceByKey、join、distinct、repartition等，应该尽量使用map类操作替代;</p>\n<p>shuffle过程：各个节点相同key都会先写磁盘，其他节点通过网络传输拉取各个节点磁盘文件中相同key，节点内存不够溢写磁盘，大量的磁盘IO和网络传输导致shuffle性能较差；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// broadcast+map替代join</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class=\"line\"><span class=\"comment\">// broadcast+map不会产生shuffle 适用于rdd2数据量较少 &lt;2G </span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd2Data = rdd2.collect()</span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data) <span class=\"comment\">// 每个Executer都有一份rdd2了</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)<span class=\"comment\">// 找到相同key然后处理</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则5-使用map-side预聚合的shuffle操作\"><a href=\"#原则5-使用map-side预聚合的shuffle操作\" class=\"headerlink\" title=\"原则5:使用map-side预聚合的shuffle操作\"></a>原则5:使用map-side预聚合的shuffle操作</h4><h4 id=\"原则6-使用高性能算子\"><a href=\"#原则6-使用高性能算子\" class=\"headerlink\" title=\"原则6:使用高性能算子\"></a>原则6:使用高性能算子</h4><p>groupByKey是直接shuffle，reduceByKey和aggregateByKey先预聚合再shuffle，区别是后者可以指定初始值并且partition内部和之间的聚合操作可以不同，如果相同可以用foldByKey，</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">AggByKeyOpt</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setAppName(<span class=\"string\">&quot;test&quot;</span>).setMaster(<span class=\"string\">&quot;local&quot;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> data = <span class=\"type\">Seq</span>((<span class=\"number\">1</span>,<span class=\"number\">3</span>),(<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">1</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</span><br><span class=\"line\">    <span class=\"keyword\">val</span> rdd = sc.parallelize(data, <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"comment\">//合并不同partition中的值，a，b得数据类型为zeroValue的数据类型</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combOp</span></span>(a:<span class=\"type\">String</span>,b:<span class=\"type\">String</span>):<span class=\"type\">String</span>=&#123;</span><br><span class=\"line\">      println(<span class=\"string\">&quot;combOp: &quot;</span>+a+<span class=\"string\">&quot;\\t&quot;</span>+b)</span><br><span class=\"line\">      a+b</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">//合并在同一个partition中的值，a的数据类型为zeroValue的数据类型，b的数据类型为原value的数据类型</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">seqOp</span></span>(a:<span class=\"type\">String</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">String</span>=&#123;</span><br><span class=\"line\">      println(<span class=\"string\">&quot;SeqOp:&quot;</span>+a+<span class=\"string\">&quot;\\t&quot;</span>+b)</span><br><span class=\"line\">      a+b</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    rdd.foreach(println)</span><br><span class=\"line\">    <span class=\"keyword\">val</span> aggregateByKeyRDD=rdd.aggregateByKey(<span class=\"string\">&quot;100&quot;</span>)(seqOp, combOp)</span><br><span class=\"line\">    aggregateByKeyRDD.foreach(println)</span><br><span class=\"line\">    sc.stop()</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>mapPartitions代替map，一次函数调用处理一个partition而非单条，容易OOM，如果内存不够垃圾回收无法回收太多对象；</p>\n<p>foreachPartitions代替foreach，对于每个partition如果连接mysql连接一次就可以；</p>\n<p>filter之后使用coaleasce操作，将rdd压缩到更少partition使用更少task处理；</p>\n<p>repartitionAndSortWithinPartitions代替repartition与sort操作，shuffle和sort同时进行，性能更高；</p>\n<h4 id=\"原则7-广播大变量\"><a href=\"#原则7-广播大变量\" class=\"headerlink\" title=\"原则7:广播大变量\"></a>原则7:广播大变量</h4><p>大变量，100M以上，默认情况下会复制多份传输到task中，大量变量副本消耗网络传输，在各个节点Executor中占用过多内存频繁GC，广播的话，每个Executor只保留一份副本，Executor中的task共用；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> list1 = ...</span><br><span class=\"line\"><span class=\"keyword\">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class=\"line\">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则8-使用Kryo优化序列化性能\"><a href=\"#原则8-使用Kryo优化序列化性能\" class=\"headerlink\" title=\"原则8 使用Kryo优化序列化性能\"></a>原则8 使用Kryo优化序列化性能</h4><p>Spark中三个地方涉及到了序列化：</p>\n<ul>\n<li>算子函数使用外部变量时，变量会被序列化后进行网络传输；</li>\n<li>自定义类型作为RDD的范型时，会进行序列化，要求自定义类实现Serializable接口；</li>\n<li>使用可序列化的持久化策略时，例如MEMORY_ONLY_SER；</li>\n</ul>\n<p>Spark默认使用Java序列化机制，ObjectOutputStream/ObjectInputStream API进行序列化和反序列化，使用Kryo性能提高10倍；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 创建SparkConf对象。</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class=\"line\"><span class=\"comment\">// 设置序列化器为KryoSerializer。</span></span><br><span class=\"line\">conf.set(<span class=\"string\">&quot;spark.serializer&quot;</span>, <span class=\"string\">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\">// 注册要序列化的自定义类型。</span></span><br><span class=\"line\">conf.registerKryoClasses(<span class=\"type\">Array</span>(classOf[<span class=\"type\">MyClass1</span>], classOf[<span class=\"type\">MyClass2</span>]))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"原则9-优化数据结构\"><a href=\"#原则9-优化数据结构\" class=\"headerlink\" title=\"原则9:优化数据结构\"></a>原则9:优化数据结构</h4><p>三种数据类型比较耗费内存:1)对象，每个Java对象都有对象头、引用等额外信息，比较占用内存；2)字符串，每个字符串内部都有一个字符数组以及长度等信息；3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry；</p>\n<p>官方建议，使用原始类型比如Int、long代替字符串，数组代替集合类型，减少内存占用，降低GC频率，提升性能；</p>\n<h3 id=\"Spark作业基本运行原理\"><a href=\"#Spark作业基本运行原理\" class=\"headerlink\" title=\"Spark作业基本运行原理\"></a>Spark作业基本运行原理</h3><p><img src=\"/2022/12/09/spark-opt/spark-submit.png\" alt=\"spark-submit\"></p>\n<p>spark-submit提交Spark作业，启动一个对应Driver进程，根据部署模式不同，本地启动或者集群某个工作节点启动。Driver会占用一定数量的内存和CPU core。Driver进程首先向资源管理集群(Yarn等)申请资源，资源指的是Executor进行，资源管理集群会根据资源参数，在各个工作节点上启动一定数量的Executor进程，每个Executor进程会占用一定数量的内存和CPU core；</p>\n<p>资源到位，Driver开始将Spark作业拆分多个stage，并为每个stage创建一批task，然后将task分配到Executor执行，task是最小的计算单元，负责执行一摸一样的计算逻辑(代码片段)，stage的所有task执行完毕之后，中间结果写入本地磁盘，Driver开始调度运行下一个stage，输入数据是上一个stage中间结果，直到全部执行；</p>\n<p>Spark根据shuffle类算子进行stage划分，每个stage会通过网络传输拉取需要自己处理的所有相同key，然后进行聚合操作，这就是shuffle；</p>\n<p>cache/persist会把task计算出来的数据保存到Executor进程的内存或者所在节点的磁盘文件中；</p>\n<p>Executor内存模型：20%执行task逻辑，20%执行shuffle操作，60%执行RDD持久化，详见参考(3)；</p>\n<p>task执行速度和CPU core直接相关，task执行独占CPU core；</p>\n<h4 id=\"num-executors\"><a href=\"#num-executors\" class=\"headerlink\" title=\"num-executors\"></a>num-executors</h4><p>Spark作业总共要用多少Executor进程执行，太少无法充分利用集群资源，太多队列无法给予，一般50-100；</p>\n<h4 id=\"executor-memory\"><a href=\"#executor-memory\" class=\"headerlink\" title=\"executor-memory\"></a>executor-memory</h4><p>Executor内存大小决定Spark作业性能，太小容易OOM，一般4G-8G；</p>\n<h4 id=\"executor-cores\"><a href=\"#executor-cores\" class=\"headerlink\" title=\"executor-cores\"></a>executor-cores</h4><p>每个Executor进程的CPU core数量，越多执行越快，一般2-4；</p>\n<h4 id=\"driver-memory\"><a href=\"#driver-memory\" class=\"headerlink\" title=\"driver-memory\"></a>driver-memory</h4><p>设置Driver进程内存，如果使用collect算子会将RDD数据拉到Driver上进行处理需要更多内存否则OOM，否则，一般1G；</p>\n<h4 id=\"spark-default-parallelism\"><a href=\"#spark-default-parallelism\" class=\"headerlink\" title=\"spark.default.parallelism\"></a>spark.default.parallelism</h4><p>每个stage默认task数量，默认是根据底层HDFS block(hadoop2.x是128M)数量，通常默认很少(几十)，官网建议num-executors*executor-cores的2-3倍，一般500-1000；</p>\n<h4 id=\"spark-storage-memoryFraction\"><a href=\"#spark-storage-memoryFraction\" class=\"headerlink\" title=\"spark.storage.memoryFraction\"></a>spark.storage.memoryFraction</h4><p>RDD持久化数据在Executor内存占比，默认0.6，如果内存不够就写磁盘，持久化操作多就提高，shuffle类操作多就降低，频繁GC说明执行task逻辑内存不够也降低(通过spark web ui查看gc耗时)；</p>\n<h4 id=\"spark-shuffle-memoryFraction\"><a href=\"#spark-shuffle-memoryFraction\" class=\"headerlink\" title=\"spark.shuffle.memoryFraction\"></a>spark.shuffle.memoryFraction</h4><p>shuffle过程中拉到上个stage输出后，进行聚。合操作能使用的Executor内存比例，默认0.2，超过溢写磁盘，降低性能。如果RDD持久化操作少，shuffle操作多建议提高，如果作业频繁GC导致运行缓慢，意味着task逻辑执行内存不够，建议降低；</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 示例</span></span><br><span class=\"line\">./bin/spark-submit \\</span><br><span class=\"line\">  --master yarn-cluster \\</span><br><span class=\"line\">  --num-executors 100 \\</span><br><span class=\"line\">  --executor-memory 6G \\</span><br><span class=\"line\">  --executor-cores 4 \\</span><br><span class=\"line\">  --driver-memory 1G \\</span><br><span class=\"line\">  --conf spark.default.parallelism=1000 \\</span><br><span class=\"line\">  --conf spark.storage.memoryFraction=0.5 \\</span><br><span class=\"line\">  --conf spark.shuffle.memoryFraction=0.3 \\</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Spark数据倾斜处理\"><a href=\"#Spark数据倾斜处理\" class=\"headerlink\" title=\"Spark数据倾斜处理\"></a>Spark数据倾斜处理</h3><h4 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h4><p>1）个别task执行慢</p>\n<p>2）突然OOM异常</p>\n<h4 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>进行shuffle时候，某个key对应数据量特别大，spark运行进度是由运行时间最长的task决定；</p>\n<h4 id=\"定位\"><a href=\"#定位\" class=\"headerlink\" title=\"定位\"></a>定位</h4><p>可能触发shuffle操作算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup(对两个RDD中的KV元素,每个RDD中相同key中的元素分别聚合成一个集合)、repartition等；</p>\n<p>Spark Web UI查看stage各个task分配的数据量、运行时间；</p>\n<p>推算stage与代码关系：shuffle类算子前后会分两个stage，如下，reduceByKey前后会划分两个stage，stage0执行textFile到map以及shuffle write操作，每个task处理的数据中，相同key会写入同一个磁盘文件；stage1执行reduceByKey到collect操作，首先执行shuffle read操作，会从stage0各个task所在节点拉取需要key，然后对相同key进行聚合或者join操作，在这里是对相同key的value累加，最后执行collect算子，将所有数据拉到Driver上打印输出；</p>\n<p>查看key分布：直接通过sparksql查看key分布，如果是RDD文件可以通过RDD.countByKey()之后collect/take打印；</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 单词计数</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> conf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\"><span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(conf)</span><br><span class=\"line\"><span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">&quot;hdfs://...&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> words = lines.flatMap(_.split(<span class=\"string\">&quot; &quot;</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> pairs = words.map((_,<span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"keyword\">val</span> wordCounts = pairs.reduceByKey(_ + _) <span class=\"comment\">// </span></span><br><span class=\"line\">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"解决方案1：使用Hive-ETL预处理数据\"><a href=\"#解决方案1：使用Hive-ETL预处理数据\" class=\"headerlink\" title=\"解决方案1：使用Hive ETL预处理数据\"></a>解决方案1：使用Hive ETL预处理数据</h4><p>预先聚合或join，将数据倾斜发生提前，减少spark shuffle时间，治标不治本，还是会发生倾斜；</p>\n<h4 id=\"解决方案2：过滤少数导致倾斜的key\"><a href=\"#解决方案2：过滤少数导致倾斜的key\" class=\"headerlink\" title=\"解决方案2：过滤少数导致倾斜的key\"></a>解决方案2：过滤少数导致倾斜的key</h4><h4 id=\"解决方案3：提高shuffle操作的并行度\"><a href=\"#解决方案3：提高shuffle操作的并行度\" class=\"headerlink\" title=\"解决方案3：提高shuffle操作的并行度\"></a>解决方案3：提高shuffle操作的并行度</h4><p>reduceByKey(1000)，增加shuffle read task并行度，spark.sql.shuffle.partitions就代表该并行度，默认值200，200个task不够，会导致单task分配过多数据，增多可以减少单task执行时间(原来单task分配5个key，增加之后只分1个key)。实现简单，但是效果有限，因为如果单key对应数据太多没法再分；</p>\n<h4 id=\"解决方案4：两阶段聚合-局部聚合-全局聚合\"><a href=\"#解决方案4：两阶段聚合-局部聚合-全局聚合\" class=\"headerlink\" title=\"解决方案4：两阶段聚合(局部聚合+全局聚合)\"></a>解决方案4：两阶段聚合(局部聚合+全局聚合)</h4><p>首先局部聚合，key+随机前缀，然后执行reduceByKey等操作，之后去掉key随机前缀，在进行全局聚合，得到最终结果；适用于聚合类shuffle操作，不适用于join类shuffle操作；</p>\n<h4 id=\"解决方案5：将reduce-join转为map-join\"><a href=\"#解决方案5：将reduce-join转为map-join\" class=\"headerlink\" title=\"解决方案5：将reduce join转为map join\"></a>解决方案5：将reduce join转为map join</h4><p>适用于join类操作时其中一个RDD或表较小(&lt;2G)，使用broadcast变量完全规避shuffle，使用map进行连接；将较小的RDD直接通过collect算子拉取到Driver端内存再分发到Executor；</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect();</span><br><span class=\"line\"><span class=\"keyword\">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data); <span class=\"comment\">// 广播</span></span><br><span class=\"line\">JavaPairRDD&lt;String,Tuple2&lt;String,Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class=\"line\">\t<span class=\"keyword\">new</span> PairFunction&lt;Tuple2&lt;Long,String&gt;,String,Tuple2&lt;String,Row&gt;&gt;()&#123;</span><br><span class=\"line\">    <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"keyword\">long</span> serialVersionUID = <span class=\"number\">1L</span>;</span><br><span class=\"line\">    <span class=\"meta\">@override</span></span><br><span class=\"line\">    <span class=\"keyword\">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) <span class=\"keyword\">throw</span> Exception&#123;</span><br><span class=\"line\">      List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); <span class=\"comment\">// 获取广播变量</span></span><br><span class=\"line\">      Map&lt;Long, Row&gt; rdd1DataMap = <span class=\"keyword\">new</span> HashMap&lt;Long, Row&gt;(); <span class=\"comment\">// 广播变量转化为Map</span></span><br><span class=\"line\">      <span class=\"keyword\">for</span>(Tuple2&lt;Long, Row&gt; data: rdd1Data)&#123;</span><br><span class=\"line\">        rdd1DataMap.put(data._1, data._2);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      <span class=\"comment\">// 获取当前rdd的key和value</span></span><br><span class=\"line\">      String key = tuple._1;</span><br><span class=\"line\">      String value = tuple._2;</span><br><span class=\"line\">      Row rdd1Value = rdd1DataMap.get(key); <span class=\"comment\">// 获取join到数据</span></span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;String, String&gt;(key, <span class=\"keyword\">new</span> Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"解决方案6：采样倾斜key并分拆join操作\"><a href=\"#解决方案6：采样倾斜key并分拆join操作\" class=\"headerlink\" title=\"解决方案6：采样倾斜key并分拆join操作\"></a>解决方案6：采样倾斜key并分拆join操作</h4><p>左表/RDD少数key数量大右表/RDD分布均匀时，左边sample算子采样出数量大的n个key，然后将拆分，加上n以内随机数前缀，右边也过滤出这些key也加上n以内前缀膨胀n倍，左右拆分出来的做join，未拆分出来的也做join，然后union结果，就是最终的结果；适用于倾斜key少的时候；</p>\n<h4 id=\"解决方案7：随机前缀和扩容RDD进行join\"><a href=\"#解决方案7：随机前缀和扩容RDD进行join\" class=\"headerlink\" title=\"解决方案7：随机前缀和扩容RDD进行join\"></a>解决方案7：随机前缀和扩容RDD进行join</h4><p>适用于在join操作时，RDD中有大量key导致数据倾斜，分拆没意义，左边所有都加n以内随机前缀，右边稳定扩容n倍，然后左右join；</p>\n<h4 id=\"解决方案8：多种方案组合使用\"><a href=\"#解决方案8：多种方案组合使用\" class=\"headerlink\" title=\"解决方案8：多种方案组合使用\"></a>解决方案8：多种方案组合使用</h4><p>针对复杂场景，需要多种方案组合使用，针对多环节数据倾斜spark作业，可以先预处理，其次可以shuffle操作提高并行度，最后针对不同的聚合或join操作，才用两段聚合、随机前缀等方案；</p>\n<h3 id=\"Shuffle调优\"><a href=\"#Shuffle调优\" class=\"headerlink\" title=\"Shuffle调优\"></a>Shuffle调优</h3><h4 id=\"HashShuffleManager运行原理\"><a href=\"#HashShuffleManager运行原理\" class=\"headerlink\" title=\"HashShuffleManager运行原理\"></a>HashShuffleManager运行原理</h4><p>假设前提：每个Executor只分配1个CPU core，也就是说无论这个Executor上分配多少个task线程，同一时间只能执行一个task线程；</p>\n<h5 id=\"未经优化的HashShuffleManager\"><a href=\"#未经优化的HashShuffleManager\" class=\"headerlink\" title=\"未经优化的HashShuffleManager\"></a>未经优化的HashShuffleManager</h5><p>shuffle write：当前stage的每个task要为下个stage创建多少份磁盘文件？下个stage有多少task就要创建多少！下个stage有100个task，当前stage的每个task就要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，那么每个Executor执行5个task，每个Executor会创建500个磁盘文件；</p>\n<p>shuffle read：通常是一个stage刚开始要做的事，该stage中的每个task从上个stage拉取相同key，网络传输到自己所在节点，然后聚合或连接；此时上个stage中的task已经分好了给下个stage每个task的磁盘文件，直接拉取；</p>\n<p>shuffle read的拉取过程是一边拉取一边聚合的，每个shuffle read task都有自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的Map进行聚合等操作，聚合完一批再拉下一批；</p>\n<p><img src=\"/2022/12/09/spark-opt/hashshufflemanager.png\" alt=\"hashshufflemanager\"></p>\n<h5 id=\"优化后的HashShuffleManager\"><a href=\"#优化后的HashShuffleManager\" class=\"headerlink\" title=\"优化后的HashShuffleManager\"></a>优化后的HashShuffleManager</h5><p>设置参数spark.shuffle.consolidateFiles=true，默认是false。出现shuffleFileGroup概念，CPU core决定可以并行task数量，每个Executor上磁盘文件数此时取决于CPU core数量*stage的task数量，consolidate机制使不同task可以复用同一批磁盘文件。减少磁盘文件数量，提升shuffle write性能；</p>\n<p><img src=\"/2022/12/09/spark-opt/hashshufflemanageropt.png\" alt=\"hashshufflemanageropt\"></p>\n<h4 id=\"SortShuffleManager运行原理\"><a href=\"#SortShuffleManager运行原理\" class=\"headerlink\" title=\"SortShuffleManager运行原理\"></a>SortShuffleManager运行原理</h4><p>SortShuffleManager分为普通运行机制和bypass运行机制，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数值时(默认200)，就启用bypass机制；</p>\n<h5 id=\"普通运行机制\"><a href=\"#普通运行机制\" class=\"headerlink\" title=\"普通运行机制\"></a>普通运行机制</h5><p>排序之后分批(1万条)溢写磁盘，写入磁盘是通过java的BufferedOutputStream实现，先缓冲内存，内存满了之后溢写磁盘，每个task最终只有一个磁盘文件，但是会有一个索引文件，标识下游各个task需要数据的start offset和end offset；</p>\n<p><img src=\"/2022/12/09/spark-opt/sortshufflemanager.png\" alt=\"sortshufflemanager\"></p>\n<h5 id=\"bypass运行机制\"><a href=\"#bypass运行机制\" class=\"headerlink\" title=\"bypass运行机制\"></a>bypass运行机制</h5><p>触发条件：shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，不是聚合类的shuffle算子(例如reduceByKey)；</p>\n<p>此时task会为每个下游task通过hash创建一个临时磁盘文件，类似于HashShuffleManager未优化版本，但是最终会做一个磁盘文件的合并，所以相对来说shuffle read性能会好些；</p>\n<p>相较于SortShuffleManager普通机制，磁盘写机制不同，不会进行排序；</p>\n<p><img src=\"/2022/12/09/spark-opt/bypass.png\" alt=\"bypass\"></p>\n<h4 id=\"spark-shuffle-file-buffer\"><a href=\"#spark-shuffle-file-buffer\" class=\"headerlink\" title=\"spark.shuffle.file.buffer\"></a>spark.shuffle.file.buffer</h4><p>默认值：32k</p>\n<p>参数说明：设置shuffle write task的BufferedOutputStream的buffer缓冲大小，将数据写到磁盘文件之前，会先写到buffer缓冲中，待缓冲写满之后，才会溢写磁盘；</p>\n<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如64k)，从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。合理调节，1%-5%性能提升；</p>\n<h4 id=\"spark-reducer-maxSizeInFlight\"><a href=\"#spark-reducer-maxSizeInFlight\" class=\"headerlink\" title=\"spark.reducer.maxSizeInFlight\"></a>spark.reducer.maxSizeInFlight</h4><p>默认值：48M</p>\n<p>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，这个buffer缓冲决定了每次拉取多少数据</p>\n<p>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小(比如96m)，从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。合理调节，1%-5%性能提升；</p>\n<h4 id=\"spark-shuffle-io-maxRetries\"><a href=\"#spark-shuffle-io-maxRetries\" class=\"headerlink\" title=\"spark.shuffle.io.maxRetries\"></a>spark.shuffle.io.maxRetries</h4><p>默认值：3</p>\n<p>参数说明：shuffle read task从shuffle write task所在节点拉取自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数代表可以重试的最大次数，如果在指定次数之内拉取还没成功，就可能导致作业执行失败。</p>\n<p>调优建议：对于那些包含特别耗时的shuffle操作作业，建议增加重试最大次数(比如60次)，以避免JVM的full gc或者网络不稳定等因素导致数据拉取失败。对于超大数据量(数十亿-上百亿)的shuffle过程，调节该参数可以大幅度提升稳定性；</p>\n<h4 id=\"spark-shuffle-io-retryWait\"><a href=\"#spark-shuffle-io-retryWait\" class=\"headerlink\" title=\"spark.shuffle.io.retryWait\"></a>spark.shuffle.io.retryWait</h4><p>默认值：5s</p>\n<p>参数说明：代表每次重试拉取数据的等待间隔，默认是5s。</p>\n<p>调优建议：建议加大间隔时长，以增加shuffle操作的稳定性。</p>\n<h4 id=\"spark-shuffle-memoryFraction-1\"><a href=\"#spark-shuffle-memoryFraction-1\" class=\"headerlink\" title=\"spark.shuffle.memoryFraction\"></a>spark.shuffle.memoryFraction</h4><p>默认值：0.2</p>\n<p>参数说明：该参数代表Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</p>\n<p>调优建议：如果内存充足，而且很少使用持久化操作，建议提高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足聚合过程中频繁读写磁盘。实践过程中，合理调节，10%性能提升；</p>\n<h4 id=\"spark-shuffle-manager\"><a href=\"#spark-shuffle-manager\" class=\"headerlink\" title=\"spark.shuffle.manager\"></a>spark.shuffle.manager</h4><p>默认值：sort</p>\n<p>参数说明：该参数用于设置ShuffleManager的类型。Spark1.5之后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2之前的默认选项，但是spark 1.2之后版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高；</p>\n<p>调优建议：SortShuffleManager默认会对数据进行排序，如果业务逻辑需要该排序机制的话，则使用默认的SortShuffleManager就可以，如果不需要对数据进行排序，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。tungsten-sort慎用～</p>\n<h4 id=\"spark-shuffle-sort-bypassMergeThreshold\"><a href=\"#spark-shuffle-sort-bypassMergeThreshold\" class=\"headerlink\" title=\"spark.shuffle.sort.bypassMergeThreshold\"></a>spark.shuffle.sort.bypassMergeThreshold</h4><p>默认值：200</p>\n<p>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值时，则shuffle write不会进行排序操作，而是按照未经优化的HashShuffleManager的方式写数据，最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件；</p>\n<p>调优建议：当使用SortShuffleManager时，如果的确不需要排序操作，调大参数，大于shuffle read task的数量，会自动启用bypass机制，map-side就不会进行排序了，减少了排序性能开销，但是依然会产生大量磁盘文件，因此shuffle write性能有待提高；</p>\n<h4 id=\"spark-shuffle-consolidateFiles\"><a href=\"#spark-shuffle-consolidateFiles\" class=\"headerlink\" title=\"spark.shuffle.consolidateFiles\"></a>spark.shuffle.consolidateFiles</h4><p>默认值：false</p>\n<p>参数说明：如果使用了HashShuffleManager，该参数有效，如果设置了true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大减少磁盘IO开销，提升性能；</p>\n<p>调优建议：如果的确不需要SortShuffleManager排序机制，除了使用bypass还可以将spark.shuffle.manager设置为hash，使用HashShuffleManager，同时开启consolidate机制。实践表示，性能比开启bypass的SortShuffleManager要高出10%-30%；</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>(1)<a href=\"https://tech.meituan.com/2016/04/29/spark-tuning-basic.html\">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a></p>\n<p>(2)<a href=\"https://tech.meituan.com/2016/05/12/spark-tuning-pro.html\">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>\n<p>(3)<a href=\"https://bbs.huaweicloud.com/blogs/325349\">https://bbs.huaweicloud.com/blogs/325349</a></p>"},{"title":"clickhouse","date":"2022-12-21T01:54:58.000Z","_content":"\n> clickhouse引擎一共分为四个系列，分别是Log、MergeTree、Integration、Special。其中包含两种特殊的表引擎Replicated、Distributed，功能上与其他表引擎正交。\n\n![engine](./clickhouse/engine.png)\n\n### 引擎\n\n**Log系列**\n\n引擎功能相对简单，主要用于快速写入小表(1百万行左右的表)，然后全部读出的场景。\n\n**Integration系列**\n\n引擎主要用于把外部数据导入到ClickHouse中，或者在Clickhouse中直接操作外部数据源。\n\n**Special系列**\n\n引擎主要为了特定场景定制。\n\n**MergeTree系列**\n\nMergeTree：\n\n引擎主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有Clickhouse sql语法，但是有些功能与MySQL并不一致，比如MergeTree主键不去重。\n\n由于MergeTree采用类似LSM tree的结构，很多存储层处理逻辑直到Compaction期间才会发生。\n\nMergeTree虽然有主键索引，但是主要作用是加速，而不是为了保持唯一。Compaction完成后还是有主键相同数据。\n\nReplicatedMergeTree:\n\n![lsmtree](./clickhouse/lsmtree.jpeg)\n\n\n\n### 参考\n\nengine:https://developer.aliyun.com/article/762461\n\nlsm-tree:https://cloud.tencent.com/developer/article/1441835\n","source":"_posts/clickhouse.md","raw":"---\ntitle: clickhouse\ndate: 2022-12-21 09:54:58\ntags: clickhouse\n---\n\n> clickhouse引擎一共分为四个系列，分别是Log、MergeTree、Integration、Special。其中包含两种特殊的表引擎Replicated、Distributed，功能上与其他表引擎正交。\n\n![engine](./clickhouse/engine.png)\n\n### 引擎\n\n**Log系列**\n\n引擎功能相对简单，主要用于快速写入小表(1百万行左右的表)，然后全部读出的场景。\n\n**Integration系列**\n\n引擎主要用于把外部数据导入到ClickHouse中，或者在Clickhouse中直接操作外部数据源。\n\n**Special系列**\n\n引擎主要为了特定场景定制。\n\n**MergeTree系列**\n\nMergeTree：\n\n引擎主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有Clickhouse sql语法，但是有些功能与MySQL并不一致，比如MergeTree主键不去重。\n\n由于MergeTree采用类似LSM tree的结构，很多存储层处理逻辑直到Compaction期间才会发生。\n\nMergeTree虽然有主键索引，但是主要作用是加速，而不是为了保持唯一。Compaction完成后还是有主键相同数据。\n\nReplicatedMergeTree:\n\n![lsmtree](./clickhouse/lsmtree.jpeg)\n\n\n\n### 参考\n\nengine:https://developer.aliyun.com/article/762461\n\nlsm-tree:https://cloud.tencent.com/developer/article/1441835\n","slug":"clickhouse","published":1,"updated":"2022-12-21T02:47:01.620Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbye1xpu0000z2gnech818bz","content":"<blockquote>\n<p>clickhouse引擎一共分为四个系列，分别是Log、MergeTree、Integration、Special。其中包含两种特殊的表引擎Replicated、Distributed，功能上与其他表引擎正交。</p>\n</blockquote>\n<p><img src=\"/2022/12/21/clickhouse/engine.png\" alt=\"engine\"></p>\n<h3 id=\"引擎\"><a href=\"#引擎\" class=\"headerlink\" title=\"引擎\"></a>引擎</h3><p><strong>Log系列</strong></p>\n<p>引擎功能相对简单，主要用于快速写入小表(1百万行左右的表)，然后全部读出的场景。</p>\n<p><strong>Integration系列</strong></p>\n<p>引擎主要用于把外部数据导入到ClickHouse中，或者在Clickhouse中直接操作外部数据源。</p>\n<p><strong>Special系列</strong></p>\n<p>引擎主要为了特定场景定制。</p>\n<p><strong>MergeTree系列</strong></p>\n<p>MergeTree：</p>\n<p>引擎主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有Clickhouse sql语法，但是有些功能与MySQL并不一致，比如MergeTree主键不去重。</p>\n<p>由于MergeTree采用类似LSM tree的结构，很多存储层处理逻辑直到Compaction期间才会发生。</p>\n<p>MergeTree虽然有主键索引，但是主要作用是加速，而不是为了保持唯一。Compaction完成后还是有主键相同数据。</p>\n<p>ReplicatedMergeTree:</p>\n<p><img src=\"/2022/12/21/clickhouse/lsmtree.jpeg\" alt=\"lsmtree\"></p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>engine:<a href=\"https://developer.aliyun.com/article/762461\">https://developer.aliyun.com/article/762461</a></p>\n<p>lsm-tree:<a href=\"https://cloud.tencent.com/developer/article/1441835\">https://cloud.tencent.com/developer/article/1441835</a></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>clickhouse引擎一共分为四个系列，分别是Log、MergeTree、Integration、Special。其中包含两种特殊的表引擎Replicated、Distributed，功能上与其他表引擎正交。</p>\n</blockquote>\n<p><img src=\"/2022/12/21/clickhouse/engine.png\" alt=\"engine\"></p>\n<h3 id=\"引擎\"><a href=\"#引擎\" class=\"headerlink\" title=\"引擎\"></a>引擎</h3><p><strong>Log系列</strong></p>\n<p>引擎功能相对简单，主要用于快速写入小表(1百万行左右的表)，然后全部读出的场景。</p>\n<p><strong>Integration系列</strong></p>\n<p>引擎主要用于把外部数据导入到ClickHouse中，或者在Clickhouse中直接操作外部数据源。</p>\n<p><strong>Special系列</strong></p>\n<p>引擎主要为了特定场景定制。</p>\n<p><strong>MergeTree系列</strong></p>\n<p>MergeTree：</p>\n<p>引擎主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有Clickhouse sql语法，但是有些功能与MySQL并不一致，比如MergeTree主键不去重。</p>\n<p>由于MergeTree采用类似LSM tree的结构，很多存储层处理逻辑直到Compaction期间才会发生。</p>\n<p>MergeTree虽然有主键索引，但是主要作用是加速，而不是为了保持唯一。Compaction完成后还是有主键相同数据。</p>\n<p>ReplicatedMergeTree:</p>\n<p><img src=\"/2022/12/21/clickhouse/lsmtree.jpeg\" alt=\"lsmtree\"></p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>engine:<a href=\"https://developer.aliyun.com/article/762461\">https://developer.aliyun.com/article/762461</a></p>\n<p>lsm-tree:<a href=\"https://cloud.tencent.com/developer/article/1441835\">https://cloud.tencent.com/developer/article/1441835</a></p>\n"},{"title":"hbase","date":"2022-12-20T06:13:02.000Z","_content":"\n> HBase相关\n\n### 定义\n\nHBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。\n\n### 参考\n\nhttps://www.cnblogs.com/bbgs-xc/p/13070724.html\n","source":"_posts/hbase.md","raw":"---\ntitle: hbase\ndate: 2022-12-20 14:13:02\ntags: HBase\n---\n\n> HBase相关\n\n### 定义\n\nHBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。\n\n### 参考\n\nhttps://www.cnblogs.com/bbgs-xc/p/13070724.html\n","slug":"hbase","published":1,"updated":"2022-12-21T01:54:25.410Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbye1xpv0001z2gnccgwgcaq","content":"<blockquote>\n<p>HBase相关</p>\n</blockquote>\n<h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/bbgs-xc/p/13070724.html\">https://www.cnblogs.com/bbgs-xc/p/13070724.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>HBase相关</p>\n</blockquote>\n<h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>HBase是一种分布式、可扩展、支持海量数据存储的NoSQL数据库。</p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p><a href=\"https://www.cnblogs.com/bbgs-xc/p/13070724.html\">https://www.cnblogs.com/bbgs-xc/p/13070724.html</a></p>\n"},{"title":"flink-start","date":"2022-12-19T02:38:33.000Z","_content":"\n> Flink作业\n\n### Flink作业提交过程\n\nMaster和TaskManager在作业提交前启动(bin/start-cluster.sh)，TaskManager将自己注册给Master中的ResourceManager，初始化和资源注册过程发生在作业提交前，称之为第0步。\n\n![flinkstart](./flink-start/flinkstart.png)\n\n1.用户通过Flink客户端Clien提交作业，调用API构建数据流图，代码和配置编译打包，提交Master的Dispatcher，形成一个应用Application；\n\n2.Dispatcher接收作业，启动JobManager，负责本次作业的各项协调工作；\n\n3.JobManager向ResourceManager申请本次作业所需资源；\n\n4.闲置TaskManager会被反馈给JobManager；\n\n5.JobManager将逻辑视图转化为并行的物理执行图，将计算任务分发部署到多个TaskManager上，任务开始执行；\n","source":"_posts/flink-start.md","raw":"---\ntitle: flink-start\ndate: 2022-12-19 10:38:33\ntags: Flink\n---\n\n> Flink作业\n\n### Flink作业提交过程\n\nMaster和TaskManager在作业提交前启动(bin/start-cluster.sh)，TaskManager将自己注册给Master中的ResourceManager，初始化和资源注册过程发生在作业提交前，称之为第0步。\n\n![flinkstart](./flink-start/flinkstart.png)\n\n1.用户通过Flink客户端Clien提交作业，调用API构建数据流图，代码和配置编译打包，提交Master的Dispatcher，形成一个应用Application；\n\n2.Dispatcher接收作业，启动JobManager，负责本次作业的各项协调工作；\n\n3.JobManager向ResourceManager申请本次作业所需资源；\n\n4.闲置TaskManager会被反馈给JobManager；\n\n5.JobManager将逻辑视图转化为并行的物理执行图，将计算任务分发部署到多个TaskManager上，任务开始执行；\n","slug":"flink-start","published":1,"updated":"2022-12-21T01:54:41.379Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clbye1xq20006z2gn382e5ipx","content":"<blockquote>\n<p>Flink作业</p>\n</blockquote>\n<h3 id=\"Flink作业提交过程\"><a href=\"#Flink作业提交过程\" class=\"headerlink\" title=\"Flink作业提交过程\"></a>Flink作业提交过程</h3><p>Master和TaskManager在作业提交前启动(bin/start-cluster.sh)，TaskManager将自己注册给Master中的ResourceManager，初始化和资源注册过程发生在作业提交前，称之为第0步。</p>\n<p><img src=\"/2022/12/19/flink-start/flinkstart.png\" alt=\"flinkstart\"></p>\n<p>1.用户通过Flink客户端Clien提交作业，调用API构建数据流图，代码和配置编译打包，提交Master的Dispatcher，形成一个应用Application；</p>\n<p>2.Dispatcher接收作业，启动JobManager，负责本次作业的各项协调工作；</p>\n<p>3.JobManager向ResourceManager申请本次作业所需资源；</p>\n<p>4.闲置TaskManager会被反馈给JobManager；</p>\n<p>5.JobManager将逻辑视图转化为并行的物理执行图，将计算任务分发部署到多个TaskManager上，任务开始执行；</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Flink作业</p>\n</blockquote>\n<h3 id=\"Flink作业提交过程\"><a href=\"#Flink作业提交过程\" class=\"headerlink\" title=\"Flink作业提交过程\"></a>Flink作业提交过程</h3><p>Master和TaskManager在作业提交前启动(bin/start-cluster.sh)，TaskManager将自己注册给Master中的ResourceManager，初始化和资源注册过程发生在作业提交前，称之为第0步。</p>\n<p><img src=\"/2022/12/19/flink-start/flinkstart.png\" alt=\"flinkstart\"></p>\n<p>1.用户通过Flink客户端Clien提交作业，调用API构建数据流图，代码和配置编译打包，提交Master的Dispatcher，形成一个应用Application；</p>\n<p>2.Dispatcher接收作业，启动JobManager，负责本次作业的各项协调工作；</p>\n<p>3.JobManager向ResourceManager申请本次作业所需资源；</p>\n<p>4.闲置TaskManager会被反馈给JobManager；</p>\n<p>5.JobManager将逻辑视图转化为并行的物理执行图，将计算任务分发部署到多个TaskManager上，任务开始执行；</p>\n"}],"PostAsset":[{"_id":"source/_posts/hello-world/2022020901.jpeg","slug":"2022020901.jpeg","post":"clbu4ffh9000f16gn5y4jccx7","modified":0,"renderable":0},{"_id":"source/_posts/hello-world/2022020902.jpeg","slug":"2022020902.jpeg","post":"clbu4ffh9000f16gn5y4jccx7","modified":0,"renderable":0},{"_id":"source/_posts/hello-world/kafka.png","slug":"kafka.png","post":"clbu4ffh9000f16gn5y4jccx7","modified":0,"renderable":0},{"_id":"source/_posts/spark-role/sparksubmit.png","slug":"sparksubmit.png","post":"clbu4ffha000m16gn0zk88msb","modified":0,"renderable":0},{"_id":"source/_posts/hive-data/orc.png","slug":"orc.png","post":"clbu4ffh9000h16gnbtod0o29","modified":0,"renderable":0},{"_id":"source/_posts/hive-data/parquet.png","slug":"parquet.png","post":"clbu4ffh9000h16gnbtod0o29","modified":0,"renderable":0},{"_id":"source/_posts/spark-sql/analyzer.png","slug":"analyzer.png","post":"clbu4ffhb000p16gnfwgi2hj4","modified":0,"renderable":0},{"_id":"source/_posts/spark-sql/constant_fold.png","slug":"constant_fold.png","post":"clbu4ffhb000p16gnfwgi2hj4","modified":0,"renderable":0},{"_id":"source/_posts/spark-sql/parser.png","slug":"parser.png","post":"clbu4ffhb000p16gnfwgi2hj4","modified":0,"renderable":0},{"_id":"source/_posts/spark-sql/predicate_pushdown.png","slug":"predicate_pushdown.png","post":"clbu4ffhb000p16gnfwgi2hj4","modified":0,"renderable":0},{"_id":"source/_posts/spark-sql/summary.png","slug":"summary.png","post":"clbu4ffhb000p16gnfwgi2hj4","modified":0,"renderable":0},{"_id":"source/_posts/spark-yarn/sparkonyarn.png","slug":"sparkonyarn.png","post":"clbu4ffhc001116gngqhhamxk","modified":0,"renderable":0},{"_id":"source/_posts/spark-yarn/yarnclient.png","slug":"yarnclient.png","post":"clbu4ffhc001116gngqhhamxk","modified":0,"renderable":0},{"_id":"source/_posts/spark-yarn/yarncluster.png","slug":"yarncluster.png","post":"clbu4ffhc001116gngqhhamxk","modified":0,"renderable":0},{"_id":"source/_posts/spark-opt/bypass.png","slug":"bypass.png","post":"clbu4ffhd001216gncy4ohc18","modified":0,"renderable":0},{"_id":"source/_posts/spark-opt/hashshufflemanager.png","slug":"hashshufflemanager.png","post":"clbu4ffhd001216gncy4ohc18","modified":0,"renderable":0},{"_id":"source/_posts/spark-opt/hashshufflemanageropt.png","slug":"hashshufflemanageropt.png","post":"clbu4ffhd001216gncy4ohc18","modified":0,"renderable":0},{"_id":"source/_posts/spark-opt/sortshufflemanager.png","slug":"sortshufflemanager.png","post":"clbu4ffhd001216gncy4ohc18","modified":0,"renderable":0},{"_id":"source/_posts/spark-opt/spark-submit.png","slug":"spark-submit.png","post":"clbu4ffhd001216gncy4ohc18","modified":0,"renderable":0},{"_id":"source/_posts/clickhouse/engine.png","slug":"engine.png","post":"clbye1xpu0000z2gnech818bz","modified":0,"renderable":0},{"_id":"source/_posts/clickhouse/lsmtree.jpeg","slug":"lsmtree.jpeg","post":"clbye1xpu0000z2gnech818bz","modified":0,"renderable":0},{"_id":"source/_posts/flink-start/flinkstart.png","slug":"flinkstart.png","post":"clbye1xq20006z2gn382e5ipx","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"clbu4ffh2000016gn0o4e3og8","tag_id":"clbu4ffh5000216gn9jr638qi","_id":"clbu4ffh7000716gn87mcac5w"},{"post_id":"clbu4ffh4000116gn6t5zgq99","tag_id":"clbu4ffh7000616gnaqrddjfz","_id":"clbu4ffh8000c16gn3ah418og"},{"post_id":"clbu4ffh6000316gn67gccpol","tag_id":"clbu4ffh8000a16gn7sqtbztf","_id":"clbu4ffh9000g16gn6y3kanep"},{"post_id":"clbu4ffh6000416gn6vrkg2t8","tag_id":"clbu4ffh9000e16gnf5po0e2q","_id":"clbu4ffha000k16gnetwn3odn"},{"post_id":"clbu4ffha000l16gn86at1ifp","tag_id":"clbu4ffh7000616gnaqrddjfz","_id":"clbu4ffhb000o16gnh3o57bnz"},{"post_id":"clbu4ffh7000516gndftv911s","tag_id":"clbu4ffha000i16gndy8meoot","_id":"clbu4ffhb000q16gne5d0gnzy"},{"post_id":"clbu4ffhb000p16gnfwgi2hj4","tag_id":"clbu4ffh5000216gn9jr638qi","_id":"clbu4ffhb000s16gne0li91bg"},{"post_id":"clbu4ffh7000816gn905734e1","tag_id":"clbu4ffha000n16gndggb764j","_id":"clbu4ffhb000t16gndaqy8y9h"},{"post_id":"clbu4ffh8000916gnb6zs7ydn","tag_id":"clbu4ffhb000r16gn6qd21v64","_id":"clbu4ffhb000v16gnhqkg3qm1"},{"post_id":"clbu4ffh8000d16gn3yvka122","tag_id":"clbu4ffhb000u16gndmec653b","_id":"clbu4ffhb000x16gn5ovd47wi"},{"post_id":"clbu4ffha000j16gn6u8q4t3k","tag_id":"clbu4ffhb000w16gn36a07e0z","_id":"clbu4ffhc000z16gn95aff4go"},{"post_id":"clbu4ffha000m16gn0zk88msb","tag_id":"clbu4ffhb000y16gnftxwbi2h","_id":"clbu4ffhc001016gn3bmga7n2"},{"post_id":"clbu4ffhc001116gngqhhamxk","tag_id":"clbu4ffhb000y16gnftxwbi2h","_id":"clbu4ffhd001316gn3ny13sd9"},{"post_id":"clbu4ffhd001216gncy4ohc18","tag_id":"clbu4ffhb000y16gnftxwbi2h","_id":"clbu4ffhd001416gn83mrbsv5"},{"post_id":"clbye1xpu0000z2gnech818bz","tag_id":"clbye1xpw0002z2gnb55p7ow4","_id":"clbye1xpz0004z2gn2n0cccik"},{"post_id":"clbye1xpv0001z2gnccgwgcaq","tag_id":"clbye1xpz0003z2gn7nrc7ni5","_id":"clbye1xpz0005z2gn20l66gol"},{"post_id":"clbye1xq20006z2gn382e5ipx","tag_id":"clbye1xq30007z2gngnu015l4","_id":"clbye1xq30008z2gnbf42f4r7"}],"Tag":[{"name":"大数据","_id":"clbu4ffh5000216gn9jr638qi"},{"name":"算法","_id":"clbu4ffh7000616gnaqrddjfz"},{"name":"数据","_id":"clbu4ffh8000a16gn7sqtbztf"},{"name":"HDFS","_id":"clbu4ffh9000e16gnf5po0e2q"},{"name":"动态规划","_id":"clbu4ffha000i16gndy8meoot"},{"name":"树","_id":"clbu4ffha000n16gndggb764j"},{"name":"链表","_id":"clbu4ffhb000r16gn6qd21v64"},{"name":"pytorch","_id":"clbu4ffhb000u16gndmec653b"},{"name":"sql","_id":"clbu4ffhb000w16gn36a07e0z"},{"name":"Spark","_id":"clbu4ffhb000y16gnftxwbi2h"},{"name":"clickhouse","_id":"clbye1xpw0002z2gnb55p7ow4"},{"name":"HBase","_id":"clbye1xpz0003z2gn7nrc7ni5"},{"name":"Flink","_id":"clbye1xq30007z2gngnu015l4"}]}}